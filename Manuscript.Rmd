---
title: 'Individual heterogeneity in the functional topography of the default-mode network in medial prefrontal cortex'
author: "Claudio Toro-Serey, Sean Tobyne, & Joseph T. McGuire"
output:
  pdf_document: 
    keep_tex: true
  word_document: 
    reference_docx: Style_reference_draft.docx
  html_document: default
bibliography: mPFC_paper.bib
header-includes:
    - \usepackage{setspace}\doublespacing
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
```

```{r Libraries and functions, include = FALSE, fig.align="center", fig.width=6, fig.height=4, echo=FALSE}

Cols <- c("aquamarine4", "#D9541A", rgb(190,190,190,100, maxColorValue = 255)) 

# In case I want to visualize stuff in the brain
labelCoords_vertex <- read.csv2('labelCoords_vertex.csv')[,2:6]
labelCoords_vertex <- transform(labelCoords_vertex, x = as.numeric(as.character(x)), y = as.numeric(as.character(y)), z = as.numeric(as.character(z)))

# general options
threshMaps <- TRUE # Whether to apply subject-wise thresholds on maps or not
permuting <- FALSE # show permutation progress?

# Libraries used for the whole process
library(tidyverse)
library(igraph)
library(pander)
library(ggnetwork)
library(ggplot2)
library(gridExtra)
library(data.table)
library(plyr)
library(mcclust)
library(lme4)
library(parallel)
library(corrplot)
library(DescTools)
library(knitr)


# produce one permutation instance of the meta-analysis (so it can be fed to mclapply for quicker results)
metaPerm <- function(list1 = studyList_sv, list2 = studyList_dmn, labels = Yeo_labels, perm = 1) {
  
  # print every 100th permutation just to have a general sense of how far we are
  if ((perm %% 100) == 0) {sprintf("Permutation %s", perm)}
  
  # combine study lists
  allStudies <- c(studyList_sv, studyList_dmn)
  
  # Sample all the studies and divide into 2 groups
  randPerm <- sample(allStudies)
  tmpSV <- randPerm[seq(length(list1))]
  tmpDMN <- randPerm[-seq(length(list1))]
  
  # Count the number of times each region was reported in the literature
  tmpDF <- data.frame(Parcel = gsub("_", "", labels),
                      SV = sapply(labels, function(roi) {length(grep(roi, tmpSV))}),
                      DMN = sapply(labels, function(roi) {length(grep(roi, tmpDMN))}))
  
  # Perform the prop test for each pair and get the max chi-squared stat
  maxStat <- max(
    apply(tmpDF[, 2:3], 1, function(data) {
      prop.test(data, c(length(tmpSV), length(tmpDMN)))$statistic
    }
    ), na.rm = T)
  
  return(maxStat)
  
}


## Cohen's D for 2 groups
# for a more flexible approach, make the data input to be a list with entries for n groups, 
# then do length(list) for the number of groups. 
cohenD <- function(group1 = 1, group2 = 2){
  
  # means
  mean1 <- mean(group1, na.rm = T)
  mean2 <- mean(group2, na.rm = T)
  
  # variance
  var1 <- var(group1, na.rm = T)
  var2 <- var(group2, na.rm = T)
  
  # equation
  out <- (mean1 - mean2) / sqrt((var1 + var2)/2)
  
  return(out)
  
}

## log-log degree distribution
logDegreeDist <- function(Graph = padjMat) {
  
  tempD <- degree(Graph)
  tempDD <- degree.distribution(Graph) # frequency of occurrencies of certain degrees
  d <- (0:(max(tempD)-1))
  ind <- (tempDD!=0)
  plot(d[ind], tempDD[ind], log = "xy", col = "blue", 
       xlab = "Log-Degree", ylab = "Log-Intensity",  
       main = "Log-Log Degree Distribution")
  
}


## Create correlation matrix between vertices of 2 ROIs
# Requires labeled time series matrix
vertexCorrMat <- function(ROI_1 = 'R_7m_ROI', ROI_2 = 'L_7m_ROI'){
  
  # extract the time series from the ROIs
  indexing_1 <- which(rownames(timeSeries)==as.character(ROI_1))
  indexing_2 <- which(rownames(timeSeries)==as.character(ROI_2))
  nVerts_1 <- length(indexing_1)
  nVerts_2 <- length(indexing_2)
  ROI_1_tseries <- timeSeries[indexing_1, ]
  ROI_2_tseries <- timeSeries[indexing_2, ]
  
  # Create empty matrix
  corrMat <- matrix(data = 0, nrow = nVerts_1, ncol = nVerts_2)
  
  # 
  for (vertex_1 in seq(nVerts_1)){
    
    # temp series 1
    tempOne <- ROI_1_tseries[vertex_1, ]
    
    for(vertex_2 in seq(nVerts_2)){
      
      # temp series 2
      tempTwo <- ROI_2_tseries[vertex_2, ]
      
      # store the Pearson correlation in the corr matrix
      corrMat[vertex_1, vertex_2] <- cor(tempOne, tempTwo, method = "pearson") 
      
    }
  }
  
  # name the dimensions of the matrix according to the surface vertex index
  rownames(corrMat) <- indexing_1
  colnames(corrMat) <- indexing_2
  
  # plot (optional?)  
  # unclustered heatmap
  heatmap(corrMat,
          Rowv = NA,
          Colv = NA,
          scale = "row",
          col = pallette(1000))
  
  # clustered heatmap
  heatmap(corrMat,
          scale = "row",
          col = pallette(1000))
  
  # print min-max corr vals
  print(range(corrMat))
  
  return(corrMat)
  
}


## Correlation matrix between an ROI and the Glasser parcels
parcelCorrMat <- function(ROI = 'R_7m_ROI', lbels = lookup$V1){
  
  # extract the time series from the ROIs
  indexing <- which(rownames(timeSeries)==as.character(ROI))
  nVerts <- length(indexing)
  ROI_tseries <- timeSeries[indexing, ]
  
  # create empty matrix to store values
  corrMat <- matrix(data = 0, nrow = nVerts, ncol = 360)
  
  # Loop through every combination of label and 7m vertex
  for (Parcel in lbels){
    
    # Get an ROI index to retrieve timeseries and store correlation vals
    indxROI <- which(lbels==as.character(Parcel))
    
    for(vertex in seq(nVerts)){
      
      # temp series
      tempROI <- as.numeric(ptSeries[indxROI, ])
      tempVertex <- ROI_tseries[vertex, ]
      
      # store the Pearson correlation in the corr matrix
      corrMat[vertex, indxROI] <- cor(tempROI, tempVertex, method = "pearson") 
      
    }
  }
  
  # name the columns according to labels
  colnames(corrMat) <- lbels
  rownames(corrMat) <- indexing
  
  # plot (optional?)  
  # unclustered heatmap
  heatmap(corrMat,
          Rowv = NA,
          Colv = NA,
          scale = "none",
          col = pallette(1000))
  
  # clustered heatmap
  heatmap(corrMat,
          scale = "none",
          col = pallette(1000))
  
  # print min-max corr vals
  print(range(corrMat))
  
  return(corrMat)
  
}


## Correlation matrix between an Parcel and every other grayordinate
# THIS CAN BE OPTIMIZED WITH A BETTER USE OF COR()
partoverCorrMat <- function(ROI = 'R_7m_ROI', lbels = verts[[1]]){
  
  # extract the time series from the ROI
  indexing <- which(rownames(ptSeries)==as.character(ROI))
  ROI_tseries <- as.numeric(ptSeries[indexing, ])
  
  # get the remaining number of vertices
  nVerts <- length(lbels) 
  
  # Create data frame to store values
  parcelIndx <- which(labelCoords_parcel$Label==ROI)
  corrVec <- data.frame(Seed = rep(ROI, nVerts),
                        Labels = lbels,
                        xstart = rep(labelCoords_parcel$x[parcelIndx], nVerts), # 7.33324 for R_7m
                        ystart = rep(labelCoords_parcel$y[parcelIndx], nVerts), # -63.142
                        zstart = rep(labelCoords_parcel$z[parcelIndx], nVerts)) # 42.5578
  
  for(vertex in seq(nVerts)){
    
    # temp series
    tempVertex <- timeSeries[vertex, ] 
    
    # store the Pearson correlation in the corr matrix
    #corrVec$Correlation[vertex] <- cor(ROI_tseries, tempVertex, method = "pearson") 
    
    # Another option, so that FDR can be applied
    tempCor <- cor.test(ROI_tseries, tempVertex, method = "pearson")
    corrVec$Correlation[vertex] <- tempCor$estimate
    corrVec$pval[vertex] <- tempCor$p.value
    
  }
  
  # Turn ROI vertices into 0s
  indexing <- which(corrVec$Labels==as.character(ROI))
  corrVec$Correlation[indexing] <- 0
  
  # Create a column with p-vals corrected for multiple comparisons
  corrVec$adjPval <- p.adjust(corrVec$pval, "BY")
  
  # normalize correlation vals for analysis (see below)
  tempTanh <- fisherTanh(Data = corrVec$Correlation)
  corrVec$tanhZ <- tempTanh$tanhZ
  corrVec$tanhPvals <- tempTanh$pvals
  corrVec$tanhPAdjusted <- tempTanh$adjustPvals
  
  return(corrVec)
  
}


## tanh-z transformation (variance stabilizing Fisher) and p-values (adjusted and not)
# This takes either a matrix of correlation values (vectors too, but manually compute pvals)
# Normalization approach suggested in network textbook (equation 7.20)
# This transformation is approximately normal with mean 0 and sd = sqrt(1 / (n - 3))
fisherTanh <- function(Data = padjMatrix){
  
  transformed <- list()
  
  # tanh
  transformed$tanhZ <- 0.5 * log((1 + Data) / (1 - Data))
  
  # p-vals
  if (is.matrix(Data)) {
    z.vec <- transformed$tanhZ[upper.tri(transformed$tanhZ)]
    n <- dim(Data)[1]
  } else if (is.vector(Data)) {
    z.vec <- transformed$tanhZ
    n <- length(Data)
  }
  transformed$pvals <- 2 * pnorm(abs(z.vec), 0 , sqrt(1 / (n - 3)), lower.tail = F) # no lower tail, but absolute Zs = two tailed
  
  # adjust pvals
  transformed$adjustPvals <- p.adjust(transformed$pvals, "BH")
  
  if (is.matrix(Data)) {
    # get pvals and their adjustment into a symetric matrix form
    # regular
    tempMat <- matrix(0, dim(Data)[1], dim(Data)[2])
    tempMat[upper.tri(tempMat)] <- transformed$pvals
    tempMat[lower.tri(tempMat)] <- transformed$pvals
    dimnames(tempMat) <- list(rownames(Data), rownames(Data))
    transformed$pvals <- tempMat
    
    # adjusted
    tempMat <- matrix(0, dim(Data)[1], dim(Data)[2])
    tempMat[upper.tri(tempMat)] <- transformed$adjustPvals
    tempMat[lower.tri(tempMat)] <- transformed$adjustPvals
    dimnames(tempMat) <- list(rownames(Data), rownames(Data))
    transformed$adjustPvals <- tempMat
  }
  
  return(transformed)
  
}


## Extract the centroid-most vertex from each parcel
parcelCentroid <- function(ROI = 'R_7m_ROI', all_coordinates = labelCoords_vertex){
  
  # The coordinate file must contain a Label column
  
  # Get the ROI-specific vertices
  indx <- grep(ROI, all_coordinates$Label)
  
  # Grab the coordinates
  tempCoords <- all_coordinates[indx, c("x","y","z")]
  rownames(tempCoords) <- indx
  
  # Compute the distance among all vertices, and turn into a matrix
  tempDist <- dist(tempCoords, diag = T)
  tempDist <- as.matrix(tempDist)
  
  # Compute the sum of distances for each vertex, and get the minimum
  sumDists <- colSums(tempDist)
  minDist <- which(sumDists == min(sumDists))
  minDist <- indx[minDist]
  
  return(minDist)
  
}


## This function takes the output from partoverCorrMat() and adds/removes the characteristics we decided on.
prepROItoVer <- function(Data = R_7m_allCorr, Coordinates = labelCoords_vertex){
  
  # Add vertex coordinates
  tempDframe <- cbind(Data, Coordinates[,1:3])
  
  # Remove rows with non-significant adjusted pvalues
  indx <- tempDframe$adjPval < 0.05
  tempDframe <- tempDframe[indx, ]
  
  # Characterize correlation direction and round up vals for plotting
  tempDframe$Relation <- ifelse(tempDframe$Correlation < 0, "Negative", "Positive")
  tempDframe$Correlation <- round(tempDframe$Correlation, digits = 2)
  
  # Add a column to differentiate between left/right hemispheres
  tempDframe$Hemisphere <- substring(tempDframe$Labels,1,1)
  
  return(tempDframe)
  
}


## Plot correlation from ROI to rest of vertices
plotROItoVertex <- function(Data = R_7m_allCorr, ROI = 'R_7m_ROI', ColRange = Cols, View = "Axial", Legends = TRUE){
  
  ROIindx <- grep(ROI, Data$Labels)
  ROIvertices <- Data[ROIindx,c("x","y","z")] # grab ROI-specific vertices to black out
  labelCoord <- ROIvertices[1,] # just to place the label
  
  if (View == "Axial") {
    ggplot() +
      geom_point(data = Data, aes(x=x, y=y, alpha=.5, color = Correlation), show.legend = Legends) +
      geom_point(data = labelCoords_vertex, aes(x=x, y=y), alpha=0.01) +
      geom_nodes(data = ROIvertices, aes(x=x, y=y)) +
      geom_nodelabel_repel(aes(x=labelCoord$x, y=labelCoord$y, label = sub("_ROI","",ROI))) +
      scale_color_gradient2(low = ColRange[1], mid = "white", high = ColRange[2], limits = c(-1,1)) +
      theme_blank()
  } else if (View == "Sagittal") {
    ggplot() +
      geom_point(data = Data, aes(x=y, y=z, alpha=.5, color = Correlation), show.legend = Legends) +
      geom_point(data = labelCoords_vertex, aes(x=y, y=z), alpha=0.01) +
      geom_nodes(data = ROIvertices, aes(x=y, y=z)) +
      geom_nodelabel_repel(aes(x=labelCoord$y, y=labelCoord$z, label = sub("_ROI","",ROI))) +
      scale_color_gradient2(low = ColRange[1], mid = "white", high = ColRange[2], limits = c(-1,1)) +
      theme_blank()
  } else if (View == "Coronal") {
    ggplot() +
      geom_point(data = Data, aes(x=x, y=z, alpha=.5, color = Correlation), show.legend = Legends) +
      geom_point(data = labelCoords_vertex, aes(x=x, y=z),  alpha=0.01) +
      geom_nodes(data = ROIvertices, aes(x=x, y=z), alpha=.5) +
      geom_nodelabel_repel(aes(x=labelCoord$x, y=labelCoord$z, label = sub("_ROI","",ROI))) +
      scale_color_gradient2(low = ColRange[1], mid = "white", high = ColRange[2], limits = c(-1,1)) +
      theme_blank()
  } else if (View == "Medial Right") {
    if (substring(ROI,1,1) == "R") {
      oneHemi <- Data[grep("R", Data$Hemisphere), ]
      oneHemi$Medial <- ifelse(oneHemi$x < 20, "Medial", "Other")
      medialHemi <- oneHemi[which(oneHemi$Medial=="Medial"), ]
      medialHemi$y <- medialHemi$y * -1
      ROIvertices$y <- ROIvertices$y * -1
      labelCoord$y <- labelCoord$y * -1
      oneHemi_all <- labelCoords_vertex[grep("R",labelCoords_vertex$Hemisphere), ] # whole surface
      oneHemi_all$Medial <- ifelse(oneHemi_all$x < 20, "Medial", "Other")
      medialHemi_all <- oneHemi_all[grep("Medial", oneHemi_all$Medial), ]
      medialHemi_all$y <- medialHemi_all$y * -1
      ggplot() +
        geom_point(data = medialHemi, aes(x=y, y=z, alpha=.1, color = Correlation), show.legend = Legends) +
        #geom_point(data = medialHemi_all, aes(x=y, y=z), alpha=0.07) +
        scale_color_gradient2(low = ColRange[1], mid = "white", high = ColRange[2], limits = c(-1,1)) +
        geom_nodes(data = ROIvertices, aes(x=y, y=z), alpha=.5) +
        geom_nodelabel_repel(aes(x=labelCoord$y, y=labelCoord$z, label = sub("_ROI","",ROI))) +
        theme_blank()
    } else {
      oneHemi <- Data[grep("R", Data$Hemisphere), ]
      oneHemi$Medial <- ifelse(oneHemi$x < 20, "Medial", "Other")
      medialHemi <- oneHemi[which(oneHemi$Medial=="Medial"), ]
      medialHemi$y <- medialHemi$y * -1
      ROIvertices$y <- ROIvertices$y * -1
      labelCoord$y <- labelCoord$y * -1
      oneHemi_all <- labelCoords_vertex[grep("R",labelCoords_vertex$Hemisphere), ] # whole surface
      oneHemi_all$Medial <- ifelse(oneHemi_all$x < 20, "Medial", "Other")
      medialHemi_all <- oneHemi_all[grep("Medial", oneHemi_all$Medial), ]
      medialHemi_all$y <- medialHemi_all$y * -1
      ggplot() +
        geom_point(data = medialHemi, aes(x=y, y=z, alpha=.1, color = Correlation), show.legend = Legends) +
        #geom_point(data = medialHemi_all, aes(x=y, y=z), alpha=0.07) +
        scale_color_gradient2(low = ColRange[1], mid = "white", high = ColRange[2], limits = c(-1,1)) +
        theme_blank()
    }
  } else if (View == "Medial Left") {
    if (substring(ROI,1,1) == "L") {
      oneHemi <- Data[grep("L", Data$Hemisphere), ]
      oneHemi$Medial <- ifelse(oneHemi$x > -20, "Medial", "Other")
      medialHemi <- oneHemi[which(oneHemi$Medial=="Medial"), ]
      oneHemi_all <- labelCoords_vertex[grep("R",labelCoords_vertex$Hemisphere), ] # whole surface
      oneHemi_all$Medial <- ifelse(oneHemi_all$x < 20, "Medial", "Other")
      medialHemi_all <- oneHemi_all[grep("Medial", oneHemi_all$Medial), ]
      ggplot() +
        geom_point(data = medialHemi, aes(x=y, y=z, alpha=.1, color = Correlation), show.legend = Legends) +
        #geom_point(data = medialHemi_all, aes(x=y, y=z), alpha=0.07) +
        scale_color_gradient2(low = ColRange[1], mid = "white", high = ColRange[2], limits = c(-1,1)) +
        geom_nodes(data = ROIvertices, aes(x=y, y=z), alpha=.5) +
        geom_nodelabel_repel(aes(x=labelCoord$y, y=labelCoord$z, label = sub("_ROI","",ROI))) +
        theme_blank()
    } else {
      oneHemi <- Data[grep("L", Data$Hemisphere), ]
      oneHemi$Medial <- ifelse(oneHemi$x > -20, "Medial", "Other")
      medialHemi <- oneHemi[which(oneHemi$Medial=="Medial"), ]
      oneHemi_all <- labelCoords_vertex[grep("R",labelCoords_vertex$Hemisphere), ] # whole surface
      oneHemi_all$Medial <- ifelse(oneHemi_all$x < 20, "Medial", "Other")
      medialHemi_all <- oneHemi_all[grep("Medial", oneHemi_all$Medial), ]
      ggplot() +
        geom_point(data = medialHemi, aes(x=y, y=z, alpha=.1, color = Correlation), show.legend = Legends) +
        #geom_point(data = medialHemi_all, aes(x=y, y=z), alpha=0.07) +
        scale_color_gradient2(low = ColRange[1], mid = "white", high = ColRange[2], limits = c(-1,1)) +
        theme_blank()
    }
  }
  
}


## Run a fastgreedy modularity community detection on ROIs
# This function relies on having the timeSeries data uploaded, and labelCoords_vertex 
# Extras dictates whether the community object + correlation matrix should also be extracted
communityDetection <- function(Data = parcelBins$First, ROIS = "None", Type = "vertex", thresh = F, extras = F) {
  
  print(paste('Computing modularity based on', Type))
  
  if (ROIS == "None"){
    
    # This will just do SP for now, for sliding window
    print("Previously concatenated data")
    
    corrMat <- cor(t(Data))
    corrMatrix <- corrMat
    transfMat <- fisherTanh(Data = corrMat)
    if (thresh == T) {
      transfMat$tanhZ[transfMat$adjustPvals > 0.05] <- 0
    }
    corrMat <- transfMat$tanhZ
    
    diag(transfMat$tanhZ) <- 0    
    diag(corrMat) <- 0
    
    corrMat <- exp(corrMat)
    corrMat[corrMat==1] <- 0
    transfMat$tanhZ <- corrMat
    
    tempGraph <- graph_from_adjacency_matrix(corrMat, weighted = T, mode = "undirected")
    tempLap <- laplacian_matrix(tempGraph, normalized=T)
    tempEigen <- eigen(tempLap)
    f.vec <- length(tempEigen$values) - 1
    tempEigen$binarized <-  as.factor(ifelse(tempEigen$vectors[,f.vec] > 0, 1, 0)) # binarized Fiedler Vector 
    
    summary <- data.frame(Label = colnames(corrMat),
                          Hemisphere = substring(colnames(corrMat),1,1),
                          EigenVal = tempEigen$values,
                          FiedlerVec = tempEigen$vectors[, (length(tempEigen$values) - 1)],
                          FiedlerBinary = tempEigen$binarized)
    
    ## Get the final components
    if (extras == T) {
      modularityResults <- list(CorrMatrix = corrMatrix,
                                TransfMatrix = transfMat,
                                Summary = summary)
    } else {
      modularityResults <- list(Summary = summary)
    }
    
  } else {
    
    if (Type == "vertex") {
      # To store the vertex indices corresponding to the ROIs
      indx <- numeric()
      
      # had to place dashes on each side because grep grabbed strings containing the names (i.e. 47m, a24pr)
      for (ROI in ROIS) {
        indx <- c(indx, grep(ROI, rownames(Data)))  
      }
      
      nVerts <- length(indx)
      ROI_tseries <- Data[indx, ]
      
      # This used to be done with the for loop, but it was too slow. cor() speeds up the process by a lot
      corrMat <- cor(t(ROI_tseries))
      
      # name the dimensions of the matrix according to the surface vertex index
      rownames(corrMat) <- indx
      
      # for storing later
      corrMatrix <- corrMat
      
      # transform to Fisher's (think of thresholding)
      transfMat <- fisherTanh(Data = corrMat)
      
      # Determine if edges should be thresholded or not
      if (thresh == T) {
        transfMat$tanhZ[transfMat$adjustPvals > 0.05] <- 0
      }
      
      # Store Fisher transformed vals for graphing
      corrMat <- transfMat$tanhZ
      
      # diagonals of 1 could be interpreted as self-loops
      diag(transfMat$tanhZ) <- 0    
      diag(corrMat) <- 0
      
      # Exponentiate to preserve distribution while ensuring positive weights
      # I'm keeping corrMat and transfMat$tanhZ separate in case I want to uncorrect transfMat in the future
      corrMat <- exp(corrMat)
      corrMat[corrMat == 1] <- 0
      transfMat$tanhZ <- corrMat
      
      # community detection
      # I initially used the absolute value of the correlation, but the exp preserves the distribution 
      # Next, try using the fisher transform
      
      tempGraph <- graph_from_adjacency_matrix(corrMat, weighted = T, mode = "undirected")
      tempCommunity <- fastgreedy.community(tempGraph)
      
      # community object
      #modularityResults$community <- tempCommunity
      
      # correlation matrix with transformed values
      #modularityResults$corrMat <- corrMatrix
      
      # get coordinate info from selected regions (useful for ggplot)
      summary <- data.frame(Vertex = indx,
                            Label = tempCommunity$names,
                            Membership = tempCommunity$membership,
                            Modularity = tempCommunity$modularity,
                            x = labelCoords_vertex[indx, "x"],
                            y = labelCoords_vertex[indx, "y"],
                            z = labelCoords_vertex[indx, "z"],
                            Hemisphere = substring(tempCommunity$names,1,1))
      
      
    } else if (Type == "parcels") {
      
      # This used to be done with the for loop, but it was too slow. cor() speeds up the process by a lot
      corrMat <- cor(t(Data))
      
      # for storing later
      corrMatrix <- corrMat
      
      # transform to Fisher's (think of thresholding)
      transfMat <- fisherTanh(Data = corrMat)
      
      # Determine if edges should be thresholded or not
      if (thresh == T) {
        transfMat$tanhZ[transfMat$adjustPvals > 0.05] <- 0
      }
      
      # Store Fisher transformed vals for graphing
      corrMat <- transfMat$tanhZ
      
      # diagonals of 1 could be interpreted as self-loops
      diag(transfMat$tanhZ) <- 0
      diag(corrMat) <- 0
      
      # Exponentiate to preserve distribution while ensuring positive weights
      # I'm keeping corrMat and transfMat$tanhZ separate in case I want to uncorrect transfMat in the future
      corrMat <- exp(corrMat)
      corrMat[corrMat==1] <- 0
      transfMat$tanhZ <- corrMat
      
      # community detection
      # I initially used the absolute value of the correlation, but the exp preserves the distribution 
      # Next, try thresholding it by the adjusted p-vals
      tempGraph <- graph_from_adjacency_matrix(corrMat, weighted = T, mode = "undirected")
      tempCommunity <- fastgreedy.community(tempGraph)
      
      # community object
      #modularityResults$community <- tempCommunity
      
      # correlation matrix with transformed values
      #modularityResults$corrMat <- corrMatrix
      
      # get coordinate info from selected regions (useful for ggplot)
      summary <- data.frame(Label = tempCommunity$names,
                            Membership = tempCommunity$membership,
                            Modularity = tempCommunity$modularity,
                            x = labelCoords_parcel[ ,"x"],
                            y = labelCoords_parcel[ ,"y"],
                            z = labelCoords_parcel[ ,"z"],
                            Hemisphere = substring(tempCommunity$names,1,1))
      
    }
    
    
    ## Get the final components
    modularityResults <- list(Community = tempCommunity,
                              CorrMatrix = corrMatrix,
                              TransfMatrix = transfMat,
                              Summary = summary)
    
  }
  
  return(modularityResults)
 
}


## plot the communities from communityDetection
plotCommunities <- function(Data = modularityResults, Hemi = "R", type = "Membership", Legends = T, bground=0.1, Cols = c("aquamarine4", "#D9541A")) {
  # This takes the output from the communityDetection function and plots them on the medial wall (since we're interested in medial now)
  # type is the name of the column to plot
  if (Hemi == "R") {
    # "Medial Right"
    oneHemi <- Data[grep("R", Data$Hemisphere), ] # from the ROIs
    oneHemi$Medial <- ifelse(oneHemi$x < 20, "Medial", "Other")
    medialHemi <- oneHemi[grep("Medial", oneHemi$Medial), ]
    medialHemi$y <- medialHemi$y * -1
    oneHemi_all <- labelCoords_vertex[grep("R",labelCoords_vertex$Hemisphere), ] # whole surface
    oneHemi_all$Medial <- ifelse(oneHemi_all$x < 20, "Medial", "Other")
    medialHemi_all <- oneHemi_all[grep("Medial", oneHemi_all$Medial), ]
    medialHemi_all$y <- medialHemi_all$y * -1
    ggplot() + 
      geom_point(data = medialHemi, aes_string(x="y", y="z", color = type), show.legend=Legends) +
      geom_point(data = medialHemi_all, aes(x=y, y=z), alpha=bground) +
      scale_color_gradient(low = Cols[1], high = Cols[2]) +
      theme_blank() 
  } else if (Hemi == "L") {
    # "Medial Left"
    oneHemi <- Data[grep("L", Data$Hemisphere), ]
    oneHemi$Medial <- ifelse(oneHemi$x > -20, "Medial", "Other")
    medialHemi <- oneHemi[which(oneHemi$Medial=="Medial"), ]
    oneHemi_all <- labelCoords_vertex[grep("L",labelCoords_vertex$Hemisphere), ] # whole surface
    oneHemi_all$Medial <- ifelse(oneHemi_all$x > -20, "Medial", "Other")
    medialHemi_all <- oneHemi_all[grep("Medial", oneHemi_all$Medial), ]
    ggplot() + 
      geom_point(data = medialHemi, aes_string(x="y", y="z", color = type), show.legend=Legends) +
      geom_point(data = medialHemi_all, aes(x=y, y=z), alpha=bground) +
      scale_color_gradient(low = Cols[1], high = Cols[2]) +
      theme_blank() 
  }
}


## get ROI coords & index
# The point here is to reduce the summary dframes from community detection to show only ROIs
# Should work for extracting any label-indexed dframe though
# I wanted to also get the index in case I want to extract specific rows from parcel/vertex coord dframes
getCoords <- function(Labels = DMN_labels, Coords = labelCoords_parcel, TimeSeries = FALSE){
  
  indx <- numeric()
  
  # If you want to select time series from raw data
  if (TimeSeries == TRUE) {
    
    for (ROI in Labels) {
      indx <- c(indx, grep(ROI, rownames(Coords)))
    }
    
  } else { # for the summary output of the community detection output
    
    for (ROI in Labels) {
      indx <- c(indx, grep(ROI, Coords$Label))
    }
    
  }
  
  results <- list()
  results$Index <- indx
  results$Coords <- Coords[indx, ]
  
  return(results)
  
}


# eigen value community detection
eigenVals <- function(Data = binnedCommunities_p$First) {
  
  # This will produce a vector of values, ready to be plotted
  tempGraph <- graph_from_adjacency_matrix(Data$TransfMatrix$tanhZ, mode = "undirected", weighted = T)
  tempGraph <- laplacian_matrix(tempGraph, normalized=T)
  tempGraph <- eigen(tempGraph)
  f.vec <- length(tempGraph$values) - 1
  tempGraph$binarized <-  as.factor(ifelse(tempGraph$vectors[,f.vec] > 0, 1, 0)) # binarized Fiedler Vector
  
  return(tempGraph)
  
}


# Attempt at setting up data for confusion matrices and Jaccard index calculations
confusionMatrix <- function(partition1 = parcelCommunities[[1]]$Membership, partition2 = parcelCommunities[[1]]$FiedlerBinary) {
  
  # Divide partitions to evaluate
  # Usually 2 will be fiedler
  
  # Check if any partition has 0s (since I binarize the Fiedler vector)
  if (0 %in% partition1) {
    
    partition1[grep(0, partition1)] <- 1
    partition1[grep(1, partition1)] <- 2
    
  } 
  
  if (0 %in% partition2) {
    
    partition2[grep(1, partition2)] <- 2   
    partition2[grep(0, partition2)] <- 1
    
  }
  
  # Community sizes
  commSizes1 <- table(partition1)
  commSizes2 <- table(partition2)
  
  # Number of communities per partition
  nComms1 <- length(commSizes1)
  nComms2 <- length(commSizes2)
  
  # Number of vertices
  n <- length(partition1)
  
  # Putting together elements of the confusion matrix
  confMatrix <- matrix(nrow = nComms1,
                       ncol = nComms2)
  
  for (i in as.numeric(case.names(commSizes1))) {
    
    # Vertices belonging to community qX of partition X
    tempComm1 <- partition1 == i
    
    for (j in as.numeric(case.names(commSizes2))) {
      
      # Vertices belonging to community qY of partition Y
      tempComm2 <- partition2 == j
      
      # Populate matrix
      confMatrix[i,j] <- sum(tempComm1 & tempComm2) 
      
    }
  }
  
  if (sum(confMatrix) != n) {warning('Sum of the confusion matrix is not equal to number of vertices')}
  
  
  return(confMatrix)
  
}


# Permutation for 2 groups
permute <- function(group1 = 1, group2 = 2, statType = mean, nPerms = 5000, paired = FALSE){
  
  # prep data
  summaryPerm <- list()
  lOne <- length(group1)
  lTwo <- length(group2)
  bigSample <- c(group1,group2)  
  
  if (paired == FALSE) {
    
    
    for (i in 1:nPerms){
      
      # relabel samples
      tempBig <- sample(bigSample)
      tempOne <- tempBig[seq(lOne)]
      tempTwo <- tempBig[-seq(lOne)]
      
      # stats
      tempDiffs <- statType(tempOne,na.rm=T) - statType(tempTwo,na.rm=T)
      summaryPerm$jointDist[i] <- tempDiffs # statType(tempDiffs, na.rm = T) 
      
    }  
    
  } else {
    
    for (i in 1:nPerms){
      
      # shift labels in a pairwise fashion
      tempDiffs <- statType((-1)^rbinom(lOne,1,0.5) * (group1 - group2))
      summaryPerm$jointDist[i] <- tempDiffs
      
    }
    
  }
  
  # get the observed difference
  diffs <- statType(group1,na.rm=T) - statType(group2,na.rm=T)
  observedAbs <- abs(diffs) # maybe leave it as means here
  observed <- diffs
  summaryPerm$Pval <- 2 * (1 - ecdf(summaryPerm$jointDist)(observedAbs))
  if (length(unique(abs(summaryPerm$jointDist))) == 1) {summaryPerm$Pval <- 1} # if the difference is always the same, then p = 1
  summaryPerm$Observed <- observed
  
  return(summaryPerm)
  
}


# Non-parametric Bootstrap for a single group
bootstrap <- function(group = 1, statType = mean, B = 5000){
  
  # prep param
  bootStats <- rep(0,B)
  
  # iterate
  for(b in 1:B){
    
    # wait group
    x <- sample(group,length(group),replace=T)  
    bootStats[b] <- statType(x,na.rm = T)
    
  }
  
  return(bootStats)
  
}


# number of times a node changes affiliation in a time series
flexibility <- function(Data = cbind(1:10, 6:15)) {
  # Calculation of flexibility based on Garcia et al., 2018  
  # This function takes in a data frame or matrix in which columns are the community affiliation 
  # It needs at least 2 colums, although it's pointless for that  
  
  # How many jumps can there be?
  nJumps <- dim(Data)[2] - 1
  
  # Matrix to store jumps
  jumpCount <- matrix(nrow = dim(Data)[1], ncol = nJumps)
  
  # For each transition, get the which nodes changed affiliation
  for (jump in seq(nJumps)) {
    
    jumpCount[, jump] <- Data[, jump] != Data[, jump + 1]
    
  }
  
  # How many times did every node jump?
  totalJumps <- rowSums(jumpCount)
  
  # Calculate the flexibility per node
  flexibility <- totalJumps / nJumps
  
  return(flexibility)
  
}


# Sliding window
slidingWindow <- function(subjTS = ROI_timeSeries[[1]], mins = 15, jump = 1, Spectral = T, Modularity = F, ROIs = c("_7m_", vmPFC_labels)) {
  # This is a fairly specific function. It takes the time series from a participant and preps/runs community detection at each specified time window. 
  # Returns the summaries for each window (based on communityDetection function)
  # The selection of a window size is based on the fact that a TR = 0.720s and an hour is 5000 TRs (HCP-based)
  # Since each subject has a different amount of time points, and all are slightly under 1 hr, I chose to round down the number of window moves to avoid unevenness
  # 
  # Parameter definitions:
  #   
  #   subjTS: Subject time series. Note: if you want to look at more than vmpfc and 7m, add a parameter for the labels that can be passed to the community detection function
  # 
  #   mins: Size of the window that will slide through the data.
  #   
  #   jump: Steps (in mins) advanced per slide
  #   
  #   Spectral: Whether to compute spectral partitioning as well. Takes significantly longer, but might be more useful for bisections
  #
  # Right now this takes ~27 mins per subject to run. Think of ways to improve that.
  
  
  # Adapted so it works with the output from getCoords
  # One wouldn't really apply a full-brain analysis of this sort anyways, too computationally intensive
  indx <- subjTS$Index
  subjTS <- subjTS$Coords
  
  # Window sizing (length)
  # Think about incorporating custom TRs
  TS <- dim(subjTS)[2] # time series for the subject
  WS <- seq(834 * (mins/10)) # window from the first TR up to 834 (~10 mins) times the desired multiplier
  jump <- 84 * jump # 84 ~ 1 min, times the number of mins that the window moves
  nJumps <- floor((TS - length(WS)) / jump) # number of jumps to be performed, based on the selected parameters
  
  ##------- using lapply
  winData <- mclapply(seq(nJumps), function(x) subjTS[,WS+(jump*(x-1))])
  commTS <- mclapply(winData, communityDetection, ROIS = "None", Type = "vertex", thresh = T, extras = F)
  return(commTS)
  
}


# compare sliding window data to overall communities
slideCompare <- function(subjData = slideCommunities[[1]], template = vmpfc7mCommunities[[1]], func = "RI", comm = "Spectral") {
  # This function compares the community partition from each window slide to the one derived from the whole data set
  # 
  # Parameter definitions:
  #   
  #   subjData: A participant's output from slideCommunities
  #   
  #   template: The partition from the whole time series
  # 
  #   func: Which function to use for comparing ("RI" for adjusted RI, "VI" for variation of information)
  # 
  #   comm: The subject data might contain modularity and spectral partitions. Choose which to use.
  
  # For storing RIs or VIs
  tempComparison <- numeric()
  
  # How many jumps does the original data contain?
  nJumps <- length(subjData)
  
  # This loop is technically backwards. I should technically divide by function, then partition method, and then run the window comparisons
  # It's still really fast, so I won't worry.
  for (Win in seq(nJumps)) {
    if (comm == "Modularity") {
      if (func == "RI") {
        tempComparison[Win] <- arandi(subjData[[Win]]$Membership, template$Membership, adjust = T)
      } else if (func == "VI") {
        tempComparison[Win] <- vi.dist(subjData[[Win]]$Membership, template$Membership)
      }
    } else if (comm == "Spectral") {
      if (func == "RI") {
        tempComparison[Win] <- arandi(subjData[[Win]]$FiedlerBinary, template$FiedlerBinary, adjust = T)
      } else if (func == "VI") {
        tempComparison[Win] <- vi.dist(subjData[[Win]]$FiedlerBinary, template$FiedlerBinary)
      }
    }
  }
  
  return(tempComparison)
  
}

# Ensure that all spectral communities associated with 7m (i.e. DMN) have the same label value of 1 
evenSpectral <- function(Data = slideCommunities[[1]][[7]]) {
  
  # Get only the values for 7m
  shortData <- Data[grep("_7m_", Data$Label), ]
  
  # Get 7m's most probable affiliation
  affil <- mean(as.numeric(shortData$FiedlerBinary)-1)
  
  # If it is close to 1, then invert the labeling. 
  # Note: This has no effect on RI or VI, since they are insensitive to actual labeling
  # This is for visualization purposes only
  if (affil < 0.5) {
    
    UD <- as.numeric(Data$FiedlerBinary) - 1
    Data$FiedlerBinary <- as.factor((UD - 1)^2)
    Data$FiedlerVec <- Data$FiedlerVec * -1
    
  }
  
  return(Data)
  
}

# Create a vector ready to be used for HCP data (32k CIFTI surface)
# The input should be the summary from community partitioning
# Once this is created, go to the terminal and input something like this
# wb_command -cifti-convert -from-text dataforCifti.txt 100307.MyelinMap_BC.32k_fs_LR.dscalar.nii testCifti.dscalar.nii
# Where the myelin file here is just a templace. It can be any dscalar.nii with the right surface size
HCPOut <- function(Data = dmnval7mCommunities[[1]], MOI = "Membership", SubjID = "100307", padding = 0){
  
  nVertices <- 59412
  tempVec <- rep(padding, nVertices)
  temp <- grep(MOI, colnames(Data))
  tempVec[Data$Vertex] <- Data[[temp]]
  write.table(file = paste(SubjID,"_",MOI,'_dataforCifti.txt', sep=""), tempVec, row.names = F, col.names = F, dec = ".")
  
}

# Perform pairwise comparisons of clustering outcomes on all subjects
comparePartitions <- function(Data = dmnval7mCommunities, MOI = "FiedlerBinary", Index = "VI", nSubjects = nSubj, subjNames = subjList) {
  # This function will compare the community partitions from all subjects and create a 'comparison matrix' for every pairwise combination of subjects
  # Alternatively, if a second MOI is added 
  # Inputs
  # 
  # Data: the list of summaries produced by the script
  # 
  # MOI: measure of interest (usually the binarized Fiedler vector). If a vector, compares across algorithms per subject
  # 
  # Index: VI for variation of information, RI for the adjusted rand index, Cor for a Pearson correlation
  # ARI can yield many decimals, thus inflating the ratios. I thus round at 4 decimals.
  
  # NOTES:
  # you can apply pairwise comparisons with outer: outer(tempList,tempList,FUN = Vectorize(arandi, SIMPLIFY = FALSE, USE.NAMES = FALSE))
  
  # Get the column position of the MOI(s)
  Columns <- colnames(Data[[1]])
  indx <- as.numeric(Columns %in% MOI)
  MOI_indx <- which(indx==1)
  
  if (length(MOI_indx) < 2) {
    # Combine the measures of interest
    allVecs <- do.call(cbind, lapply(Data, "[[", MOI))
    
    # Create empty matrix
    indexMatrix <- matrix(data = NA, nrow = nSubjects, ncol = nSubjects)
    dimnames(indexMatrix) <- list(subjNames, subjNames)
    
    # Run every pairwise comparison with the index of interest on the measure of interest
    if (Index == "VI") {
      for (subj in seq(nSubjects)) {
        for (subj2 in seq(nSubjects)) {
          indexMatrix[subj,subj2] <- vi.dist(allVecs[, subj], allVecs[, subj2])
        }
      }
    } else if (Index == "RI") {
      for (subj in seq(nSubjects)) {
        for (subj2 in seq(nSubjects)) {
          indexMatrix[subj,subj2] <- arandi(allVecs[, subj], allVecs[, subj2], adjust = T)
        }
      }
    } else if (Index == "Cor") {
      for (subj in seq(nSubjects)) {
        for (subj2 in seq(nSubjects)) {
          indexMatrix[subj,subj2] <- cor(allVecs[, subj], allVecs[, subj2])
        }
      }
    }
    
    indexMatrix <- round(indexMatrix, digits = 4)
    
  } else {
    indexMatrix <- data.frame(SubjID = as.character(subjList),
                              Index = rep(0, nSubjects))
    # Run every pairwise comparison with the index of interest on the measure of interest
    if (Index == "VI") {
      for (subj in seq(nSubjects)) {
        indexMatrix$Index[subj] <- vi.dist(Data[[subj]][, MOI_indx[1]], Data[[subj]][, MOI_indx[2]])
      }
    } else if (Index == "RI") {
      for (subj in seq(nSubjects)) {
        indexMatrix$Index[subj] <- arandi(Data[[subj]][, MOI_indx[1]], Data[[subj]][, MOI_indx[2]], adjust = T)
      }
    } else if (Index == "Cor") {
      for (subj in seq(nSubjects)) {
        indexMatrix$Index[subj] <- cor(Data[[subj]][, MOI_indx[1]], Data[[subj]][, MOI_indx[2]])
      }
    }
    
    indexMatrix$Index <- round(indexMatrix$Index, digits = 4)
  
  }
  
  return(indexMatrix) 
  
}

# Generate a matrix with submatrices of 1s of a given size along the diagonal
# Useful to extract specific portions of data from a similarity matrix
diagBlocks <- function(dims = 20, sqSize = 4) {
  div <- dims %/% sqSize
  finalMatrix<-matrix(0,nrow=dims,ncol=dims)
  index <- 1
  for(k in seq(div)){
    finalMatrix[index:(index + (sqSize-1)),index:(index + (sqSize-1))] <- 1
    index<-index + sqSize
  }
  finalMatrix <- finalMatrix == 1
  finalMatrix
}

# This function will select a two-tailed fiedler vector threshold with respect to the stability of the vertex
# The threshold will be based on proportion DMN given by thresholdRange
threshFV <- function(Data, thresholdRange = c(0.01, 0.99), nthresh = 50) {

  ## Check if the proportions even go that low, otherwise replace by next most stable low/high value
  # get the 2 values with highest reports (this should be stable upper/lower proportions)
  stableExtremes <- sort(table(Data$slidePropDMN), decreasing = T)[seq(2)]
  stableExtremes <- sort(as.numeric(names(stableExtremes)))
  
  
  # if the values are not extreme enough, replace the original threshold with the new stable ones
  if (thresholdRange[1] < stableExtremes[1]) {
    
    thresholdRange[1] <- stableExtremes[1]
    
  }
  
  if (thresholdRange[2] > stableExtremes[2]) {
    
    thresholdRange[2] <- stableExtremes[2]
    
  }
  
  ## Run the actual process
  # placeholder to be returned
  thresholds <- c(NA,NA)
  
  
  # start the counter at chance
  meanProp <- 0.5
  
  
  # select a range of FV values
  thresh <- seq(0, min(Data$FiedlerVec), length.out = nthresh)
  
  
  # Iterate over the possible FV until DMN affiliations go below 1%
  count <- 1
  while (meanProp >= thresholdRange[1]) {
    
    # get the mean proportion of DMN from remaining vertices based on putative threshold
    meanProp <- Data %>% filter(FiedlerVec <= thresh[count]) %>% summarize(mean(slidePropDMN))
    
    # if a threshold meets requirement, store its value
    if (meanProp <= thresholdRange[1]) {
      thresholds[1] <- thresh[count]
    }

    count <- count + 1
    
  }
  
  
  # repeat for the upper threshold
  # restart parameters
  meanProp <- 0.5
  thresh <- seq(0, max(abs(Data$FiedlerVec)), length.out = nthresh)
  
  
  # Iterate over the possible FV until DMN affiliations go over 99%
  count <- 1
  while (meanProp <= thresholdRange[2]) {
    
    # get the mean proportion of DMN from remaining vertices based on putative threshold
    meanProp <- Data %>% filter(FiedlerVec > thresh[count]) %>% summarize(mean(slidePropDMN))
    
    # if a threshold meets requirement, store its value
    if (meanProp >= thresholdRange[2]) {
      thresholds[2] <- thresh[count]
    }

    count <- count + 1
    
  }
  
  
  return(round(thresholds, digits = 5))

  
}
```
  
``` {r For HCP data, fig.align="center", echo=FALSE, message = FALSE}
setwd('./For_HCP')
temp <- list.files(pattern = '*Binary*')
FB <- lapply(temp, read.table)
Summed <- do.call(cbind, lapply(FB, "[[", 'V1')) - 1
FB_Means <- rowMeans(Summed)
nSubj <- length(temp)
rm(temp)
```

1. Department of Psychological and Brain Sciences, Boston University, Boston, USA

Corresponding author:

\newpage

## Abstract 

  There is extensive topographic overlap between cortical regions associated with the default-mode network (DMN) and regions that exhibit effects of subjective value during decision making. The two sets of functional effects involve common regions of medial prefrontal cortex (mPFC) and posterior cingulate cortex (PCC). At the same time, differences between the two sets of activation patterns are not apparent at the aggregated meta-analytic level, and further individual-level functional subdivisions might be obscured by group averaging. Here we used cortical surface-based meta-analysis to identify a parcel in human PCC that was preferentially implicated in DMN effects. We then used resting-state fMRI data and a data-driven network analysis algorithm, spectral partitioning, to divide mPFC/PCC into distinct cortical communities in individual participants (n = 100 from the Human Connectome Project). We labeled the communities "DMN" and "non-DMN" on the basis of their overlap with the meta-analytically identified PCC region. The spectral partitioning algorithm efficiently identified individual-level cortical subdivisions that were reliable over time and across test-retest days using moderate amounts of fMRI data (30 min per day). The topography of the identified subdivisions differed across individuals, and the degree of inter-individual variability was greater in mPFC than in PCC. Our results will aid future task-based fMRI studies in identifying functionally specific cortical subdivisions, and suggest it would be premature to infer that different task manipulations call upon shared cognitive processes solely because they evoke overlapping activation in mPFC at the group or meta-analytic level. 

**Keywords:** brain networks; DMN; individual differences

\newpage

## Introduction

  Human medial prefrontal cortex (mPFC) and posterior cingulate cortex (PCC) are jointly associated with a large set of disparate cognitive functions [@Hiser2018; @Kragel2018], including the subjective value (SV) assigned to decision outcomes [@Bartra2013; @Clithero2014; @Hiser2018; @Kable2007; @Levy2011]. Choice-related activity in these areas has shown significant spatial overlap with the default mode network (DMN) along the medial wall [@Fox2005; @Laird2009; @Yeo2011], prompting the question of whether (and to what extent) these two systems can be dissociated.

  Previous coordinate-based meta-analyses have identified topographically similar activation patterns in mPFC for valuation [@Bartra2013; @Clithero2014] and default-mode task deactivation [@Laird2009]. More focused meta-analytic work aimed at disentangling DMN from subjective value has determined that these systems are inseparable within mPFC [@Acikalin2017], even though there is clear divergence in other areas (ventral striatum in favor of SV, with DMN exclusively on sections of PCC and the temporoparietal junction). These findings highlight the multifaceted embedding of psychological constructs in subregions of mPFC, and suggest that DMN and valuation might indeed be subserved by the same neural system. 
  
  A key limitation of group-averaging and meta-analysis is that individual-specific functionl subdivisions can be overlooked. Averaging has traditionally been beneficial in identifying tendencies in brain function and organization when faced with short scanning sessions per subject. However, it has been shown that mPFC is subject to considerable idiosyncratic cortical folding [@Zilles2013; @Mackey2014] and inter-subject functional variability [@Mueller2013a], thus making group-averaging studies difficult to interpret. 
  
  Seeking to remedy these shortcomings, recent work has prescribed relevance to the analysis of single subjects in fMRI. Individual analyses of fMRI data have identified idiosyncratic, reliable, and valid functional organization that would otherwise be blurred in aggregative estimations [@Laumann2015; @Gordon2017; @Gratton2018a; @Tobyne2018]. Furthermore, subject-specific network arrangements have been found to predict behavioral characteristics [@Kong2018]. In regards to DMN, the trend of individualized analyses has led to finer idiosyncratic subdivisions of the DMN through careful selection of seed-based functional connectivity [@Braga2017]. It is thus possible that the indistinguishable overlap of DMN and valuation effects can be attributed to a lack of spatial resolution that is better understood at the individual level. With this in mind, an important first step in disentangling these phenomena would be to determine the degree of topological heterogeneity of the DMN present within mPFC.

  Interrogating all sources of covariation through functional connectivity provides an avenue for capturing subject-specific brain organization. Connectome-based analyses of resting state functional connectivity (rsFC) have been fruitful in characterizing individualized functional topologies that match task-induced activity [@Gordon2017; @Laumann2015; @Tobyne2017]. A popular approach to analyze these connectomes is to rely on graph theoretic methods, which are properly equiped to understand brain network dynamics [@Bassett2018a]. From these, community detection algorithms have been successfully utilized to section brain networks into cohesive substructures [@Garcia2017]. Such communities represent clusters of network nodes that are more connected with each other than with the rest of the network [@Fortunato2016]. Among the algorithms used to estimate these communities, modularity has been widely effective in subsectioning brain networks into multiple groups [@Garcia2017]. However, in attempting to refine the DMN topology at the individual level, dividing these regions into what belongs and does not belong to DMN makes a forced dissection through spectral partitioning (SP) a more valuable option [@Fiedler1975; @Belkin2003]. SP has recently been used to characterize the posterior-anterior functional gradient of the insula using rsfMRI data [@Tian2018]. This method can thus capture a refined group of brain regions that more cohesively share activation patterns during rest.

  In this study, we aimed to subsection regions commonly attributed to both DMN and subjective value into subject-specific DMN and non-DMN partitions, quantifying the degree of topographical heterogeneity present in this network within and across individuals. We did this by capitalizing on the respective strengths of meta-analytic and subject-specific analyses of brain networks. First, we established a search space by selecting parcels from an established brain atlas [@Glasser2016] that considerably overlapped with previously-defined DMN and limbic networks along the medial wall [@Yeo2011]. A meta-analysis of DMN and SV literatures revealed areas in PCC and mPFC that were uniquely related to DMN and SV, respectively (these areas were used as markers to identify the affiliation of each individual's communities). We then produced rsFC networks of all the surface vertices within these regions of interest (ROI) for each of 100 individual resting state dataset from the Human Connectome Project (HCP)[@VanEssen2012], and estimated the extent of DMN coverage through spectral partitioning. Focusing on individual vertices in this search space rather than the parcels (as is usual in brain network analyses) allowed us to finely delineate the topographical extent of each community. We found that these areas of overlap can be partitioned into two communities that topographically vary across subjects, but that follow a comon organizational principle. This partition was in substantial agreement with results from a different community detection algorithm (modularity). A sliding window analysis showed that spectral partitioning allows for a more immediate identification of noisy viertices with unreliable affiliation. Refining the partitions by filtering out these nodes resulted in idiosyncratic partitions that were highly stable across scanning runs per individual, and this pattern was higher for mPFC. We show that partitions produced by spectral partitioning had higher test-retest reliability per subject than common seed-based functional connectivity, without the need of group-drived priors [@Kong2018]. Lastly, we provide evidence that these communities resembled an organizational scheme recently observed in a handful of subjects [@Braga2017]. Our work highlights the usefulness of estimating brain effects at the individual level in these regions, and points to limitations of aggregative methods in determining the degree of neural overlap between DMN and valuation.


## Methods

### *Search space*

For all our analyses, we defined our search space based on the 17 network parcellation proposed by Yeo et al. [-@Yeo2011]. First, We selected vertices on the cortical surface that were contained by the DMN and limbic networks along the medial wall. Next, we overlayed these networks on a parcellated atlas of the human cortical surface (360 regions) [@Glasser2016], and retained parcels that shared a considerable amount of vertices with them (visually inspected). This resulted in 45 regions of interest (ROI) across hemispheres (Table 1). These ROIs were naturally divided into two spatially discontiguous clusters that covered PCC and mPFC, thus facilitating the study of each general region separately.

\bigskip

### *Meta-analysis*

We used a novel approach to cortical surface parcel-based meta-analysis to assess whether individual parcels were differentially associated with valuation or DMN effects. For subjective value, we gathered peak brain coordinates of activity from 198 studies that were associated with positive effects for current or prospective rewards [@Bartra2013]. For DMN, we acquired coordinates from a previous meta-analysis of 80 studies that were related to task-deactivation [@Laird2009]. These coordinates represent the surviving areas post-statistical thresholding from each study. For each study, we created an indicator map in standard space (MNI152, 1 mm resolution) which contained values of 1 in a 10 mm radius sphere around each reported activation peak, and values of 0 elsewhere [@Wager2009]. The indicator map for each study was then projected to a standard cortical mesh (fs_average, 32,000 vertices, projfrac-max from 0 to 1 by 0.25) using FreeSurfer's mri_vol2surf [@Fischl2012] (http://surfer.nmr.mgh.harvard.edu/). Any sphere sections that overlapped with the cortex were mapped to our ROIs, and we tallied how many studies had positive indicator values in each parcel.  

With these lists, we identified areas that were significantly more associated to either literature. We permuted the label of every study (i.e. DMN or valuation) 5000 times while preseving the total number of studies observed in each domain, and on each iteration stored the maximum resulting statistic (across all parcels) from chi-squared tests of proportions that compared the number of studies that contained each area. This gave us a null distribution of 5000 chi-squared values. The 95th percentile of this distribution served as an FWE-corrected significance threshold to evaluate unpermuted chi-squared values.   

### *fMRI Data*

Our fMRI analyses used resting-state fMRI data from the Human Connectome Project [@VanEssen2012] Q6 release (n = `r nSubj`, randomly sampled from the total pool of 469 available subjects). Each subject's data was acquired over two days at Washington University in St. Louis on a Siemens CONNECTOM Skyra MRI scanner (Siemens, Erlangen, Germany). Four resting state runs (TR = 0.720 s, TE = 33.1 ms, FA = 52, multiband factor = 8, 72 slices, 2 mm isotropic voxels) each comprised 1200 time points(14 min 33 s) for a total of 4800 time points. Two runs were acquired on each day, with the phase encoding direction set to left-right for one run and right-left for the other. Only subjects with both left-right and right-left phase encoding were included (i.e. subjects with four rsfMRI sessions). In addition, only datasets with either low motion levels (under 1.5mm) or under 0.5 mm mean framewise displacement (FD) were used.

Data initially underwent the HCP minimally preprocessing pipeline [@Glasser2013], which included considerable gradient nonlinearity correction, motion correction, EPI distortion correction, temporal denoising based on independent components (FIX, [@Griffanti2014; @Salimi-Khorshidi2014]), high-pass filtering (0.0005 Hz threshold), and MNI152-based normalization. Data were further preprocessed using an in-house pipeline described previously [@Tobyne2017]. Steps included linear interpolation across timepoints with over 0.5 mm of framewise displacement, band-pass filtering (allowed frequencies ranged from 0.009 and 0.08 Hz), temporal denoising and high-motion censoring by deletion, as well as mean greyordinate signal regression [@Burgess2016]. Volumes that displayed an FD of over 0.5 mm were considered as spikes and removed from the subject's dataset, and subjects whose spike counts were above 5% of their total data were excluded from analyses. Finally, data acquired on the same day (i.e. left-right and right-left phase encoding session) were temporally demeaned. Each subject's brain was comprised of 32k standard HCP greyordinates per hemisphere (combined in a CIFTI file). We retained only the cortical surfaces, which resulted in 59,412 total surface vertices per subject. 

### *Within-ROI Network Definition*

All network analyses were performed using the igraph package [@G.2006] in R 3.4.1 [@RTeam2018]. To establish each subject's network, we selected all the vertices contained within the ROIs (n = 4,801 per subject) and computed the Pearson correlation of the time series for every pair of vertices. This produced a weighted network for each subject, where surface vertices were the nodes, and edges the correlations among all of them. Next, we applied Fisher's r to z transformation, and performed a two-sided significance test to identify strong correlations. The resulting p-values were corrected for multiple comparisons (FDR < 0.05), and edges with non-significant correlations were turned to 0 (all other edges retained their respective z-values). Next, we took the exponential of these values so that all weights were positive while maintaining the ordinal ranks of the original correlation distribution. Values of 1 of this transformed distribution were returned to 0 in order to avoid the inclusion of these edges in the network analyses. We report the proportion of retained edges (i.e. network density) in the Results section. As part of the evaluation step, we produced these network weight matrices (and the community detection outlined below) at four levels: 1) for each run separately (1200 TRs each); 2) for the concatenated time series for the two runs on the same day (2400 TRs total); 3) for each subject's full dataset (4800 TRs total); and 4) on each step of a sliding window analysis (see Partition Evaluation for more details).

### *Community Detection*

Communities (i.e. clusters) were identified by means of spectral partitioning [@Fiedler1975; @Belkin2003]. First, each network was represented as an $n$ x $n$ network weight matrix (where $n$ = number of vertices within the ROIs, 4,801). This matrix was then transformed into its normalized Laplacian form

$$\begin{aligned}
L = I - D^{-\frac{1}{2}}WD^{-\frac{1}{2}}
\end{aligned}$$

Where $I$ is the identity matrix, $D$ is a diagonal matrix containing the strength of each vertex (i.e. the sum of its correlation values with every other vertex), and $W$ is the network weight matrix. This resulted in a matrix wherein each entry was the normalized value of the connection (from 0 to 1) between any two vertices relative to their combined connectivity strength, and with ones along the diagonal. This transformation ensures that every row adds up to zero. We then computed the eigenvalues and eigenvectors of the normalized Laplacian, and used the eigenvector associated with the second-to-lowest eigenvalue to divide the network into two. This vector (traditionally called the 'Fiedler vector') provides a set of positive and negative values to binarize the network with [@Fiedler1975]. This method enforces the partition of a network into similarly-sized communities, and guarantees that they share at least one edge [@Chung1999]. In this way, spectral partitioning avoids producing communities that are too small to be physiologically meaningful (for example, small sets of vertices that are spuriously correlated due to measurement noise). Every individual partition is therefore binarized by sign in order to evaluate the topography of each community (see next section for details). Given that this data-driven method does not label the two communities or establish correspondence across participants, we defined each individual's "DMN" community to be the one containing the DMN-specific PCC parcel identified in our meta-analysis. The high density of these networks (see Results) ensured that spectral partitioning did not face the issues associated with its use in sparse networks [@Fortunato2016].

In order to evaluate the convergent validity of the resulting partitions across community-detection methods, we also estimated these communities using the more traditional approach of modularity maximization, based on the algorithm from Clauset et al. [-@Clauset2004]. *(TBD) This method maximizes a quality function that compares the strength of the connection between any two vertices against a null model of their probability of being randomly connected to any other vertex in the network. The method heuristically iterates through many possible combinations of vertices, and selects the clustering that maximizes the network's modularity.* Unlike SP, modularity can fractionate a network into more than two functional groups. We assumed that if the partitions from the bounded and unbounded community detection methods showed high levels of agreement, we could gain confidence in that we are indeed fractioning the ROIs into those belonging to DMN and not. We also diagnosed our choice of two communities by reporting the rank (from lowest to largest) of the first eigenvalue produced by spectral partitioning to be non-zero. This eigenvalue serves as an estimator of the optimal number of communities in a network [@Chung1999], such that a rank of two in our partitions justifies limiting the number of communities to two.


### *Partition Evaluation*

In order to evaluate the stability and topographical heterogeneity of the communities within and across subjects, we chose the adjusted rand index (ARI) as the reference metric of agreement between two clusterings [@Hubert1985]. The base of the ARI is computed by the formula

$$\frac{a + b}{a + b + c + d}$$

Where $a$ is the number of pairs of nodes that were grouped together in both partitions, $b$ is the number that were grouped separately, and $c$ and $d$ denote the number grouped together (separately) in one partition, but separately (together) in the other. Therefore, ARI estimates the ratio at which every pair of nodes in partition A were jointly assigned to the same/different parcels on partition B, over all possible relational changes. The resulting ratio is adjusted by comparing it to a baseline given by the expectation of a random node assignment across partitions. This means that even though clustering deviations are heavily penalized, positive ARI values compare favorably against chance clustering (and can take negative values if the index given by the formula above falls below 50%). In short, the ARI determines the chance-corrected percentage of agreement between any two partitions while being agnostic to the labeling scheme.

We performed a number of comparisons among partitions. First, we computed the degree of agreement between SP and modularity per subject to validate the resulting functional patterns. Modularity and SP have been previously found to show overfitting and underfitting tendencies, respectively, in their community detection performance in a diverse number of network types [@Ghasemian2018]. Their alignment would thus give us confidence in the validity of the resulting topologies. Next, we compared the functional organization derived from the whole time series across subjects, and report the mean pairwise ARI for the group. We then performed the same evaluation for PCC and mPFC separately, and examined whether there are mean differences in overall agreement within these regions by performing a paired permutation analysis (5000 iterations) on all pairwise ARI comparisons across subjects. Given that the functional heterogeneity of brain areas is unevenly distributed across the cortex [@Mueller2013a], we performed all the subsequent analyses on PCC and mPFC separately.

To estimate the degree of stability of these partitions per subject we performed a sliding window analysis (20 min windows, 1 min increments, mean number of windows per subject = 37), comparing each window's resulting partition against the partition derived from the whole subject's data. A 20-min window is typically adequate for identifying stable community features in brain networks [@Gordon2017]. We assessed whether the magnitude of the Fiedler vector value for a given vertex (for the full subject-level data set) was associated with the stability of that vertexs sub-network assignment across time windows. To do this, we fit a mixed effects logistic regression model, in which the dependent variable was the proportion of times each vertex participated in the DMN community across windows, and the explanatory variables included a random effect of subject and a fixed effect of the Fiedler vector value for that vertex (derived from their full time series). Based on this relationship, we identified a threshold of Fiedler vector values for each subject, such that vertices above these thresholds were persistently associated with either DMN or non-DMN over 95% of the time.

We then estimated the level of agreement between network partitions estimated using data from individual days (with 2 days per participant). If the functional organization estimated by SP is indeed subject-specific, we should see higher agreement within subject (test/re-test across days) than across subjects. We tested this idea by computing the ratio of the mean ARI within and between subjects. Ratios close to one denoted similar within-subject and across-subject alignment, whereas ratios considerably higher than one suggested that partitions were more similar within-subject than across subjects. We then extended this idea by computing the overall agreement across all runs (4 per subject) Similar to the day-based analysis, we assessed whether session-level data showed higher agreement within-subject than between subjects. 

<!--
### *Estimation of Noise Effects*

BOLD signal recording through fMRI in mPFC in general, and vmPFC in particular, suffers from technical issues that promote signal dropout [@Logothetis]. Even though we made attempts to remove clear sources of noise through data preprocessing and stability analyses, it was still important to quantify the remaining potential influence of structured noise in our findings. We examined this through two complementary steps. First, we computed the temporal signal-to-noise ratio (tSNR) of all the vertices in the ROIs from the HCP minimally preprocessed data (i.e. prior to the complementary denoising described in the "fMRI Data" section). We compared the mean tSNR for each region across subjects through a permutation analysis (all permutation analyses involved 5000 iterations). In addition, we compared the mean tSNR between mPFC and PCC through a permutation analysis for each separate subject, and report the percentage of subjects who had a significantly lower mean tSNR in mPFC (post FDR correction for multiple testing [@Benjamini2001]). 

Next, we examined whether a vertex's affiliation to the DMN community was driven by its tSNR. First, we computed the mean time series of each subject's DMN community. We then performed a mass univariate analysis to predict the average time series from each individual vertice's time series. The resulting $R^2$ values from each model were extracted to determine the total variance of the mean DMN time series explained by each individual vertex. If high noise in more ventral mPFC areas is affecting our results, we should see a gradient of $R^2$ values that decreases ventrally across subjects. We then spatially correlated (Pearson) each subject's $R^2$ and tSNR values to determine whether low tSNR negatively impacted the individual contribution among vertices, and report the mean correlation across subjects. 
-->

### *Seed-based FC vs Community Detection*

We evaluated the performance of the spectral partitioning algorithm in comparison to a simpler partitioning approach using seed-based functional connectivity. First, we estimated each subject's DMN partition in mPFC based on its vertex-wise functional correlation (Pearson) with the average PCC activity for each day separately. We compared these seed-based maps with the Fiedler vectors produced by SP by spatially correlating them across days and subjects in three ways: 1) between seed-based maps; 2) between SP-based maps; and 3) between seed- and SP-based maps. These spatial correlations were aimed to determine the test/re-test reliability of each approach, as well as the overall level of agreement between them. The two methods were expected to produce somewhat similar arrangements, but the one displaying the higher within-subject agreement across days should be preferred (for a discussion on the stability of functional networks see [@Kong2018; @Mueller2013a]). We thus compared the mean within-subject inter-day correlation coefficients of each method through a paired permutation analysis (5000 iterations).   

\bigskip

## Results

### *Meta-analysis*

``` {r Meta-analysis permutation, echo=FALSE, message = FALSE, cache = TRUE}
# This version is literally what I had selected originally, plus 25, OFC, 10v, and RSC
Yeo_labels <- c("L_25_ROI",
              "L_OFC_ROI",
              "L_10v_ROI",
              "R_25_ROI",
              "R_OFC_ROI",
              "R_10v_ROI",
              "L_s32_ROI",
              "L_RSC_ROI",
              "R_RSC_ROI",
              "R_23d_ROI",
              "R_d23ab_ROI",
              "R_31a_ROI",
              "R_31pv_ROI",
              "R_31pd_ROI",
              "R_7m_ROI",
              "R_v23ab_ROI",
              "R_p24_ROI",
              "R_d32_ROI",
              "R_9m_ROI",
              "R_p32_ROI",
              "R_a24_ROI",
              "R_10r_ROI",
              "R_10d_ROI",
              "L_23d_ROI",
              "L_d23ab_RO",
              "L_31a_ROI",
              "L_31pv_ROI",
              "L_31pd_ROI",
              "L_7m_ROI",
              "L_v23ab_ROI",
              "L_p24_ROI",
              "L_d32_ROI",
              "L_9m_ROI",
              "L_p32_ROI",
              "L_a24_ROI",
              "L_10r_ROI",
              "L_10d_ROI",
              "R_s32_ROI",
              "R_9a_ROI",
              "L_PCV_ROI") 

# Permutation analysis
# Note: this doesn't care whether a region was uni or bilaterally activated. Maybe do that.
# Load the coordinates as studies in a list, because for the contrast the studies are permuted
# Use table(unlist(list)) to get the counts in that case
setwd('./POS')
templist <- dir()
studyList_sv <- lapply(templist, readr::read_csv, col_names = F, col_types = cols())

setwd('../DMN/')
templist <- dir()
studyList_dmn <- lapply(templist, readr::read_csv, col_names = F, col_types = cols())

# raw counts for each area per literature
nRegions_sv <- do.call(rbind, studyList_sv) %>% count
nRegions_dmn <- do.call(rbind, studyList_dmn) %>% count
nRegions_all <- left_join(nRegions_sv, nRegions_dmn, by = "X1")
 
# optional because it can take a long time to run
if (permuting) {
  
  # reported statistics are computed @ 5k, but it's computationally demanding to do that every time.
  nPerms <- 1000 
  
  # For each permutation, reassign the study labels, maintaining the original number of studies per condition
  nullDist <- mclapply(seq(nPerms), function(permutation) {metaPerm(perm = permutation)})
  nullDist <- do.call(rbind, nullDist)
  
  # Now do the same for the actual data, to see how it compares to the null dist
  tmpDF <- data.frame(Parcel = gsub("_", "", Yeo_labels),
                      SV = sapply(Yeo_labels, function(roi) {length(grep(roi, studyList_sv))}),
                      DMN = sapply(Yeo_labels, function(roi) {length(grep(roi, studyList_dmn))}),
                      propSV = sapply(Yeo_labels, function(roi) {length(grep(roi, studyList_sv))}) / length(studyList_sv),
                      propDMN = sapply(Yeo_labels, function(roi) {length(grep(roi, studyList_dmn))}) / length(studyList_dmn))
  
  # Perform the prop test for each pair and get the max chi-squared stat + pvalue based on the computed null distribution
  tmpDF$Chi_squared <- apply(tmpDF[, 2:3], 1, function(data) {
    round(prop.test(data, c(length(studyList_sv), length(studyList_dmn)))$statistic, digits = 2)
    })
  tmpDF$Pval <- 1 - ecdf(nullDist)(tmpDF$Chi_squared) # probability of finding a chi-squared stat at least as large as the observed under a permuted null distribution
  
  # ROIs that survived the threshold based on Chi-squared value (>95th quantile from null distribution -> 8.43)
  significantROIs <- tmpDF %>% filter(Chi_squared >= quantile(nullDist, 0.95))
  
}
# results!
# 25 bilateral (prportion sv = L: 0.42, R: 0.39; proportion DMN = L: 0.18, R: 0.16, Chi = L: 12.91, R: 12.83, both p = 0.01)
# 7m bilateral (prportion sv = L: 0.17, R: 0.15; proportion DMN = L: 0.36, R: 0.40, Chi = L: 10.07, R: 18.89, L: p < 0.05, R: p < 0.0001)
# v23 right (proportion sv = 0.16; proportion DMN = 0.36, chi = 11.51, p = 0.01)
# null 95th-q chi <- 9.11 (varies; this was done ith 100 but resembles the 5k one)

```


``` {r ROI computation (overlapping: DELETE), echo=FALSE, message = FALSE}
#TO DO: CHANGE THIS SO IT'S BASED ON STUDYLIST_SV/DMN, COUNTING JUST LIKE IN THE PERMUTATION ABOVE
## ROI definition
# First, load the raw count csv files
valCounts <- read.csv2('decCounts.csv', sep = ",")
dmnCounts <- read.csv2('dmnCounts.csv', sep = ",")

# Then, get the means of the counts per domain, both within/outside the search space
# This is to get a broad estimate of the average report amounts in the literature
tempBoot <- bootstrap(valCounts$Counts)
BV <- round(mean(tempBoot))
BVquant <- round(quantile(tempBoot, c(0.025, 0.975)), digits = 2)
tempBoot <- bootstrap(dmnCounts$Counts)
BD <- round(mean(tempBoot))
BDquant <- round(quantile(tempBoot, c(0.025, 0.975)), digits = 2)
rm(tempBoot)

# Now select only the ROIs from Yeo et al (TEMP)
valCounts <- getCoords(Labels = Yeo_labels, Coords = valCounts)$Coords
dmnCounts <- getCoords(Labels = Yeo_labels, Coords = dmnCounts)$Coords

# Now, for every ROI, do a prop test to check if the proportion of reports is significantly greater
# than the mean report over the number of studies (i.e. sample-based null probability, which could be changed to perm)
for (i in seq(valCounts$Counts)) {
  indx <- valCounts$Counts[i]
  tempTest <- prop.test(indx, 27, alternative = "greater", p = BV/27)
  valCounts$Chi[i] <- tempTest$statistic
  valCounts$pVal[i] <- tempTest$p.value
}
for (i in seq(dmnCounts$Counts)) {
 indx <- dmnCounts$Counts[i]
 tempTest <- prop.test(indx, 77, alternative = "greater", p = BD/77)
 dmnCounts$Chi[i] <- tempTest$statistic
 dmnCounts$pVal[i] <- tempTest$p.value
}
# Retain only the significant parcels
t1 <- valCounts$Label[valCounts$pVal < 0.05]
t2 <- dmnCounts$Label[dmnCounts$pVal < 0.05]
# Remove hemispheric references
t1 <- unique(substring(t1,2))
t2 <- unique(substring(t2,2))
# Labels to put in text
dmnLbls <- gsub("ROI","", t2)
dmnLbls <- gsub("_","", dmnLbls)
valLbls <- gsub("ROI","", t1)
valLbls <- gsub("_","", valLbls)

#bothLbls <- dmnLbls[dmnLbls %in% valLbls] # This part is just for reference, as the labels are loaded from a file
# final overlap
# And get the set of overlapping areas between literatures
# This is missing some cingulate and PFC areas, and adds V2.
# However, we can say that we are filling in the gap for completeness,
# and removing V2 because it's demonstrably irrelevant.
#bothLbls <- unique(c(t1[t1 %in% t2], t2[t2 %in% t1]))
# You could also try to permute the allocation of reports per literature,
# and then perform a chi-squared per parcel. Get the max stat per iteration.
# Now you have your null distribution.

```

We performed a coordinate-based meta-analysis to identify unique standard regions across DMN and SV literatures. Volumetric coordinates from a set of DMN and SV studies were projected onto a cortical surface, and mapped to a multimodal cortical parcellation [@Glasser2016] to produce a list of brain areas reported per study. The 45 areas considered were limited to the medial portion of the default mode and limbic networks, as defined by the Yeo et al.'s [-@Yeo2011] 17 network parcellation. Unique areas were identified by first permuting the affiliation of each study (i.e. DMN or SV) to create a distribution of chi-squared statistics based on the comparison of the number of reports per area between literatures (see Methods for details). This putative null distribution was used to identify regions more often reported in either literature. 

Figure 1 shows the proportion of times every parcel was reported in each literature, as well as the resulting significant ROIs from the permutation. The 95th percentile of the permuted chi-squared distribution was 9.11. Based on this threshold, area 7m was the only one to show unique bilateral affiliation to DMN (Left: observed $\chi^2$ = 10.07, *p* = 0.034; Right: observed $\chi^2$ = 18.89, *p* < 0.0001), while SV was uniquely represented bilaterally by area 25 (Left: observed $\chi^2$ = 12.91, *p* = 0.014; Right: observed $\chi^2$ = 12.83, *p* = 0.014). The purpose of these supra-threshold areas was to identify the most-likely affiliation of the two communities in our partitions by their coverage of these regions. However, area 25 sits in close proximity to the ventral striatum, which is reliably active in SV studies [@Bartra2013]. Considering that our projection from volumetric spheres to the cortex might have attributed striatal reports to this region, we decided not to use it as a marker of SV. We also chose not to use area v23 due to its unilateral significance (Right: observed $\chi^2$ = 11.51, *p* = 0.018). Instead, we only rely on area 7m to demarcate the DMN community.

<!--
Using each literature's mean number of reports per area as the null probability (DMN = `r BD`, 95% CI [`r BDquant`]; SV = `r BV`, 95% CI [`r BVquant`]), one-way chi-squared tests of proportions were performed to identify regions that were significantly present in their respective literatures. The resulting lists were used to identify the set of common brain areas. Unique areas were then determined by permuting the affiliation of each study (i.e. DMN or SV) to create a null distribution of chi-squared statistics, which as used to identify regions more often reported in either literature. Importantly, we chose to examine these areas bilaterally, even if their significance was present in only one hemisphere.
DMN included `r length(t2)` significant regions, while SV was comprised of `r length(t1)` (see Supplemental Table 1). As expected, overlapping areas included 15 bilateral regions in PCC (31a, 31pv, RSC, d23ab) and mPFC (9m, 10r, 10v, OFC, a24, d32, p24, p32, s32), as well as a region in IPL (PF) and TPJ (PGi) (all*p*< 0.05). A region in visual cortex (V2) was also significant, but was removed due to its know unrelated function.
-->

 

### *Partition Comparison: Full Datasets*

``` {r Load Summary Data}
# Load data
setwd('./Summary/')
temp <- list.files()
subjList <- sapply(temp, substring, first=1, last=6)
Summaries <- lapply(temp, read.csv)
nSubj <- length(Summaries)
nVertices <- nrow(Summaries[[1]]) # in case I need the number of vertices included per brain
```

```{r Density calculation, out.width="60%", fig.align = "center"}
# Load density data
setwd('./Densities/')
temp <- list.files()
Densities <- do.call(rbind, lapply(temp, read.csv))

# Get the mean density for all data, daily scans, and sessions
# Check distributions, since median might be better
mdAll <- mean(Densities$Density[grep("Overall", Densities$Type)])
sddAll <- sd(Densities$Density[grep("Overall", Densities$Type)])
mdDay <- mean(Densities$Density[grep("Day", Densities$Type)])
mdSess <- mean(Densities$Density[grep("Session", Densities$Type)])
```

With these `r length(unique(str_remove(Yeo_labels, "_ROI")))` ROIs (`r (length(unique(str_remove(Yeo_labels, "_ROI"))) - 2)` overlapping, 1 DMN-specific, and 1 SV-specific), we estimated each individual's DMN coverage using their full time series (approximately 4800 total TRs from four scanning sessions over two days). Each subject's network (n = `r nSubj`) was constructed by performing pairwise correlations among the time series of all the surface vertices contained within the ROIs (`r nVertices` total vertices). After correcting for multiple comparisons, we performed spectral partitioning on these networks to identify cohesive functional communities denoting DMN and non-DMN areas. On average, networks had high densities (i.e. highly connected) regardless of whether they were produced with a single run all the full time series (mean = `r round(mdAll, digits = 2)`, SD = `r round(sddAll, digits = 3)`). This high degree of density is important, as this method is known to face challenges when computed on sparse networks [@Fortunato2016]. 

Figure 2 shows a stereotypical subdivision of the ROIs for a single subject (100307) using spectral partitioning. This method dissects a network by the positive and negative values derived from the Fiedler vector (the eigenvector related to the second-to-lowest eigenvalue of the network's Laplacian matrix, see Methods), which we used to binarize the ROIs. Since this is a data-driven approach, there is no a priori labeling for these communities. We thus defined the DMN community as the one containing the one DMN-specific area from the meta-analysis (7m), and ensured that the binary labeling of the vertices was consistent across subjects. In general, the resulting pattern shows large DMN coverage in PCC, with non-DMN vertices populating its anterior segment. On the other hand, mPFC is mostly covered by DMN vertices in its ventral-anterior and dorsal-anterior areas, with a persistent non-DMN pocket between them. This non-DMN section extends posteriorly onto pregenual cinglate cortex (area a24). 

Before determining the degree of generazibility of this topological pattern across individuals, we examined the validity of these partitions by comparing them to patterns derived with a modularity algorithm [@Clauset2004]. Modularity seeks to find the pattern of communities that maximizes a quality function that compares the connectivity strength of any two nodes with a null probability model. Since modularity is not bound to a desired number of communities, it was capable of finding more than two in our datset. We quantified the cross-method agreement with the adjusted rand index (ARI). ARI measures the proportion of pairs of nodes in a network that were either clustered together or separately in both partitions, over all possible affiliation changes. This measure has the benefit of being agnostic to labeling schemes, as well as controlling for the expectation that the clusters were randomly produced [@Hubert1985]. This means that positive ARI values compare favorably against chance clustering.

``` {r Partition comparison: all 2, echo=FALSE, message = FALSE, cache = TRUE}
## Community number evaluation (across and within methods)
# ARI between methods
RI_all_within <- comparePartitions(Data = Summaries, Index = "RI", MOI = c("FiedlerBinary", "Membership"), nSubjects = nSubj, subjNames = subjList)

# Number of communities per subject for modularity
nModularity <- sapply(Summaries, function(data) {max(data$Membership)}) # n of estimated communities by modularity

# Median size of the extra (>2) communities estimated by modularity
modularityCounts <- lapply(Summaries, function(data) {sort(count(data$Membership)$freq, decreasing = T)})
modularityCounts_two <- sapply(modularityCounts, function(x) {mean(x[seq(2)], na.rm = T)})
modularityCounts_extra <- sapply(modularityCounts, function(x) {mean(x[3:4], na.rm = T)})

# eigenvalue-based diagnosis of optimal n of communities
prcntDissection <- lapply(Summaries, function(data) {which(data$EigenVal < 0.00001)}) # how many consecutive 0s?
prcntDissection <- mean((sapply(prcntDissection, length) + 1) == 2) * nSubj # % of subjects who only had 1 zero (thus optimal cuts = 2)

## Across subjects
# Note: the upper and lower triangles can totally be combined before plotting, instead of adding one corrplot over another. Might work on that eventually.
RI_all_between <- comparePartitions(Data = Summaries, Index = "RI", nSubjects = length(Summaries), subjNames = subjList)
RI_all_between_lowtri <- RI_all_between[lower.tri(RI_all_between)]

posteriorIndx <- Summaries[[1]]$y < 0
tempComm_PCC <- lapply(Summaries, "[", i = posteriorIndx, j =)
tempComm_PFC <- lapply(Summaries, "[", i = !posteriorIndx, j =)
RI_PFC <- comparePartitions(Data = tempComm_PFC, Index = "RI", nSubjects = length(Summaries), subjNames = subjList)
RI_PCC <- comparePartitions(Data = tempComm_PCC, Index = "RI", nSubjects = length(Summaries), subjNames = subjList)

# Let's compare stuff
testPCC <- RI_PCC[lower.tri(RI_PCC)]
testPFC <- RI_PFC[lower.tri(RI_PFC)]
Overall_PCCvsPFC_permTest <- permute(testPCC, testPFC, statType = mean, paired = T)
Overall_PCCvsPFC_ES <- cohenD(testPCC, testPFC) # caveat: maybe not ideal for bounded values, like RI. BUT VI yields similar results.

# To get the rate of stability per vertex across subjects
FB <- do.call(cbind, lapply(Summaries, "[[", "FiedlerBinary"))
FV <- do.call(cbind, lapply(Summaries, "[[", "FiedlerVec"))
FBS <- rowMeans(FB)
FBS[FBS < 0.5] <- 1 - FBS[FBS < 0.5]

# Now let's test something: 
# Traditionally, a vertex-wise test would be carried across subjects.
# If these traditional analyses miss out on these idiosyncrasies, you should get unsignificant results.
# So we did a prop.test per vertex, and corrected with FDR to mimic this situation.
All_proptests <- apply(FB, 1, function(x) {prop.test(sum(x), nSubj)})
Prop_pvals_corrected <- p.adjust(sapply(All_proptests, "[[", "p.value"), method = "BY")
Prop_pvals_corrected <- ifelse(Prop_pvals_corrected < 0.05, 1, 0)
Chisquares <- sapply(All_proptests, "[[", "statistic")

# And a t-test on the FV vals
All_ttests <- apply(FV, 1, t.test)
Ttest_pvals_corrected <- p.adjust(sapply(All_ttests, "[[", "p.value"), method = "BY")
Ttest_pvals_corrected <- ifelse(Ttest_pvals_corrected < 0.05, 1, 0)
Tstats <- sapply(All_ttests, "[[", "statistic")


### Testing grounds
# this part compares the distribution of Fiedler values for DMN and nonDMN
# note: remove filter(y > 0) for 'full brain'
# First plot compares the median FV between communities across subjects
allSummaries <- do.call(rbind, Summaries)
allSummaries$SubjID <- rep(subjList, each = nVertices) 
ecdfPCC <- allSummaries %>%
          mutate(FV_abs = abs(FiedlerVec),
                 Community = ifelse(FiedlerBinary == 0, "nonDMN", "DMN")) %>%
          filter(y < 0) %>%
          ggplot(aes(FV_abs, group = interaction(SubjID, Community), color = Community)) +
          stat_ecdf(alpha = 0.3) +
          #scale_color_manual(values = c("Yellow", "Red")) +
          xlim(0,0.05) +
          geom_vline(xintercept = 0.009, lty = 2) +
          labs(title = "Per-subject ECDF of Fiedler values: PCC") +
          theme_classic()

ecdfPFC <- allSummaries %>%
          mutate(FV_abs = abs(FiedlerVec),
                 Community = ifelse(FiedlerBinary == 0, "nonDMN", "DMN")) %>%
          filter(y > 0) %>%
          ggplot(aes(FV_abs, group = interaction(SubjID, Community), color = Community)) +
          #scale_color_manual(values = c("Yellow", "Red")) +
          stat_ecdf(alpha = 0.3) +
          xlim(0,0.05) +
          labs(title = "Per-subject ECDF of Fiedler values: PFC") +
          geom_vline(xintercept = 0.009, lty = 2) +
          theme_classic()

ecdfALL <- allSummaries %>%
  mutate(FV_abs = abs(FiedlerVec),
         Community = ifelse(FiedlerBinary == 0, "nonDMN", "DMN")) %>%
  ggplot(aes(FV_abs, group = interaction(SubjID, Community), color = Community)) +
  #scale_color_manual(values = c("Yellow", "Red")) +
  stat_ecdf(alpha = 0.5) +
  xlim(0,0.05) +
  labs(title = "Per-subject ECDF of Fiedler values") +
  geom_vline(xintercept = 0.009, lty = 2) +
  theme_classic()

```

This comparison showed that both clustering methods were in high agreement across subjects (mean = `r round(mean(RI_all_within$Index), digits = 2)`, SD = `r round(sd(RI_all_within$Index), digits = 2)`). Modularity showed a tendency to produce additional communities (median = `r median(nModularity)`, max = `r max(nModularity)`). However, these communities encompassed a small number of vertices (mean = `r mean(modularityCounts_extra, na.rm = T)`, IQR = `r IQR(modularityCounts_extra, na.rm = T)`) compared to the principal two (mean = `r mean(modularityCounts_two, na.rm = T)`, IQR = `r IQR(modularityCounts_two, na.rm = T)`), suggesting that the overall network is composed of two main communities. In addition, the estimation of the optimal number of cuts based on the rank of the first non-zero eigenvalue (see "Partition evaluation" for details) showed that `r prcntDissection` percent of the individual networks are best partitioned into two communities.   

Next, we examined how similar the partitions were across subjects by computing the ARI between every pair of subjects, and found that there was moderate agreement overall (mean = `r round(mean(RI_all_between_lowtri), digits = 2)`, SD = `r round(sd(RI_all_between_lowtri), digits = 2)`). However, qualitative inspection of the brain community topologies showed good alignment for PCC, with a topologically similar mPFC pattern that shifted topographically across subjects. To quantify this heterogeneity in mPFC, we performed the same analysis for each region separately (Figure 3). Indeed, this analysis showed that the functional topology of PCC is better maintained across subjects (mean = `r round(mean(testPCC), digits = 2)`, SD = `r round(sd(testPCC), digits = 2)`) than mPFC (mean = `r round(mean(testPFC), digits = 2)`, SD = `r round(sd(testPFC), digits = 2)`)(paired permutation, *p* = `r Overall_PCCvsPFC_permTest$Pval`; Cohen's D = `r round(Overall_PCCvsPFC_ES, digits = 2)`). 

### *Pattern stability over time*


``` {r Stability per subject: Overall, echo=FALSE, message = FALSE, cache = TRUE}
# Load files
setwd('./Sliding_window/')
temp <- list.files(pattern = '*Comparisons.csv')
SC <- lapply(temp, read.csv2)
temp <- list.files(pattern = '*Values.csv')
SVals <- lapply(temp, read.csv2)

# Do a mixed effects GLM (binomial, quasi unavailable on glmer) to estimate the relationship between Fiedler vec and propDMN
SC_FVvPDMN_model <- glmer(slidePropDMN ~ FiedlerVec + (1 | SubjID), family = "binomial", data = allSummaries)
SC_FVvPDMN_modelSummary <- summary(SC_FVvPDMN_model)
allSummaries$FittedVals <- fitted(SC_FVvPDMN_model)

# Create a smaller dataframe just to plot the fit
# Otherwise, plotting the thousands fitted can place an unnecessary burden on the document
# (at some point, knitting to pdf wouldn't show the logistic plot because of this)
FiedlerVector <- seq(min(allSummaries$FiedlerVec), max(allSummaries$FiedlerVec), length.out = nVertices)
formula <- SC_FVvPDMN_modelSummary$coefficients[1,1] + (SC_FVvPDMN_modelSummary$coefficients[2,1] * FiedlerVector)
Fitted_Proportion <- exp(formula) / (1 + exp(formula))
SC_FVvPDMN_fits <- data.frame(FiedlerVector, Fitted_Proportion)
rm(FiedlerVector, formula, Fitted_Proportion)

# Overall
SC_all <- do.call(rbind, SC)
SC_plot <- ggplot(data = SC_all)
SC_plot <- SC_plot + geom_line(aes(x = Window, y = Index, group = as.factor(SubjID)), color = "slategray4",alpha = 0.3, show.legend = F) +
              geom_smooth(aes(x = Window, y = Index), color = Cols[2], fill = Cols[2]) +
              ylim(0,1) + 
              theme(text = element_text(size=35)) +
              labs(y = "ARI") +
              theme_classic()


# Mean RI across windows per subject
SC_summary <- aggregate(SC_all$Index, by = list(SC_all$SubjID), FUN = mean)
SC_summary$SD <- aggregate(SC_all$Index, by = list(SC_all$SubjID), FUN = sd)$x
colnames(SC_summary) <- list("SubjID", "RI", "SD")
SC_sd_plot <- ggplot(data = SC_summary, aes(x=SD)) + 
  geom_density(color = "black", fill = Cols[2]) + 
  xlim(0,1) +
  labs(x = "SD") +
  theme(text = element_text(size=35)) +
  theme_classic() 

## PLAYING WITH AR AND MA PROCESSES
# tempSC <- do.call(rbind, SC)
# tempSC <- SC_PFC
# tempSC$SubjID <- as.factor(tempSC$SubjID)
# test <- numeric()
# intercepts <- numeric()
# coefficients <- numeric()
# 
# for (i in seq(100)) {
#   temp <- arima(SC[[i]]$Index, c(1,0,0), method = "ML")
#   test <- c(test, temp$residuals)
#   intercepts[i] <- temp$coef[2]
#   coefficients[i] <- temp$coef[1] 
# }
# 
# tempSC$Res <- test
# 
# ggplot(data = tempSC, aes(Window, Res, group = SubjID, color = SubjID)) +
#   geom_line(show.legend = F, alpha = 0.5) +
#   theme_classic()
# 
# ggplot(data = tempSC, aes(Res, group = SubjID, color = SubjID)) +
#   geom_density(alpha = 0.5, show.legend = F) +
#   theme_classic()
# 
# 
# test <- aggregate(SC_PCC$Index, by = list(SC_PFC$SubjID), arima, order = c(10,0,0), method = "ML")
# intercepts <- numeric()
# coefficients <- numeric()
# for(i in seq(100)) {
#   intercepts[i] <- test$x[[i]]['intercept']
#   coefficients[i] <- test$x[[i]]['ar1']
# }
# 
# plot(intercepts)

# AND WITH MORE SIMILARITY MATRICES
# THE POINT HERE IS THAT I TOOK THE ARI SIMILARITY MATRIX OF ALL THE SLIDING WINDOWS PER SUBJECT AND AVERAGED ACROSS SUBJECTS
# THIS PRODUCES A MATRIX WHERE YOU CAN TELL THE AUTOCORRELATED NATURE OF THE DATA IF UNCLUSTERED, BUT OTHERWISE THERE ARE MARKED DAY CLUSTERS
# simMatrices <- list()
# nrun <- 1
# for (subj in SVals) {
#   nWins <- ncol(subj)
#   if (nWins != 37) {
#     print("Fewer than 37 windows...")
#   } else {
#     tempMat <- matrix(NA, nWins, nWins)
#     for (i in seq(nWins)) {
#       for(j in seq(nWins)) {
#         tempMat[i,j] <- arandi(subj[,i], subj[,j], adjust = T)
#       }
#     }
#     simMatrices[[nrun]] <- tempMat
#     nrun <- nrun + 1
#   }
# }
# 
# meanMatrices <- Reduce("+", simMatrices) / length(simMatrices)
# corrplot(meanMatrices, method = "color", is.corr = F, cl.lim = c(0,1))
```

In order to estimate the degree of stability of these partitions in time, we performed a sliding window analysis on each subject's full time series (20 min windows shifting at 1 min), comparing the community arrangement derived from each window with the partition computed using the whole time series (Figure 4). 

``` {r Stability per subject: PCC vs mPFC, fig.align="center", fig.width=10, fig.height=4, echo=FALSE, message = FALSE, cache = TRUE}
## Check stability for PCC and mPFC separately
# Get the sliding values for PCC and PFC separately
tempSW_PCC <- lapply(SVals, "[", i = posteriorIndx, j =)
tempSW_PFC <- lapply(SVals, "[", i = !posteriorIndx, j =)

SC_PCC <- data.frame()
SC_PFC <- data.frame()

for (subj in seq(nSubj)) {
  tempPCC <- numeric()
  tempPFC <- numeric()
  nWins <- ncol(tempSW_PCC[[subj]])
  for (Win in seq(nWins)) {
    tempPCC[Win] <- arandi(tempComm_PCC[[subj]]$FiedlerBinary, tempSW_PCC[[subj]][,Win], adjust = T)
    tempPFC[Win] <- arandi(tempComm_PFC[[subj]]$FiedlerBinary, tempSW_PFC[[subj]][,Win], adjust = T)
  }
  dfPCC <- data.frame(SubjID = rep(subjList[subj], nWins),
                      Window = seq(nWins),
                      Index = tempPCC)
  dfPFC <- data.frame(SubjID = rep(subjList[subj], nWins),
                      Window = seq(nWins),
                      Index = tempPFC)
  SC_PCC <- rbind(SC_PCC, dfPCC)
  SC_PFC <- rbind(SC_PFC, dfPFC)
}

SC_PCC_summary <- aggregate(SC_PCC$Index, by = list(SC_PCC$SubjID), FUN = mean)
SC_PCC_summary$SD <- aggregate(SC_PCC$Index, by = list(SC_PCC$SubjID), FUN = sd)$x
SC_PCC_summary$Region <- rep("PCC", nSubj)
SC_PFC_summary <- aggregate(SC_PFC$Index, by = list(SC_PFC$SubjID), FUN = mean)
SC_PFC_summary$SD <- aggregate(SC_PFC$Index, by = list(SC_PFC$SubjID), FUN = sd)$x
SC_PFC_summary$Region <- rep("mPFC", nSubj)
SC_PCCPFC_summary <- rbind(SC_PFC_summary, SC_PCC_summary)
colnames(SC_PCCPFC_summary) <- list("SubjID", "Index", "SD", "Region")
rm(SC_PCC_summary, SC_PFC_summary)

# Compare mean ARI of sliding windows between regions
SW_PCCvPFC_permuteTest <- permute(SC_PCCPFC_summary$Index[SC_PCCPFC_summary$Region == "PCC"], SC_PCCPFC_summary$Index[SC_PCCPFC_summary$Region == "mPFC"], paired = T)
SW_PCCvPFC_ES <- cohenD(SC_PCCPFC_summary$Index[SC_PCCPFC_summary$Region == "PCC"], SC_PCCPFC_summary$Index[SC_PCCPFC_summary$Region == "mPFC"])

```

This analysis showed that the mean ARI along each subject's time series was high for both PCC (mean = `r round(mean(SC_PCCPFC_summary$Index[SC_PCCPFC_summary$Region=="PCC"]), digits = 2)`; SD = `r round(sd(SC_PCCPFC_summary$Index[SC_PCCPFC_summary$Region=="PCC"]), digits = 2)`) and mPFC (mean = `r round(mean(SC_PCCPFC_summary$Index[SC_PCCPFC_summary$Region=="mPFC"]), digits = 2)`; SD = `r round(sd(SC_PCCPFC_summary$Index[SC_PCCPFC_summary$Region=="mPFC"]), digits = 2)`), with significantly higher ARI for PCC (paired permutation, *p* = `r SW_PCCvPFC_permuteTest$Pval`; Cohen's D = `r round(SW_PCCvPFC_ES, digits = 2)`). These results suggest that the each individual's DMN distribution is cosiderably stable in time.

There is precedence in thhe literature that the magnitude (and not just the sign) of the Fiedler vector values conveys important information about the role of each node in the network [@Gkantsidis2003; @Tian2018]. Therefore, we asked whether the magnitude of these eigenvector values was associated with the stability of nodes in time. Specifically, we examined whether Fiedler vector values could predict the proportion of DMN affiliations per node by performing a logistic mixed effects model (Figure 5). The model yielded a positive significant relationship between these features ($\beta$ = `r round(SC_FVvPDMN_modelSummary$coefficients[2,1], digits = 2)`, SE = `r round(SC_FVvPDMN_modelSummary$coefficients[2,2], digits = 2)`, *p* < 0.001), meaning that vertices with higher absolute Fiedler vector values were more persistent in their relationship with their corresponding community in time. We refer to this proportion of affiliations over time as the stability of a given node.

``` {r Threshold values}
# Get a FV threshold for each subject based on mean vertex stability
# Run the function safely, because some noisy subjects have no relationship between stability and FV
# Note, the mean resulting threshold is similar to the group's 25th and 75th quantiles
TFV <- safely(threshFV)
tempList <- lapply(Summaries, TFV)

# Store thre FV thresholds for all subject 
threshDF <- data_frame(SubjID = substring(subjList,1,6),
                       Lower = NA,
                       Upper = NA)
for (i in seq_along(tempList)) {
  if (is.null(tempList[[i]]$result)) {
    # if there wass no clear threshold (i.e. noisy participants), then leave it as 0
    threshDF[i,2:3] <- c(0,0)
  } else {
    threshDF[i,2] <- as.numeric(tempList[[i]]$result[1])
    threshDF[i,3] <- as.numeric(tempList[[i]]$result[2])
  }
}

# get descriptives
threshDescriptives <- threshDF %>% 
                        filter(Lower < 0, Upper > 0) %>% 
                        summarize(mL = mean(Lower), 
                                  sdL = sd(Lower), 
                                  mU = mean(Upper), 
                                  sdU = sd(Upper)) %>% 
                        round(., digits = 4)

# number of subjects without stable vertices
nUnstable <- threshDF %>% 
                filter(Lower == 0, Upper == 0) %>% 
                nrow

rm(tempList)

```

These analyses show that the original pattern identified with the full dataset is stable in time, and that there is potential value in thresholding the Fiedler vector as a means to capture idiosyncratic and reliable DMN and non-DMN vertices, even in relatively short resting fMRI sessions (i.e. less than 20 mins). We therefore thresholded each subject's Fiedler vector to produce these refined maps. We estimated this threshold by selecting the smallest absolute Fiedler vector value that yielded a stability of 95%, for positive (mean = `r threshDescriptives[3]`, SD = `r threshDescriptives[4]`) and negative (mean = `r threshDescriptives[1]`, SD = `r threshDescriptives[2]`) values separately. Individuals without such stable nodes (n = `r nUnstable`) were not thresholded, but their partitions were not discarded. Vertices under these thresholds were set to zero (instead of removed) so that the dimensionality of the data was unaffected, and 0.5 in the binarized maps so that they wouldn't bias the calculation of averages. Figure 6 (left) shows the updated partition for the one showed in figure 2. The maps used in all subsequent analyses were thresholded by this individualized criterion.

### *Partition Comparison: Thresholded Full Datasets*
``` {r Partition comparison: all thresholded, echo=FALSE, message = FALSE, cache = TRUE}
# Apply threshold to Summaries
# IMPORTANT: THIS PROCESS OVERWRITES THE VARIABLES FROM THE FIRST COMPARISON SECTION
# THIS IS DONE FOR STORAGE-SAKE, BUT IT MIGHT BE WORTH GIVING THESE TEIR OWN NAME
if (threshMaps) {
  for (i in seq(nSubj)) {
  Summaries[[i]] <- Summaries[[i]] %>%
    mutate(FiedlerVec = ifelse(FiedlerVec > as.numeric(threshDF[i,2]) & FiedlerVec < as.numeric(threshDF[i,3]), 0, FiedlerVec)) %>%
    mutate(FiedlerBinary = ifelse(FiedlerVec > 0, 1, ifelse(FiedlerVec < 0, 0, 0.5)))
  }
}

# percent of stable nodes per subject
prcntStable <- sapply(Summaries, function(data) {mean(data$FiedlerBinary != 0.5)})

# Across subjects
# Note: the upper and lower triangles can totally be combined before plotting, instead of adding one corrplot over another. Might work on that eventually.
RI_all_between <- comparePartitions(Data = Summaries, Index = "RI", nSubjects = length(Summaries), subjNames = subjList)
RI_all_between_lowtri <- RI_all_between[lower.tri(RI_all_between)]

posteriorIndx <- Summaries[[1]]$y < 0
tempComm_PCC <- lapply(Summaries, "[", i = posteriorIndx, j =)
tempComm_PFC <- lapply(Summaries, "[", i = !posteriorIndx, j =)
RI_PFC <- comparePartitions(Data = tempComm_PFC, Index = "RI", nSubjects = length(Summaries), subjNames = subjList)
RI_PCC <- comparePartitions(Data = tempComm_PCC, Index = "RI", nSubjects = length(Summaries), subjNames = subjList)

# Let's compare stuff
testPCC <- RI_PCC[lower.tri(RI_PCC)]
testPFC <- RI_PFC[lower.tri(RI_PFC)]
Overall_PCCvsPFC_permTest <- permute(testPCC, testPFC, statType = mean, paired = T)
Overall_PCCvsPFC_ES <- cohenD(testPCC, testPFC) # caveat: maybe not ideal for bounded values, like RI. BUT VI yields similar results.

# To get the rate of stability per vertex across subjects
FB <- do.call(cbind, lapply(Summaries, "[[", "FiedlerBinary"))
FV <- do.call(cbind, lapply(Summaries, "[[", "FiedlerVec"))
FBS <- rowMeans(FB)
FBS[FBS < 0.5] <- 1 - FBS[FBS < 0.5]

# Now let's test something: 
# Traditionally, a vertex-wise test would be carried across subjects.
# If these traditional analyses miss out on these idiosyncrasies, you should get unsignificant results.
# So we did a prop.test per vertex, and corrected with FDR to mimic this situation.
All_proptests <- apply(FB, 1, function(x) {prop.test(sum(x), nSubj)})
Prop_pvals_corrected <- p.adjust(sapply(All_proptests, "[[", "p.value"), method = "BY")
Prop_pvals_corrected <- ifelse(Prop_pvals_corrected < 0.05, 1, 0)
Chisquares <- sapply(All_proptests, "[[", "statistic")

# And a t-test on the FV vals
All_ttests <- apply(FV, 1, t.test)
Ttest_pvals_corrected <- p.adjust(sapply(All_ttests, "[[", "p.value"), method = "BY")
Ttest_pvals_corrected <- ifelse(Ttest_pvals_corrected < 0.05, 1, 0)
Tstats <- sapply(All_ttests, "[[", "statistic")

```

The average percentage of stable vertices per subject was `r round(mean(prcntStable[prcntStable < 1]), digits = 2)` (SD = `r round(sd(prcntStable[prcntStable < 1]), digits = 2)`). With these thresholded partitions, we recomputed the overall similarity of their functional organization across participants. Compared to before, there was even less overall agreement of the full maps among individuals (mean = `r round(mean(RI_all_between_lowtri), digits = 2)`, SD = `r round(sd(RI_all_between_lowtri), digits = 2)`). The same was true for both PCC (mean = `r round(mean(testPCC), digits = 2)`, SD = `r round(sd(testPCC), digits = 2)`) and mPFC (mean = `r round(mean(testPFC), digits = 2)`, SD = `r round(sd(testPFC), digits = 2)`) separately, although the significance of the differences between these areas was preserved (paired permutation, *p* = `r Overall_PCCvsPFC_permTest$Pval`; Cohen's D = `r round(Overall_PCCvsPFC_ES, digits = 2)`). Figure 6 (right) shows the spatial average of the thresholded binarized partitions across all participants, denoting the proportion of times a vertex was affiliated with the DMN community. This summary conveys a common organizational principle for both communities, but also highlights the considerable variability across individuals in mPFC compared to PCC.

``` {r Noise Effects: Per-subject Fiedler by R-sqd, fig.align="center", fig.width=3, fig.height=3, fig.cap = "FORMER Figure 13. Fiedler vector values are linearly related to the total variance explained by each vertex with respect to the mean DMN time series.", echo=FALSE, message = FALSE, cache = TRUE}
setwd('./Variance/')
temp <- list.files()
Variance <- lapply(temp, read.csv)

## append the Rsquared values onto the current summaries
# unfortunately this only counts on the DMN community
appendVar <- lapply(seq(nSubj), function(subj) {left_join(Summaries[[subj]], Variance[[subj]], by = "Vertex")})

## correlate tSNR with Rsquared before and after thresholds
snrR2_cors <- list()

snrR2_cors$pre_thresh <- lapply(appendVar, function(data) {
  data %>% 
    filter(!is.na(Rsquared)) %>%
    mutate(region = ifelse(y > 0, "mPFC", "PCC")) %>%
    group_by(region) %>%
    dplyr::summarise(correlation = cor(varCoeff, Rsquared)) %>%
    ungroup()
}) %>% reduce(left_join, by = "region") 

snrR2_cors$post_thresh <- lapply(appendVar, function(data) {
  data %>% 
    filter(!is.na(Rsquared), FiedlerBinary != 0.5) %>%
    mutate(region = ifelse(y > 0, "mPFC", "PCC")) %>%
    group_by(region) %>%
    dplyr::summarise(correlation = cor(varCoeff, Rsquared)) %>%
    ungroup()
}) %>% reduce(left_join, by = "region")

## next get the mean tSNR of PCC and mPFC before/after thresholding
# the first piece of code gets each subject's mean tSNR per region, and reduces to a single matrix
# the second one is similar, but filtering out vertices that didn't meet the threshold
# the third one applies summary stats to each list to create a summary table
snrR2_means <- list()

snrR2_means$pre_thresh <- lapply(appendVar, function(data) {
  data %>% 
    mutate(region = ifelse(y > 0, "mPFC", "PCC")) %>%
    group_by(region) %>% 
    dplyr::summarise(mean_tsnr = mean(varCoeff)) %>%
    ungroup()
}) %>% reduce(left_join, by = "region")

snrR2_means$post_thresh <- lapply(appendVar, function(data) {
  data %>% 
    filter(FiedlerBinary != 0.5) %>%
    mutate(region = ifelse(y > 0, "mPFC", "PCC")) %>%
    group_by(region) %>% 
    dplyr::summarise(mean_tsnr = mean(varCoeff)) %>%
    ungroup()
}) %>% reduce(left_join, by = "region")

# and compute summary stats on each 
snrR2_means$overall <- sapply(snrR2_means, function(x) {
  c(rowMeans(x[, -1]), apply(x[, -1], 1, sd))
})
rownames(snrR2_means$overall) <- c("mPFC_mean", "PCC_mean", "mPFC_SD", "PCC_SD")

# permutation pre vs post
tsnrPermute_mPFC <- permute(as.numeric(snrR2_means$pre_thresh[1, -1]), as.numeric(snrR2_means$post_thresh[1, -1]), paired = T)
tsnrPermute_PCC <- permute(as.numeric(snrR2_means$pre_thresh[2, -1]), as.numeric(snrR2_means$post_thresh[2, -1]), paired = T)

# effect size pre vs post
tsnrCohen_mPFC <- CohenD(as.numeric(snrR2_means$post_thresh[1, -1]), as.numeric(snrR2_means$pre_thresh[1, -1]))
tsnrCohen_PCC <- CohenD(as.numeric(snrR2_means$post_thresh[2, -1]), as.numeric(snrR2_means$pre_thresh[2, -1]))

# ##### old
# 
# # Compare SNR between PCC and mPFC per subject after thresholding (this filtering is relevant here because we aren't using ARI anymore)
# tempComm_PCC <- lapply(tempComm_PCC, filter, FiedlerVec != 0)
# tempComm_PFC <- lapply(tempComm_PFC, filter, FiedlerVec != 0) 
# SNR_PCCvsPFC_perm <- sapply(seq_along(tempComm_PCC), function(x) {permute(tempComm_PCC[[x]]$varCoeff, tempComm_PFC[[x]]$varCoeff)$Pval})
# SNR_PCCvsPFC_perm <- p.adjust(SNR_PCCvsPFC_perm, method = "BY")
# SNR_PCCvsPFC_ES <- sapply(seq_along(tempComm_PCC), function(x) {cohenD(tempComm_PCC[[x]]$varCoeff, tempComm_PFC[[x]]$varCoeff)})
# prcntSig <- mean(SNR_PCCvsPFC_perm < 0.05) * 100
# 
# # Get the mean SNR for each region and compare overall levels 
# SNR_meanPCC <- sapply(tempComm_PCC, function(data) {mean(data$varCoeff)})
# SNR_meanPFC <- sapply(tempComm_PFC, function(data) {mean(data$varCoeff)})
# SNR_meanComp <- list(Values = data.frame(Region = c(rep("PCC", nSubj), rep("mPFC", nSubj)), SNR = c(SNR_meanPCC, SNR_meanPFC)),
#                      Comparison = permute(SNR_meanPCC, SNR_meanPFC)$Pval,
#                      ES = cohenD(SNR_meanPCC, SNR_meanPFC))
# rm(SNR_meanPCC, SNR_meanPFC)


```

To discard the possibility that the higher inter-subject variability in mPFC was driven by retaining vertices with lower signal quality, we compared the mean temporal signal to noise ratio (tSNR) for each region pre and post thresholding. We used subject time series that were not demeaned, but were otherwise equivalent to the data originally used. Paired permutation analyses (5000 iterations) showed that the mean tSNR in mPFC actually significantly increased post-thresholding across subjects (Pre: mean = `r snrR2_means$overall[1, 1]`, SD = `r snrR2_means$overall[3, 1]`; Post: mean = `r snrR2_means$overall[1, 2]`, SD = `r snrR2_means$overall[3, 2]`; paired permutation p-value = `r tsnrPermute_mPFC$Pval`, Cohen's D = `r tsnrCohen_mPFC[1]`), whereas the mean signal quality in PCC was unaffected (Pre: mean = `r snrR2_means$overall[2, 1]`, SD = `r snrR2_means$overall[4, 1]`; Post: mean = `r snrR2_means$overall[2, 2]`, SD = `r snrR2_means$overall[4, 2]`; paired permutation p-value = `r tsnrPermute_PCC$Pval`, Cohen's D = `r tsnrCohen_PCC[1]`). 

### *Partition Comparison: Day 1 vs Day 2*

The relative high variability seen in the aggregate map could be due to variations in the functional organization of mPFC across days. While the sliding window analysis suggested that these partitions are stable, many of the windows contained data from both days. A stricter evaluation of subject-specificity would be to compare every subject's subdivisions produced by data from each day separately. If the variability that we observed was truly due to having captured an individual's unique organization, day to day ARI values should be consistently higher within- than between-subjects. Figure 7 shows pairwise comparisons among ten example subjects for whole brain (left), as well as for PCC and mPFC separately (right). 

``` {r Partition comparison: halves, echo=FALSE, message = FALSE, cache = TRUE}
# Load data
setwd('./Summary_halves/')
temp <- list.files(pattern = "*_finalSummary.csv")
subjList_halves <- sapply(temp, substring, first=1, last=9)
Summaries_halves <- lapply(temp, read.csv)

# Before thresholding, capture within/between daily comparisons for FV vs correlation analysis (only for daily)
# Some subjects didn't have the correct labeling scheme in one of their days (see corr vs FV below), so we're manually reverting it after visual inspection
revertSubjs <- c("144226_H2", "165840_H1", "192843_H1", "303624_H1", "687163_H1", "108525_H2", "156637_H1", "156637_H1")
for (subj in revertSubjs) {
  indx <- grep(subj, subjList_halves)
  Summaries_halves[[indx]] <- transform(Summaries_halves[[indx]], FiedlerVec = FiedlerVec * -1)
}
Summaries_halves_unthresh <- Summaries_halves

# Some participants have strongly uncorrelated maps because 7m isn't fully covered on one of the days
# ARI doesn't mind this, so the results are fine
# But the Corr vs FV analysis below is badly affected
# For now I'm just selecting the individuals that have negatively correlated FVectors
# It might be worth visually inspecting each to see if a simple heuristic works (i.e. if corr < -0.5, then FV * -1 & 1 - FB)
# Although this doesn't adress which half is the culprit
unCorrParticipants <- data.frame(SubjID = subjList,
                                 Corrs = sapply(seq(1, nSubj *2, by = 2), function(x) {cor(Summaries_halves_unthresh[[x]]$FiedlerVec, Summaries_halves_unthresh[[x+1]]$FiedlerVec)})) %>%
  filter(Corrs < 0)

# Load the correlation-based maps from each day for analysis in a subsequent section
temp <- list.files(pattern = "*_corHalves.csv")
Correlation_halves <- lapply(temp, read.csv, header = F)

# Apply threshold to Summaries
if (threshMaps) {
  # create a vector of repeated indices, so each threshold is applied to each subject twice
  threshindex <- rep(seq(nSubj), each = 2)
  count <- 1
  for (i in threshindex) {
    Summaries_halves[[count]] <- Summaries_halves[[count]] %>%
            mutate(FiedlerVec = ifelse(FiedlerVec > as.numeric(threshDF[i,2]) & FiedlerVec < as.numeric(threshDF[i,3]), 0, FiedlerVec)) %>%
            mutate(FiedlerBinary = ifelse(FiedlerVec > 0, 1, ifelse(FiedlerVec < 0, 0, 0.5)))
    count <- count + 1
  }
}

# # Now recompute so that all network analyses are performed on thresholded data
# # Select subsection of subjects for plotting the similarity matrices
# Summaries_halves_plot <- Summaries_halves[selectSubj]
# subjList_halves_plot <- subjList_halves[selectSubj]

# # Compare all halves with RI (plot only)
# RI_all_between_halves_plot <- comparePartitions(Data = Summaries_halves_plot, Index = "RI", nSubjects = length(Summaries_halves_plot), subjNames = subjList_halves_plot)
# 
# # Compare PCC and PFC among all subjects (plot only)
# tempComm_PCC_halves_plot <- lapply(Summaries_halves_plot, "[", i = posteriorIndx, j =)
# tempComm_PFC_halves_plot <- lapply(Summaries_halves_plot, "[", i = !posteriorIndx, j =)
# RI_PFC_halves_plot <- comparePartitions(Data = tempComm_PFC_halves_plot, Index = "RI", nSubjects = length(Summaries_halves_plot), subjNames = subjList_halves_plot)
# RI_PCC_halves_plot <- comparePartitions(Data = tempComm_PCC_halves_plot, Index = "RI", nSubjects = length(Summaries_halves_plot), subjNames = subjList_halves_plot)
```

``` {r Partition comparison: halves ratios, echo=FALSE, message = FALSE, cache = TRUE}
## Now compare all possible pairs of subjects (for actual reporting)
# Compare all halves with RI
RI_all_between_halves <- comparePartitions(Data = Summaries_halves, Index = "RI", nSubjects = length(Summaries_halves), subjNames = subjList_halves)

# Compare PCC and PFC among all subjects
tempComm_PCC_halves <- lapply(Summaries_halves, "[", i = posteriorIndx, j =)
tempComm_PFC_halves <- lapply(Summaries_halves, "[", i = !posteriorIndx, j =)
RI_PFC_halves <- comparePartitions(Data = tempComm_PFC_halves, Index = "RI", nSubjects = length(Summaries_halves), subjNames = subjList_halves)
RI_PCC_halves <- comparePartitions(Data = tempComm_PCC_halves, Index = "RI", nSubjects = length(Summaries_halves), subjNames = subjList_halves)

## Let's compare stuff (not accounting for complex interdependencies in the observations)
## Overall differences in mean RI between PCC and mPFC
lowPCC_halves <- RI_PCC_halves[lower.tri(RI_PCC_halves)]
lowPFC_halves <- RI_PFC_halves[lower.tri(RI_PFC_halves)]
Halves_PCCvPFC_perm <- permute(lowPCC_halves, lowPFC_halves, statType = mean, paired = T)
Halves_PCCvPFC_ES <- cohenD(lowPCC_halves, lowPFC_halves) # caveat: maybe not ideal for bounded values, like RI. BUT VI yields similar results.


## Differences in RI values for sessions within vs between subjects (all subjects pooled together)
# create a selection matrix containing all within-subject session comparisons
template <- diagBlocks(ncol(RI_all_between_halves), 2) 
lowTemplate <- template[lower.tri(template)]

# Overall
# First list element is the within subject values, second is the between subject values
lowRIHalves <- RI_all_between_halves[lower.tri(RI_all_between_halves)]
Halves_overallComparison <- list(lowRIHalves[lowTemplate],
                                lowRIHalves[!lowTemplate])
Halves_overallComparison[[3]] <- permute(Halves_overallComparison[[1]], Halves_overallComparison[[2]])
Halves_overallComparison[[4]] <- cohenD(Halves_overallComparison[[1]], Halves_overallComparison[[2]])

# PCC 
Halves_PCCComparison <- list(lowPCC_halves[lowTemplate],
                           lowPCC_halves[!lowTemplate])
Halves_PCCComparison[[3]] <- permute(Halves_PCCComparison[[1]], Halves_PCCComparison[[2]])
Halves_PCCComparison[[4]] <- cohenD(Halves_PCCComparison[[1]], Halves_PCCComparison[[2]])

# mPFC
Halves_PFCComparison <- list(lowPFC_halves[lowTemplate],
                           lowPFC_halves[!lowTemplate])
Halves_PFCComparison[[3]] <- permute(Halves_PFCComparison[[1]], Halves_PFCComparison[[2]])
Halves_PFCComparison[[4]] <- cohenD(Halves_PFCComparison[[1]], Halves_PFCComparison[[2]])


## Ratio of mean within subject sessions over between subject ones (per-subject)
# Overall
# Split the within-subject session RI values per subject
withinSubjVals <- RI_all_between_halves[lower.tri(RI_all_between_halves)][lowTemplate]

# Now grab the columns from the similarity matrix for a subject
betweenSubjVals <- lapply(seq(1, ncol(RI_all_between_halves), by = 2), function(x) RI_all_between_halves[-(x:(x+1)), x:(x+1)])

# And get a vector of the meanWithin / meanBetween ratio per subject
Halves_RIRatios_overall <- sapply(seq(nSubj), function(i) {withinSubjVals[[i]] / mean(betweenSubjVals[[i]])})


# PCC
# Split the within-subject session RI values per subject
withinSubjVals <- RI_PCC_halves[lower.tri(RI_PCC_halves)][lowTemplate]

# Now grab the columns from the similarity matrix for a subject
betweenSubjVals <- lapply(seq(1, ncol(RI_PCC_halves), by = 2), function(x) RI_PCC_halves[-(x:(x+1)), x:(x+1)])

# And get a vector of the meanWithin / meanBetween ratio per subject
Halves_RIRatios_PCC <- sapply(seq(nSubj), function(i) {withinSubjVals[[i]] / mean(betweenSubjVals[[i]])})

# PFC
# Split the within-subject session RI values per subject
withinSubjVals <- RI_PFC_halves[lower.tri(RI_PFC_halves)][lowTemplate]

# Now grab the columns from the similarity matrix for a subject
betweenSubjVals <- lapply(seq(1, ncol(RI_PFC_halves), by = 2), function(x) RI_PFC_halves[-(x:(x+1)), x:(x+1)])

# And get a vector of the meanWithin / meanBetween ratio per subject
Halves_RIRatios_PFC <- sapply(seq(nSubj), function(i) {withinSubjVals[[i]] / mean(betweenSubjVals[[i]])})


# Summarize for plotting
Halves_RIRatios_summary <- data.frame(Region = c(rep("PCC", nSubj), rep("mPFC", nSubj)),
                                      RI = c(Halves_RIRatios_PCC, Halves_RIRatios_PFC))

# Stats
Halves_mPFCvPCC_permTest <- permute(Halves_RIRatios_PFC, Halves_RIRatios_PCC, paired = T, statType = median)
Halves_mPFCvPCC_ES <- cohenD(Halves_RIRatios_PFC, Halves_RIRatios_PCC)

# Select a submatrix with the best-performing subjects to show on the matrix figures
# These will be also displayed on the session matrix
# This way I don't have to recompute stuff
tmp <- data_frame(SubjID = subjList,
                  DailyARI = Halves_PFCComparison[[1]])
best <- tmp %>% top_n(10, wt = DailyARI) 
bestindx <- substr(subjList_halves,1,6) %in% best$SubjID

RI_all_between_halves_plot <- RI_all_between_halves[bestindx, bestindx]
RI_PFC_halves_plot <- RI_PFC_halves[bestindx, bestindx]
RI_PCC_halves_plot <- RI_PCC_halves[bestindx, bestindx]

# Get the same subsection of participants for the correlation plots
Correlation_halves_plot <- Correlation_halves[bestindx] 

# Save unthresholded subjects for comparing and plotting against correlation maps
# this comment is temporary, just to force the cache to update
Foo <- "Fighters"
unthreshComm_PFC_halves_plot <- lapply(Summaries_halves_unthresh[bestindx], "[", i = !posteriorIndx, j =)
unthreshComm_PFC_halves <- lapply(Summaries_halves_unthresh, "[", i = !posteriorIndx, j =)
```

Once again we found low alignment across subjects for PCC (mean = `r round(mean(Halves_PCCComparison[[2]]), digits = 2)`, SD = `r round(sd(Halves_PCCComparison[[2]]), digits = 2)`) and mPFC (mean = `r round(mean(Halves_PFCComparison[[2]]), digits = 2)`, SD = `r round(sd(Halves_PFCComparison[[2]]), digits = 2)`), but both areas showed relatively high levels of agreement within subjects (PCC: mean = `r round(mean(Halves_PCCComparison[[1]]), digits = 2)`, SD = `r round(sd(Halves_PCCComparison[[1]]), digits = 2)`; mPFC: mean = `r round(mean(Halves_PFCComparison[[1]]), digits = 2)`, SD = `r round(sd(Halves_PFCComparison[[1]]), digits = 2)`). However, a coarse comparison of mean ARIs neglects the degree to which subject-wise partitions were indeed idiosyncratic. Therefore, we divided everyone's within-subject ARI by the mean ARI of their between-subject comparison. This ratio denotes the relative specificity of their partition, such that partitions that align well across subjects should yield ratios close to 1, while unique and stable partitions would produce high ratios. Given what we know about these general regions, we expected PCC to be well aligned both within and between subjects. On the other hand, we hypothesized low agreement of mPFC partitions across participants, but with considerable stability across days and sessions within subject.

Figure 8 shows these ARI ratios for PCC and mPFC for all subjects. As expected, we found that while both regions had good specificity (i.e. ratios > 1), the ratios for mPFC (median = `r round(median(Halves_RIRatios_PFC), digits = 2)`; SD = `r round(sd(Halves_RIRatios_PFC), digits = 2)`) were significantly higher than those for PCC (median = `r round(median(Halves_RIRatios_PCC), digits = 2)`; SD = `r round(sd(Halves_RIRatios_PCC), digits = 2)`) when compared with a paired permutation test (*p* = `r Halves_mPFCvPCC_permTest$Pval`; Cohen's D = `r round(Halves_mPFCvPCC_ES, digits = 2)`). These test/retest results suggest that the topographic variability seen in mPFC is reliably rooted in subject-specific organization, and not an inability to capture its topology.

### *Partition Comparison: Session based*

We extended the analysis of daily data by examining whether the organization of the DMN could be extracted using session data only. It is worth noting that the length of each session (approx. 14 mins long) falls below the stability threshold for fMRI-based modularity estimations [@Gordon2017]. Nonetheless, high ARI ratios with reduced data could signify the ability to localize a subject's DMN organization with fMRI sessions that are easier to fit along with other study sessions of interest.

``` {r Partition comparison: session similarity matrices, echo=FALSE, message = FALSE, cache = TRUE}
# Load data
setwd('./Summary_sess/')
temp <- list.files()
subjList_sess <- sapply(temp, substring, first=1, last=9)
Summaries_sess <- lapply(temp, read.csv)

# Apply threshold to Summaries
if (threshMaps) {
  # create a vector of repeated indices, so each threshold is applied to each subject twice
  threshindex <- rep(seq(nSubj), each = 4)
  count <- 1
  for (i in threshindex) {
    Summaries_sess[[count]] <- Summaries_sess[[count]] %>%
            mutate(FiedlerVec = ifelse(FiedlerVec > as.numeric(threshDF[i,2]) & FiedlerVec < as.numeric(threshDF[i,3]), 0, FiedlerVec)) %>%
            mutate(FiedlerBinary = ifelse(FiedlerVec > 0, 1, ifelse(FiedlerVec < 0, 0, 0.5)))
    count <- count + 1
  }
}

# # Select 10 subjects from the sample to plot
# selectSubj <- seq(40)
# subjList_sess_plot <- subjList_sess[selectSubj]
# Summaries_sess_plot <- Summaries_sess[selectSubj]
# 
# 
# # Across subjects (just for plotting)
# RI_all_between_sess_plot <- comparePartitions(Data = Summaries_sess_plot, Index = "RI", nSubjects = length(Summaries_sess_plot), subjNames = subjList_sess_plot)
# tempComm_PCC_sess <- lapply(Summaries_sess_plot, "[", i = posteriorIndx, j =)
# tempComm_PFC_sess <- lapply(Summaries_sess_plot, "[", i = !posteriorIndx, j =)
# RI_PFC_sess_plot <- comparePartitions(Data = tempComm_PFC_sess, Index = "RI", nSubjects = length(Summaries_sess_plot), subjNames = subjList_sess_plot)
# RI_PCC_sess_plot <- comparePartitions(Data = tempComm_PCC_sess, Index = "RI", nSubjects = length(Summaries_sess_plot), subjNames = subjList_sess_plot)
```

``` {r Partition comparison: session compare all, echo=FALSE, message = FALSE, cache = TRUE}
# Now run comparisons among all subjects (for actual reporting)
RI_all_between_sess <- comparePartitions(Data = Summaries_sess, Index = "RI", nSubjects = length(Summaries_sess), subjNames = subjList_sess)
tempComm_PCC_sess <- lapply(Summaries_sess, "[", i = posteriorIndx, j =)
tempComm_PFC_sess <- lapply(Summaries_sess, "[", i = !posteriorIndx, j =)
RI_PFC_sess <- comparePartitions(Data = tempComm_PFC_sess, Index = "RI", nSubjects = length(Summaries_sess), subjNames = subjList_sess)
RI_PCC_sess <- comparePartitions(Data = tempComm_PCC_sess, Index = "RI", nSubjects = length(Summaries_sess), subjNames = subjList_sess)

## Let's compare stuff (not accounting for complex interdependencies in the observations)
## Overall differences in mean RI between PCC and mPFC
lowPCC_sess <- RI_PCC_sess[lower.tri(RI_PCC_sess)]
lowPFC_sess <- RI_PFC_sess[lower.tri(RI_PFC_sess)]
Sess_PCCvPFC_perm <- permute(lowPCC_sess, lowPFC_sess, statType = mean, paired = T)
Sess_PCCvPFC_ES <- cohenD(lowPCC_sess, lowPFC_sess) # caveat: maybe not ideal for bounded values, like RI. BUT VI yields similar results.


## Differences in RI values for sessions within vs between subjects (all subjects pooled together)

# create a selection matrix containing all within-subject session comparisons
template <- diagBlocks(ncol(RI_all_between_sess), 4) 
lowTemplate <- template[lower.tri(template)]

# Overall
# First list element is the within subject values, second is the between subject values
lowRISess <- RI_all_between_sess[lower.tri(RI_all_between_sess)]
Sess_overallComparison <- list(lowRISess[lowTemplate],
                               lowRISess[!lowTemplate])
Sess_overallComparison[[3]] <- permute(Sess_overallComparison[[1]], Sess_overallComparison[[2]])
Sess_overallComparison[[4]] <- cohenD(Sess_overallComparison[[1]], Sess_overallComparison[[2]])

# PCC 
Sess_PCCComparison <- list(lowPCC_sess[lowTemplate],
                           lowPCC_sess[!lowTemplate])
Sess_PCCComparison[[3]] <- permute(Sess_PCCComparison[[1]], Sess_PCCComparison[[2]])
Sess_PCCComparison[[4]] <- cohenD(Sess_PCCComparison[[1]], Sess_PCCComparison[[2]])

# mPFC
Sess_PFCComparison <- list(lowPFC_sess[lowTemplate],
                           lowPFC_sess[!lowTemplate])
Sess_PFCComparison[[3]] <- permute(Sess_PFCComparison[[1]], Sess_PFCComparison[[2]])
Sess_PFCComparison[[4]] <- cohenD(Sess_PFCComparison[[1]], Sess_PFCComparison[[2]])

# Select the best performing for plotting, from daily performance
bestindx <- substr(subjList_sess,1,6) %in% best$SubjID
RI_all_between_sess_plot <- RI_all_between_sess[bestindx, bestindx]
RI_PFC_sess_plot <- RI_PFC_sess[bestindx, bestindx]
RI_PCC_sess_plot <- RI_PCC_sess[bestindx, bestindx]
```

Computing SP on networks from individual sessions showed that we can still coarsely capture a subject's unique organizational pattern (Figure 9), even though the overall levels of agreement decreased (PCC between subjects: mean = `r round(mean(Sess_PCCComparison[[2]]), digits = 2)`, SD = `r round(sd(Sess_PCCComparison[[2]]), digits = 2)`; PCC within subjects: mean = `r round(mean(Sess_PCCComparison[[1]]), digits = 2)`, SD = `r round(sd(Sess_PCCComparison[[1]]), digits = 2)`; mPFC between subjects: mean = `r round(mean(Sess_PFCComparison[[2]]), digits = 2)`, SD = `r round(sd(Sess_PFCComparison[[2]]), digits = 2)`; mPFC within subjects: mean = `r round(mean(Sess_PFCComparison[[1]]), digits = 2)`, SD = `r round(sd(Sess_PFCComparison[[1]]), digits = 2)`). We again computed each subject's ARI ratio in order to quantify the specificity of these partitions, but this time using the mean ARI acros sessions (Figure 10). 

``` {r Partition comparison: session ratios, echo=FALSE, message = FALSE, cache = TRUE}
## Brain storming:
## Within-subject comparisons would be more interesting? Like, comparing the mean RI within-between per subject
## Perhaps a more interresting idea would be to generate an index of variance within/between subject
## These are just attempts at that are not perfect:

# 1) Ratio of mean within subject sessions over between subject ones (per-subject)
# Overall
# Split the within-subject session RI values per subject
temp <- ceiling(seq_along(Sess_overallComparison[[1]])/6)
withinSubjVals <- split(Sess_overallComparison[[1]], temp) 

# Now grab the columns from the similarity matrix for a subject
betweenSubjVals <- lapply(seq(1, ncol(RI_all_between_sess), by = 4), function(x) RI_all_between_sess[-(x:(x+3)), x:(x+3)])

# And get a vector of the meanWithin / meanBetween ratio per subject
Sess_RIRatios_overall <- sapply(seq(nSubj), function(i) {mean(withinSubjVals[[i]]) / mean(betweenSubjVals[[i]])})

# PCC
temp <- ceiling(seq_along(Sess_PCCComparison[[1]])/6)
withinSubjVals <- split(Sess_PCCComparison[[1]], temp) 

# Now grab the columns from the similarity matrix for a subject
betweenSubjVals <- lapply(seq(1, ncol(RI_PCC_sess), by = 4), function(x) RI_PCC_sess[-(x:(x+3)), x:(x+3)])

# And get a vector of the meanWithin / meanBetween ratio per subject
Sess_RIRatios_PCC <- sapply(seq(nSubj), function(i) {mean(withinSubjVals[[i]]) / mean(betweenSubjVals[[i]])})

# PFC
temp <- ceiling(seq_along(Sess_PFCComparison[[1]])/6)
withinSubjVals <- split(Sess_PFCComparison[[1]], temp) 

# Now grab the columns from the similarity matrix for a subject
betweenSubjVals <- lapply(seq(1, ncol(RI_PFC_sess), by = 4), function(x) RI_PFC_sess[-(x:(x+3)), x:(x+3)])

# And get a vector of the meanWithin / meanBetween ratio per subject
Sess_RIRatios_PFC <- sapply(seq(nSubj), function(i) {mean(withinSubjVals[[i]]) / mean(betweenSubjVals[[i]])})

# Summarize for plotting
Sess_RIRatios_summary <- data.frame(Region = c(rep("PCC", nSubj), rep("mPFC", nSubj)),
                                    RI = c(Sess_RIRatios_PCC, Sess_RIRatios_PFC))

# Stats
Sess_mPFCvPCC_permTest <- permute(Sess_RIRatios_PFC, Sess_RIRatios_PCC, paired = T, statType = median)
Sess_mPFCvPCC_ES <- cohenD(Sess_RIRatios_PFC, Sess_RIRatios_PCC)
```



We can see again that the ratios for mPFC were higher than those of PCC (permutation,*p*= `r Sess_mPFCvPCC_permTest$Pval`; Cohen's D = `r round(Sess_mPFCvPCC_ES, digits = 2)`), but both were mostly above 1. This further confirms that the intrinsic functional organization of mPFC is uniquely arranged per individual, and provides evidence that such patterns can be identified with small amounts of data in spite of a reduction in the estimates. 

### *Correlation vs community detection in mPFC*

``` {r Correlation vs community detection: compute, fig.align="center", echo=FALSE, message = FALSE, cache = TRUE}
## Between halves analysis for correlation and communities (mPFC only)
unthreshComm_PFC_halves_plot <- lapply(seq_along(unthreshComm_PFC_halves_plot), function(x) cbind(unthreshComm_PFC_halves_plot[[x]], Correlation_halves_plot[[x]]))
D1 <- unthreshComm_PFC_halves_plot
D2 <- lapply(unthreshComm_PFC_halves_plot, function(data) transform(data, V1 = FiedlerVec)) # the comparison function can only take 1 column name for this stuff
Data <- c(D1,D2)
subjList_halves_plot <- rep(best$SubjID, each = 2)
allHalfCombs <- comparePartitions(Data = Data, MOI = "V1", nSubjects = 40, Index = "Cor", subjNames = c(subjList_halves_plot, subjList_halves_plot))
```

``` {r Correlation vs community detection: comparison, fig.align="center", echo=FALSE, message = FALSE, cache = TRUE}
# Compare correlation with community approach on mPFC
# tempComm_PFC_halves <- lapply(seq_along(tempComm_PFC_halves), function(x) cbind(tempComm_PFC_halves[[x]], Correlation_halves[[x]]))
# tempComm <- sapply(seq(1, 20, by = 2), function(x) cor(tempComm_PFC_halves[[x]]$FiedlerVec, tempComm_PFC_halves[[x+1]]$FiedlerVec))
# tempCor <- sapply(seq(1, 20, by = 2), function(x) cor(Correlation_halves[[x]]$V1, Correlation_halves[[x+1]]$V1))
# prepostComp <- data.frame(Community = tempComm, Correlation = tempCor)
# prepostComp <- melt(prepostComp)
# colnames(prepostComp) <- c("Approach", "Correlation")
# prepostComp_test <- permute(tempComm, tempCor, paired = T)
# 
# 
# ## Between halves analysis for correlation and communities (mPFC only)
# D1 <- tempComm_PFC_halves
# D2 <- lapply(tempComm_PFC_halves, function(data) transform(data, V1 = FiedlerVec)) # the comparison function can only take 1 column name for this stuff
# Data <- c(D1,D2)
# allHalfCombs <- comparePartitions(Data = Data, MOI = "V1", nSubjects = length(Data), Index = "Cor", subjNames = c(subjList_halves, subjList_halves))
# 

# Compare correlation with community approach on mPFC
tempComm <- sapply(seq(1, length(unthreshComm_PFC_halves), by = 2), function(x) cor(unthreshComm_PFC_halves[[x]]$FiedlerVec, unthreshComm_PFC_halves[[x+1]]$FiedlerVec))
tempCor <- sapply(seq(1, length(unthreshComm_PFC_halves), by = 2), function(x) cor(Correlation_halves[[x]]$V1, Correlation_halves[[x+1]]$V1))
tempBoth <- sapply(seq(1, length(unthreshComm_PFC_halves), by = 2), function(x) cor(unthreshComm_PFC_halves[[x]]$FiedlerVec, Correlation_halves[[x+1]]$V1))
prepostComp <- data.frame(Community = tempComm, Correlation = tempCor, Between = tempBoth)
prepostComp <- melt(prepostComp)
colnames(prepostComp) <- c("Approach", "Correlation")
prepostComp_permTest <- permute(tempComm, tempCor, paired = T)
prepostComp_ES <- cohenD(tempComm, tempCor)
```

The computational demands of our approach escalate quickly as the number of vertices increases. We thus explored the possible advantage to using community detection over a simple seed-based functional connectivity analysis from PCC. Given that other groups have found the spatial distribution of the DMN to be considerably stable within individuals [@Laumann2015; @Gordon2017; @Braga2017; @Kong2018], we examined whether maps generated with spectral partitioning were more similar per subject across days than those computed from seed-based correlations. First, we computed each individual's mean time series of all the vertices contained within PCC, and correlated it with the activity of every mPFC. Since correlation values are continuous, we compared them to their respective unbinarized Fiedler vector maps. We performed pairwise spatial Pearson correlations among maps computed for each day and method from all subjects. For `r nrow(unCorrParticipants %>% filter(Corrs < 0))` subjects, the maps produced with one of the days' datasets insufficiently covered area 7m, and so the community labeling was reversed (for one additional subject, both daily maps were reverted). ARI is robust to these labeling issues, but this produced strong negative correlations of the Fiedler vector for these subjects, even though the topography was highly similar. We thus visually inspected these subjects (n = 7) and reverted the labeling when it was clear that the topologies aligned across days. Figure 11 shows that these pairwise comparisons resemble those from the day-to-day comparisons above, and suggests good alingment between methods, but particularly high agreement within subject and method.

Figure 12 shows the overall degree of similarity across days for patterns dervied using community detection, seed-based correlation, and across methods (i.e. day one community detection versus day two seed-based correlation). While both approaches are very consistent, community detection displayed a significantly higher ARI across days than a simple correlation (paired permutation *p* < 0.001; Cohen's D = `r round(prepostComp_ES, digits = 2)`). It is worth noting that the cross-method agreement is fair, meaning that both approaches show similar topological features. These findings suggest that graph-theoretic community detection algorithms provide a better way to detect stable functional topologies.

### *Similarities with other work*

The thresholded partitions in our work qualitatively resembled the topographical arrangement of networks A and B proposed by Braga and Buckner [-@Braga2017]. We explored this similarity by reproducing their seed-based connectivity approach in three of our subjects. In their work, Braga and Buckner selected individual vertices along the dorsolateral prefrontal cortex (DLPFC) that produced these two anticorrelated networks. Therefore, we hypothesized that if the communities we identified on the medial wall matched their proposed networks, we should be able to find seed vertices in DLPFC that can reproduce them. The whole-brain seed-based functional connectivity maps for these subjects are juxtaposed to their spectral partitioning counterparts on figure 13.

Visual inspection of these networks shows a remarkable similarity between our DMN and non-DMN communities and their networks A and B. These qualitative results are important, as here we reinforce the idea that canonical DMN can be reliably subdivide at the individual level. Further, it shows the potential of community detection methods to automatically extract these partitions without the need to rely on group data. One way in which our results are at odds with those from Braga and Buckner is the functional labeling of these sub-networks. Based on our criteria, network B would more closely identify with DMN (per 7m affiliation), while we currently do not have enough evidence to label network A. These discrepancies should be addressed with explicit behavioral manipulations in the future.

## Discussion

A considerable amount of meta-analytic work has been dedicated to pinpointing the spatial embedding of behavioral constructs in medial prefrontal cortex (mPFC), revealing both dissociable and overlapping activation across psychological phenomena [@DelaVega2016; @Kragel2018; @Hiser2018]. In particular, the effects elicited by the subjective value (SV) of an option and the default mode network (DMN) have been suggested to be inseparable in this area, an overlap that partially extends to posterior cingulate cortex (PCC) [@Acikalin2017; @Bartra2013; @Clithero2014; @Laird2009]. However, recent studies have shown considerable variability of the brain's functional topography across individuals, emphasizing the importance of capturing its organization at the individual subject level, and pointing to limitations in aggregative estimations [@Braga2017; @Gordon2017; @Gratton2018a; @Kong2018; @Laumann2015; @Tobyne2018]. Therefore, determining the distinct functional mapping of psychological processes (and the potential extent of their overlap) first requires: 1) understanding and quantifying the degree of heterogeneity in the intrinsic functional organization of this area in a large group of individuals; and 2) a method to reliably and precisely capture it in isolated subjects.

Here we address these challenges by using spectral partitioning, a graph-theoretic clustering algorithm, to subdivide areas of overlap into DMN and non-DMN in 100 individuals from the Human Connectome Project. Our results show that the intrinsic functional topography of mPFC and PCC can be reliably partitioned into at least two idiosyncratically arranged networks that can be obscured when averaged across subjects. These sub-networks were stable over time and across test-retest days using moderate amounts of fMRI data (up to 30 min per day), and resembled an organizational scheme recently observed in a handful of subjects [@Braga2017]. We also show that spectral partitioning is capable of identifying these sub-networks in isolated individuals better than more direct seed-based functional connectivity approaches. 



Outline:

- Summarize the results
- Ours are not the first to show variability, but previous studies have focused on a relatively small number of subjects and often rely on qualitative assessment of these differences. They also use parcellations for networks, whereas we focused on individual vertices to gain spatial finesse. Also, to the best of our knowledge we are the first to quantify the organizational heterogeneity of these areas in the human brain. 
- We are also not the first to show functional subdivision of DMN [@Kernbach2018] (but that's using group averages on parcellated data), or this particular individualized pattern of subdivision (braga). Our results are complementary in the sense that we show similar outcomes in 100 subjects. The major discrepancy is that we assign meaning to the communities based on their coverage of regions identified through a meta-analysis.
- Talk about SP as a way to identify an individual's partition: deterministic, with simple diagnostics, and requires less data requirements.
- The main gap is mapping behavior to functional organization, but we will do that with behavioral manipulations of subjective value in the future. Instead, we focus on understanding the importance of individualized mapping in these areas first, as a baseline.
- Clinical implications: individualized targets, and more accurate estimation of impacts based on assessment of individual patients. Similarly, our approach focuses on individual estimations without group priors, so it's well fit to address the problem of low n in clinical research. Also, SP can produce these effects with moderate amounts of data. Previous results suggest impractical amounts.

- Limitations
  - community detection ground truth
  - other algorithms (benefit of SP is that it's deterministic, allows for diagnostics, and discourages extremely large/small communities)
  - bipartition: we focus on that since we tried to capture what belongs to DMN and non-DMN. This intuition can be generalized to other behaviors. SP can be expanded to extract additional communities in combination with k-means and the identification of the first non-zero eigenvalue in the low-high rank.
  - Small search space: working at the vertex-level makes it computationally difficult to produce estimates at the whole-brain level. 





For clinical implications, see Hiser & Koenigs 2018. In short, being able to identify functional idiosyncrasies without much data can yield important insights in diagnosis (meh, we always say that..) of a variety of psychological diseases associated with mPFC. It would be more interesting to identify the repercussions of damage in mPFC per individual. They mention that subgenual vmPFC is a common target for deep brain stimulation (or TMS) as a depression treatment. The refinement of the technique is thus coupled to being able to identify personalized functional topologies. They list a number of results related to mPFC functional connectivity too. In the end, their suggestions for clinical implications of mPFC studies rely on the individual measure of functional expanse. (note that social-relevant areas in their meta analysis look similar to our patterns, and the emotion part to Barbas' latest)

It will also be good to note that there is concurrent debate on the ground truth of community detection. In other fields, identifying a community structure is not necessarily associated with the true organization of the network. What happens is that you might find a tangential grouping that gives you information different from your original goal [@Clauset2004]. In our case, however, averaging everyone's partitions produces organizational maps that resemble those from previous work (e.g. Margulies).

Talk about how there are other community detection algorithms, and that modularity and SP over and underfit, respectively. However, SP eigenmaps have been found to provide relevant functional information. In addition, both modularity and SP rely on connectivity strength, and thus is amenable to the correlative nature of functional connectivity.


And write a portion on the lack of subcortical reward-related regions (i.e. striatum). Say something like 'incorporating the ventral striatum did not refine these partitions through either method. This could be due to the lack of signal modulation of this area at rest, and should be examined through behavioral manipulations of value in the future. Instead, here we focus on capturing the intrinsic functional heterogeneity of mPFC as a way to pave the path to refined analyses aimed at disentangling DMN and SV'. It is worth noting that the SV-only area from the meta-analysis tends to land in non-DMN, although it ends up in no-ones land when thresholding the maps.

It might also be worth mentioning that previous work has established the shift in network types during rest through hidden Markov models (Vidaurre et al., 2016?). While this suggests that we should see fluctuations in affiliation, and not stability, Vidaurre's work establishes the dynamics of prevalent function among networks, and not their topologies. Our work focuses on the topographical arrangement of the DMN network, which can be active or not while maintaining its form. So, the two works are compatible. Note, I played a bit with AR and MA processes per subject to try to understand the autocorrelation in the sliding window. First, both acf/pacf and arima (order = n,0,n) R functions suggested that the data was better fit by AR. Moreover, ARs from 1 to 10 had neglegible differences, so I stuck with 1 results for simplicity. As expected the data is significantly autocorrelated, and when I checked plotted the residuals they were mostly within the 0.15 bound (normally distributed). Coefficients ranged from 0.6 to 1. So, there is clear rythmicity in the agreement between each window and the overall pattern. However, the intercepts from the ARn processes gave more or less the same estimates as the mean rand index over all the windows. This is mostly expected, as we saw that the standard deviation of the rand index was low across participants. I am currently interpreting this as a signal that there is clear periodicity that is otherwise harmless for our results: in the end, the agreement remains stable throughout the 4800 TRs.

In terms of noise, there are a handful of things to keep in mind. First is that yes, mPFC is noisier overall, but that did not preclude the relatively high levels of agreement across days and sessions per subject, even though ARI heavily penalizes discrepancies (less than linearly). Second, low $R^2$ values were found all throughout the z-axis, meaning that variance explained is not strictly determined by the cardinal position of the vertex (and while the correlation was significant due to the sheer number of items, the coefficient was relatively low). Right now this might be ok as a way to bypass noise comments, but it would also be interesting to take advantage of the Fiedler vector here. Since it's linearly related to $R^2$ per subject, perhaps finding a way to threshold these values would provide cleaner and even more consistent maps. 

So what's the advantage of this work over previous ones? Gratton, Gordon, and Kong have already found some degree of subject stability and intersubject variability. Ours complements these findings by looking at the vertex level, not just mean parcel activity (this made sense for them, as they were looking at the whole brain). Also, most of these analyses rely on a small set of oversampled subjects (i.e. MSC). Here we extend this to 100, giving credit to individualize examinations even without oversampling. 

Make sure to emphasize that the high overlap between sliding windows makes it more likely that differences are due to noise in the signal, and not strong variability. Like, you wouldn't expect the actual network distribution to shift that much after shifting the time series by 1 minute.


\newpage
## Acknowledgements

We would like to thank Dan Sussman for initial guidance on community detection and evaluation methods. Data were provided [in part] by the Human Connectome Project, WU-Minn Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers that support the NIH Blueprint for Neuroscience Research; and by the McDonnell Center for Systems Neuroscience at Washington University.


## Supplemental Materials


### *Table 1. Regions of interest from Glasser et al. (2016). Prefixes indicate hemispheric location (e.g. R = right).*
| Literature           | Significant Brain Regions                 |
| :------------------- | ----------------------------------------: |
| ROIs                 | `r str_remove(Yeo_labels, "_ROI")` |


## Figures

```{r Meta-analysis ROIs (Fig. 1), out.width="50%", fig.align = "center", fig.cap= "A: Proportion of times each ROI was reported in each literature. B: common and unique brain regions associated with DMN and subjective value. DMN regions defined by Yeo et al. (2011) in blue, which encompass areas of considerable overlap between DMN and SV. Literature-specific regions (bilateral 7m and right v23 in yellow, and bilateral 25 in red) were identified from a permutation analysis of chi-squared tests comparing these literatures."}
# Change to inflated eventually
include_graphics("./Images/ROIS4.png")
```

\bigskip

```{r Template partition (Fig. 2), out.width="50%", fig.align = "center", fig.cap = "Brain partition for an example subject (100307). Fiedler vector values (top) are mapped onto the brain surface, dividing it into positive and negative communities. The bottom brain shows the binarized Fiedler vector, with yellow areas denoting the DMN community (as indicated by coverage of area 7m, bordered)."}
include_graphics("./Images/FB_FV_example.png")
```

\bigskip

``` {r Partition comparison: all 2 (Fig. 3), fig.align="center", out.width = "50%", fig.cap = "Similarity matrix showing ARI values among all subjects for PCC (lower triangle) and mPFC (upper triangle) separately. PCC is better aligned across individuals than mPFC.", echo=FALSE, message = FALSE}
# code to produce the plot
# par(xpd = T)
# corrplot(abs(RI_PCC), # absolute for display purposes only, as it avoids the couple of negative ARIs and evens out the color bar
#          is.corr = F, 
#          method = "color",
#          type = "lower", 
#          tl.pos = "n", 
#          tl.cex = 0.1, 
#          cl.lim = c(0,1),
#          cl.length = 3,
#          #addgrid.col = "grey99",
#          cl.pos = "r",
#          mar = c(1,1,2,0))
# corrplot(RI_PFC, 
#          is.corr = F, 
#          method = "color", 
#          type = "upper", 
#          tl.pos = "n", 
#          tl.cex = 0.1,
#          cl.pos = "n",
#          #addgrid.col = "grey90",
#          mar = c(1,1,2,0), 
#          add = T)
# segments(0.5,0.5,0.5,100.5, lwd = 2)
# segments(0.5,0.5,100.5,0.5, lwd = 2)
# segments(0.5,100.5,100.5,100.5, lwd = 2)
# segments(100.5,0.5,100.5,100.5, lwd = 2)
# mtext(expression(bold("Subject Partitions")), line = 3)
# mtext(expression(bold("Subject Partitions")), side = 2, line = -0.8)
# mtext(expression(bold("mPFC")), line = 2, at = par("usr")[1]+0.73*diff(par("usr")[1:2]), col = "steelblue4")
# mtext(expression(bold("PCC")), side = 1, line = 4, at = par("usr")[1]+0.18*diff(par("usr")[1:2]), col = "steelblue4")

include_graphics("./Images/similarity_all.png")

```

\bigskip

``` {r Stability per subject: Fiedler vector vs prop. DMN plot (Fig 5), fig.align="center", fig.width=3, fig.height=3, fig.cap = "A: For each subject, we produced partitions for each 20 minute sliding window (1 min shift, ~84 TRs) throughout their full time series (median number of windows = 37). B: Proportion of times each vertex was affiliated with the DMN community across windows (up), and the continuous Fiedler vector map for the current subject using their full time series (down). The maps share considerable qualitative similarities in their gradients along the cortical surface. C: Relationship between the magnitude of Fiedler vector values and the proportion of DMN affiliations. Grey lines display each subject's association, while the black line shows the fit from a mixed-effects logistic regression. Dashed red lines indicate the mean FV value at which maps were thresholded.", echo=FALSE, message = FALSE}
# code to generate plots

# # time series
# # this one uses the actual subject's loaded time series (not included due to space)
# n <- 4 # n of vertices to include
# tr <- 4800 
# ts <- data.matrix(t1[sample(59000, n), 1:tr])
# df <- tibble(vertex = rep(seq(n), each = tr),
#              Time = floor(rep(seq(0, tr, length.out = tr), n)),
#              Signal = matrix(ts, ncol = 1) + rep(seq(-1, 1, length.out = n), each = tr)) %>%
#   ggplot(data = df, aes(Time, Signal, group = vertex, color = as.character(vertex))) +
#     geom_line(show.legend = F, lwd = 0.2, alpha = 0.6) +
#     theme_classic()
# 
# # Fiedler Vector x proportion of times a vertex was affiliated with DMN
# SC_FVvPDMN_plot <- ggplot(data = allSummaries) +
#   geom_smooth(aes(FiedlerVec, slidePropDMN, group = SubjID), color = "gray", fill = "gray", show.legend = F, alpha = 0.3) +
#   geom_line(data = SC_FVvPDMN_fits, aes(FiedlerVector, Fitted_Proportion), lwd = 2) +
#   geom_vline(xintercept = c(-0.014, 0.013), linetype = "dashed", color = Cols[2]) +
#   labs(x = "Fiedler Vector Values", y = "Proportion of DMN Affiliations") +
#   scale_y_continuous(limits = c(0,1)) +
#   theme_classic()
# 
# SC_FVvPDMN_plot

include_graphics("./Images/SW_props.png")

```

\bigskip

```{r Thresholded maps (Fig 7), out.width = "50%", fig.align = "center", fig.cap= "A: Thresholded Fiedler vector maps for subject 100307 (top), and its binarized form (bottom). Values that did not meet the threshold were set to zero on the Fiedler vector, and 0.5 on the binarized maps (giving rise to a third community of high-variability vertices). B: Spatial mean of the binarized maps across all participants, indicating the proportion of DMN affiliations per vertex in our sample. This aggregate map shows the common organizational principle of the DMN and non-DMN communities, while also pinpointing the relatively high level of variability in mPFC."}

include_graphics("./Images/thresholded.png")

```

\bigskip

``` {r Partition comparison: halves and runs (Fig. 8), fig.align="center", out.width = "100%", fig.cap = "Left: Similarity matrix for 10 example participants (2 days of scans each), showing pattern agreement across days and subjects for PCC and mPFC separately. Middle: ratio of within-subject ARI to between-subject mean ARI for all individuals across days suggests idiosyncrasies in community arrangement for both PCC and mPFC (ratios > 1, solid line), with higher subject-specificity in the latter. Right: within-to-between subject mean ARI ratios for each subject again show higher idiosyncratic organization for mPFC.", echo=FALSE, message = FALSE}
### code for plots in this fig

## similarity matrix
# corrplot(abs(RI_PCC_halves_plot), # absolute to get rid of that annoying -0.02 that messes up the limits. This is just for example.
#          is.corr = F,     
#          method = "color",
#          type = "lower", 
#          tl.pos = "n", 
#          cl.lim = c(0,1),
#          cl.pos = "r",
#          addgrid.col = "grey90",
#          cl.length = 3)
# corrplot(RI_PFC_halves_plot, 
#          is.corr = F, 
#          method = "color",
#          type = "upper", 
#          tl.pos = "n", 
#          cl.pos = "n", 
#          addgrid.col = "grey90",
#          add = T)
#mtext(expression(bold("Subjects (Day 1 and Day 2)")), line = 1)
#mtext(expression(bold("mPFC")), line = -0.15, at = par("usr")[1]+0.8*diff(par("usr")[1:2]), col = "steelblue4")
#mtext(expression(bold("PCC")), side = 1, line = 0.6, at = par("usr")[1]+0.09*diff(par("usr")[1:2]), col = "steelblue4")

## ARI for days
# ggplot(data = Halves_RIRatios_summary, aes(Region, RI, fill = Region)) + 
#   geom_boxplot(show.legend = F) +
#   scale_fill_manual(values = Cols[seq(2)]) + 
#   ylim(0, 20) +
#   geom_hline(yintercept = 1) +
#   labs(y = "ARI Ratio", x = "") + 
#   theme_classic(base_size = 14)

## ARI for session
# ggplot(data = Sess_RIRatios_summary, aes(Region, RI, fill = Region)) + 
#   geom_boxplot(show.legend = F) +
#   scale_fill_manual(values = Cols[seq(2)]) + 
#   ylim(0, 20) +
#   geom_hline(yintercept = 1) +
#   labs(y = "ARI Ratio", x = "") + 
#   theme_classic(base_size = 14)


include_graphics("./Images/ratios.png")


```

\bigskip

``` {r Correlation vs community detection (Fig. 14), fig.align="center", fig.cap = "A: Correlation matrix comparing the inter-day spatial stability of maps derived from seed-based functional connectivity (PCC-based) and the Fiedler vector for 10 example subjects. The top-left quadrant represents seed-based FC maps, and the bottom-right the Fiedler vector. There is high agreement within method (diagonal quadrants), but both methods suggest similar partitions (off-diagonal quadrants). B: Day 1 vs Day 2 correlation coefficients for every method, as well as between methods. Community detection through spectral partitioning provides the most stable inter-day estimates, even though both methods show good level of agreement.", echo = FALSE, message = FALSE}
# code to produce the plots

# correlation matrix
# corrplot(allHalfCombs, 
#          #title = "Comparing All Community and Correlation mPFC Partitions \n Across All Subjects",
#          method = "color", 
#          #tl.col = "black",
#          #tl.cex = 0.6,
#          addgrid.col = "grey90",
#          tl.pos = "n",
#          cl.pos = "r",
#          cl.length = 3,
#          mar = c(1,2,2,0))
# segments(0.5,0.5,0.5,40.5, lwd = 2)
# segments(0.5,0.5,40.5,0.5, lwd = 2)
# segments(0.5,40.5,40.5,40.5, lwd = 2)
# segments(40.5,0.5,40.5,40.5, lwd = 2)
# segments(0.5,20.5,40.5,20.5, lwd = 2)
# segments(20.5,0.5,20.5,40.5, lwd = 2)
# mtext(expression(bold("Subject Partitions (Day 1 and Day 2)")), line = 3, at = par("usr")[1]+0.45*diff(par("usr")[1:2]))
# mtext(expression(bold("Subject Partitions (Day 1 and Day 2)")), side = 2, line = 3)
# mtext(expression(bold("Fiedler")), line = 1.5, at = par("usr")[1]+0.68*diff(par("usr")[1:2]))
# mtext(expression(bold("Correlation")), line = 1.5, at = par("usr")[1]+0.23*diff(par("usr")[1:2]))
# mtext(expression(bold("Fiedler")), side = 2, line = 2, at = par("usr")[1]+0.25*diff(par("usr")[1:2]))
# mtext(expression(bold("Correlation")), side = 2, line = 2, at = par("usr")[1]+0.68*diff(par("usr")[1:2]))
# 
# # Day 1 vs Day 2 Spatial Correlation Comparison
# prepostComp_plot <- ggplot(data = prepostComp, aes(Approach, Correlation, fill = Approach)) +
#   geom_boxplot(show.legend = F) +
#   labs(x = "", y = "Correlation Coefficient") +
#   ylim(0,1) +
#   theme(text = element_text(size=35)) +
#   theme_classic()
# prepostComp_plot

include_graphics("./Images/commvscorr.png")

```

\bigskip

```{r BandB comparison (Figure 16), out.width = "100%", fig.align = "center", fig.cap= "Qualitative comparison between maps produced by seed-based connectivity based on Braga and Buckner's (2017) criterion, and those produced by spectral partitioning. Networks A and B were produced by carefully selecting a DLPFC seed (pointed by the green arrows). In each case, our DMN and non-DMN networks map remarkably well to their proposed functional organization."}

# Change to inflated eventually
include_graphics("./Images/braga_comparison.png")

```

\newpage 

## References
