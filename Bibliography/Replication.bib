Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Nuzzo2014,
abstract = {P values, the 'gold standard' of statistical validity, are not as reliable as many scientists assume.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Nuzzo, Regina},
doi = {10.1136/bmj.1.6053.66},
eprint = {1011.1669},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Nuzzo - 2014 - Statistical errors P values, the gold standard of statistical validity, are not as reliable as many scientists assume(2).pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {0959-8138},
journal = {Nature},
number = {7487},
pages = {150--152},
pmid = {9441199},
title = {{Statistical errors: P values, the "gold standard" of statistical validity, are not as reliable as many scientists assume.}},
volume = {506},
year = {2014}
}
@article{John2012,
abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who  have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
doi = {10.1177/0956797611430953},
eprint = {arXiv:1011.1669v3},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/John, Loewenstein, Prelec - 2012 - Measuring the Prevalence of Questionable Research Practices With Incentives for Truth Telling.pdf:pdf},
isbn = {1011770956},
issn = {14679280},
journal = {Psychological Science},
keywords = {disclosure,judgment,methodology,professional standards},
number = {5},
pages = {524--532},
pmid = {22508865},
title = {{Measuring the Prevalence of Questionable Research Practices With Incentives for Truth Telling}},
volume = {23},
year = {2012}
}
@article{Ioannidis2005,
abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
archivePrefix = {arXiv},
arxivId = {gr-qc/0208024},
author = {Ioannidis, John P A},
doi = {10.1371/journal.pmed.0020124},
eprint = {0208024},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Ioannidis - 2005 - Why most published research findings are false.pdf:pdf},
isbn = {3540239081},
issn = {15491277},
journal = {PLoS Medicine},
number = {8},
pages = {0696--0701},
pmid = {16060722},
primaryClass = {gr-qc},
title = {{Why most published research findings are false}},
volume = {2},
year = {2005}
}
@article{Cambier2018,
abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p {\textless} .05), we found that 15 (54{\%}) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p {\textless} .0001), 14 (50{\%}) of the replications still provided such evidence, a reflection of the extremely high- powered design. Seven (25{\%}) of the replications yielded effect sizes larger than the original ones, and 21 (75{\%}) yielded effect sizes smaller than the original ones. The median comparable Cohen's ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small ({\textless} 0.20) in 16 of the replications (57{\%}), and 9 effects (32{\%}) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39{\%}) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
author = {Cambier, Fanny and Gill, Tripat and Pollmann, Monique M. H. and P{\'{e}}rez-S{\'{a}}nchez, Rolando and Wichman, Aaron L. and Dozo, Nerisa and Murphy, Sean C. and Aveyard, Mark and Grahek, Ivan and Wood, Michael and Szumowska, Ewa and de Vries, Marieke and Busching, Robert and van der Hulst, Marije and van Assen, Marcel A. L. M. and Bialobrzeska, Olga and Verniers, Catherine and Ceric, Francisco and Ocampo, Aaron and Adams, Reginald B. and Sowden, Walter and Vianello, Michelangelo and Voermans, Ingrid P. J. and Dagona, Zubairu K. and Giessner, Steffen R. and Chang, Jen-Ho and Oikawa, Masanori and Zelenski, John M. and Dunham, Yarrow and Steiner, Troy G. and Gandi, Joshua C. and Sekerdej, Maciej B. and Conway, Morgan A. and Smith-Castro, Vanessa and Spachtholz, Philipp and Joy-Gaba, Jennifer A. and Green, Eva G. T. and Nosek, Brian A. and Finck, Carolyn and Packard, Grant and Collisson, Brian and Coen, Sharon and Frankowska, Natalia and de Bruijn, Maaike and Tear, Morgan J. and Pinter, Brad and {Ann Vaughn}, Leigh and Curran, Paul G. and English, Alexander Scott and Inbar, Yoel and Tanzer, Norbert and Cheong, Winnee and Batra, Rishtee and Szeto, Stephanie and Saeri, Alexander K. and Keller, Victor N. and Zhijia, Zeng and Kervyn, Nicolas and Ujhelyi, Adrienn and Neijenhuijs, Koen and Huntsinger, Jeffrey R. and Hai, Kakul and Lazarevi{\'{c}}, Ljiljana B. and Cai, Huajian and Tybur, Joshua M. and Ghoshal, Tanuka and Hasselman, Fred and Levitan, Carmel A. and Chandler, Jesse and Alper, Sinan and Salomon, Erika and Rutchick, Abraham M. and Marotta, Satia A. and Vega, Luis Diego and Theriault, Jordan and Jim{\'{e}}nez-Leal, William and Kende, Anna and Bocian, Konrad and Losee, Joy E. and Huynh, Ho Phi and Krueger, Lacy E. and Haigh, Matthew and Coleman, Jennifer A. and Axt, Jordan R. and Mallett, Robyn K. and Dalgar, Ilker and Ong, Elsie and Hall, Michael P. and Grahe, Jon E. and Bernstein, Michael J. and Eller, Anja and Carmichael, Cheryl L. and Gonz{\'{a}}lez, Roberto and Hicks, Joshua A. and Berkics, Mih{\'{a}}ly and {van 't Veer}, Anna Elisabeth and Sobkow, Agata and Innes-Ker, {\AA}se H. and Neto, F{\'{e}}lix and Cicero, David C. and Brandt, Mark J. and KamiloÄŸlu, Roza G. and Devos, Thierry and Smolders, Karin C. H. J. and Verschoor, Mark and van Aert, Robbie C. M. and Williams, Lisa A. and Kovacs, Carrie and Sch{\"{o}}nbrodt, Felix D. and Davis, William E. and Orosz, G{\'{a}}bor and Houdek, Petr and Bahn{\'{i}}k, {\v{S}}t{\v{e}}p{\'{a}}n and {V{\'{a}}squez- Echeverr{\'{i}}a}, Alejandro and Lins, Samuel and Tang, Andrew C. W. and Kurtz, Jamie and Babalola, Mayowa T. and Malingumu, Winfrida and Osowiecka, Malgorzata and Maitner, Angela T. and Kappes, Heather Barry and Kurapov, German and Corker, Katherine S. and Graham, Jesse and Kne{\v{z}}evi{\'{c}}, Goran and Sundfelt, Oskar K. and van Lange, Paul A. M. and Klein, Richard A. and Karick, Haruna and Welch, Cheryl and Skorinko, Jeanine L. M. and Mena-Pacheco, Fernando and Lipsey, Nikolette P. and Pogge, Gabrielle and John, Melissa-Sue and Traczyk, Jakub and Sirlop{\'{u}}, David and Karabati, Serdar and Cushman, Fiery and Oikawa, Haruka and Neave, Nick and Adams, Byron G. and Schmidt, Kathleen and Dukes, Kristin Nicole and Srivastava, Manini and Chen, Eva E. and Thomae, Manuela and Wronska, Marta K. and Galliani, Elisa Maria and Haines, Elizabeth L. and Edlund, John E. and Petrovi{\'{c}}, Boban and Street, Chris N. H. and Cantarero, Katarzyna and Gnambs, Timo and Vranka, Marek A. and G{\'{o}}mez, {\'{A}}ngel and Berry, Daniel R. and Smith, Michael A. and Woodzicka, Julie A. and Nelson, Anthony J. and R{\'{e}}dei, Anna Cabak and {De Schutter}, Leander and Chatard, Armand and Milfont, Taciano L. and Heffernan, Marie E. and Lewis, Neil A. and Stouten, Jeroen and Young, Liane and Maassen, Esther and IJzerman, Hans and Binan, Evans Dami and Morris, Wendy L. and Myachykov, Andriy and Pilati, Ronaldo and MeÄ‘edovi{\'{c}}, Janko and V{\'{a}}zquez, Alexandra and Friedman, Mike and Lakens, Dani{\"{e}}l and Podesta, Lysandra and Saavedra, Patricio and Freyre, Miguel-{\'{A}}ngel and {Dalla Rosa}, Anna and DoÄŸulu, Canay and Torres, David and Durrheim, Kevin and O'Donnell, Susan L. and {Lee Nichols}, Austin and Ebersole, Charles R.},
doi = {10.1177/2515245918810225},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Cambier et al. - 2018 - Many Labs 2 Investigating Variation in Replicability Across Samples and Settings.pdf:pdf},
issn = {2515-2459},
journal = {Advances in Methods and Practices in Psychological Science},
number = {4},
pages = {443--490},
title = {{Many Labs 2: Investigating Variation in Replicability Across Samples and Settings}},
volume = {1},
year = {2018}
}
