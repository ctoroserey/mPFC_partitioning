Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Davis-Stoberid2018,
abstract = {Sample means comparisons are a fundamental and ubiquitous approach to interpreting experimental psychological data. Yet, we argue that the sample and effect sizes in published psychological research are frequently so small that sample means are insufficiently accurate to determine whether treatment effects have occurred. Generally, an estimator should be more accurate than any benchmark that systematically ignores information about the relations among experimental conditions. We consider two such benchmark estimators: one that randomizes the relations among conditions and another that always assumes no treatment effects. We show conditions under which these benchmark estimators estimate the true parameters more accurately than sample means. This perverse situation can occur even when effects are statistically significant at traditional levels. Our argument motivates the need for regularized estimates, such as those used in lasso, ridge, and hierarchical Bayes techniques.},
author = {Davis-Stoberid, Clintin P and Dana, Jason and Rouder, Jeffrey N},
doi = {10.1371/journal.pone.0207239},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Davis-Stoberid, Dana, Rouder - 2018 - Estimation accuracy in the psychological sciences.pdf:pdf},
isbn = {1111111111},
keywords = {stats},
mendeley-tags = {stats},
title = {{Estimation accuracy in the psychological sciences}},
year = {2018}
}
@article{Nuzzo2014,
abstract = {P values, the 'gold standard' of statistical validity, are not as reliable as many scientists assume.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Nuzzo, Regina},
doi = {10.1136/bmj.1.6053.66},
eprint = {1011.1669},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Nuzzo - 2014 - Statistical errors P values, the gold standard of statistical validity, are not as reliable as many scientists assume(2).pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {0959-8138},
journal = {Nature},
number = {7487},
pages = {150--152},
pmid = {9441199},
title = {{Statistical errors: P values, the "gold standard" of statistical validity, are not as reliable as many scientists assume.}},
volume = {506},
year = {2014}
}
@article{Ng2004,
abstract = {We consider supervised learning in the presence of very many irrelevant features, and study two different regularization methods for preventing overfitting. Focusing on logistic regression, we show that using L1 regularization of the parameters, the sample complexity (i.e., the number of training examples required to learn "well,") grows only logarithmically in the number of irrelevant features. This logarithmic rate matches the best known bounds for feature selection, and indicates that L1 regularized logistic regression can be effective even if there are exponentially many irrelevant features as there are training examples. We also give a lower-bound showing that any rotationally invariant algorithm---including logistic regression with L2 regularization, SVMs, and neural networks trained by backpropagation---has a worst case sample complexity that grows at least linearly in the number of irrelevant features.},
author = {Ng, Andrew Y},
doi = {10.1145/1015330.1015435},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Ng - 2004 - Feature selection, L 1 vs. L 2 regularization, and rotational invariance(2).pdf:pdf},
isbn = {1581138285},
issn = {0253-0465},
journal = {Twenty-first international conference on Machine learning - ICML '04},
pages = {78},
pmid = {15040217},
title = {{Feature selection, L 1 vs. L 2 regularization, and rotational invariance}},
url = {http://doi.acm.org/10.1145/1015330.1015435{\%}5Cnhttp://portal.acm.org/citation.cfm?doid=1015330.1015435},
year = {2004}
}
@article{Poline2012,
abstract = {In this review, we first set out the general linear model (GLM) for the non technical reader, as a tool able to do both linear regression and ANOVA within the same flexible framework. We present a short history of its development in the fMRI community, and describe some interesting examples of its early use. We offer a few warnings, as the GLM relies on assumptions that may not hold in all situations. We conclude with a few wishes for the future of fMRI analyses, with or without the GLM. The appendix develops some aspects of use of contrasts for testing for the more technical reader. ?? 2012 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Poline, Jean Baptiste and Brett, Matthew},
doi = {10.1016/j.neuroimage.2012.01.133},
eprint = {arXiv:1011.1669v3},
isbn = {1095-9572 (Electronic)$\backslash$r1053-8119 (Linking)},
issn = {10538119},
journal = {NeuroImage},
keywords = {Functional MRI,General linear model},
number = {2},
pages = {871--880},
pmid = {22343127},
publisher = {Elsevier Inc.},
title = {{The general linear model and fMRI: Does love last forever?}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2012.01.133},
volume = {62},
year = {2012}
}
@article{Tomczak2014,
abstract = {Recent years have witnessed a growing number of published reports that point out the need for reporting various effect size estimates in the context of null hypothesis testing (H 0) as a response to a tendency for reporting tests of statistical significance only, with less attention on other important aspects of statistical analysis. In the face of considerable changes over the past several years, neglect to report effect size estimates may be noted in such fields as medical science, psychology, applied linguistics, or pedagogy. Nor have sport sciences managed to totally escape the grips of this suboptimal practice: here statistical analyses in even some of the current research reports do not go much further than computing p-values. The p-value, however, is not meant to provide information on the actual strength of the relationship between variables, and does not allow the researcher to determine the effect of one variable on another. Effect size measures serve this purpose well. While the number of reports containing statistical estimates of effect sizes calculated after applying parametric tests is steadily increasing, reporting effect sizes with non-parametric tests is still very rare. Hence, the main objectives of this contribution are to promote various effect size measures in sport sciences through, once again, bringing to the readers' attention the benefits of reporting them, and to present examples of such estimates with a greater focus on those that can be calculated for non-parametric tests.},
author = {Tomczak, M and Tomczak, E},
doi = {10.7326/0003-4819-159-3-201308060-00005},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Tomczak, Tomczak - 2014 - The need to report effect size estimates revisited. An overview of some recommended measures of effect size.pdf:pdf},
issn = {22999590},
journal = {Trends in Sport Sciences},
keywords = {effect size calculation,methodology,non-parametric tests,parametric,sport science,sport science, effect size calculation, parametric,tests},
number = {21},
pages = {19--25},
title = {{The need to report effect size estimates revisited. An overview of some recommended measures of effect size}},
volume = {1},
year = {2014}
}
@article{Examples2014,
author = {Examples, Data Analysis and Effects, Mixed and Regression, Logistic},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Examples, Effects, Regression - 2014 - R Data Analysis Examples Mixed Effects Logistic Regression(2).pdf:pdf},
pages = {1--13},
title = {{R Data Analysis Examples : Mixed Effects Logistic Regression}},
year = {2014}
}
@article{Wasserstein2016,
abstract = {Additional reading: http://www.nature.com/news/statisticians-issue-warning-over-misuse-of-p-values-1.19503},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
doi = {10.1080/00031305.2016.1154108},
eprint = {1011.1669},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Wasserstein, Lazar - 2016 - The ASA's Statement on ipi -Values Context, Process, and Purpose(2).pdf:pdf},
isbn = {0003-1305 1537-2731},
issn = {0003-1305},
journal = {The American Statistician},
number = {2},
pages = {129--133},
pmid = {25246403},
publisher = {Taylor {\&} Francis},
title = {{The ASA's Statement on {\textless}i{\textgreater}p{\textless}/i{\textgreater} -Values: Context, Process, and Purpose}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108},
volume = {70},
year = {2016}
}
@article{Hastie2009,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come a vast amount of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hastie, T; Tibshirani},
doi = {10.1007/b94608},
eprint = {arXiv:1011.1669v3},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Hastie - 2009 - The Elements of Statistical Learning(2).pdf:pdf},
isbn = {978-0-387-84857-0},
issn = {03436993},
journal = {Math. Intell.},
number = {2},
pages = {83--85},
pmid = {15512507},
title = {{The Elements of Statistical Learning}},
url = {http://www.springerlink.com/index/10.1007/b94608},
volume = {27},
year = {2009}
}
@book{Boyd2017,
abstract = {This groundbreaking textbook combines straightforward explanations with a wealth of practical examples to offer an innovative approach to teaching linear algebra. Requiring no prior knowledge of the subject, it covers the aspects of linear algebra - vectors, matrices, and least squares - that are needed for engineering applications, discussing examples across data science, machine learning and artificial intelligence, signal and image processing, tomography, navigation, control, and finance. The numerous practical exercises throughout allow students to test their understanding and translate their knowledge into solving real-world problems, with lecture slides, additional computational exercises in Julia and MATLAB, and data sets accompanying the book online. It is suitable for both one-semester and one-quarter courses, as well as self-study, this self-contained text provides beginning students with the foundation they need to progress to more advanced study.},
author = {Boyd, S and Vandenberghe, L},
doi = {10.1017/9781108583664},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Boyd, Vandenberghe - 2017 - Introduction to Applied Linear Algebra Vectors, Matrices, and Least Squares.pdf:pdf},
isbn = {978-1-316-51896-0},
pages = {476},
title = {{Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares}},
url = {https://web.stanford.edu/{~}boyd/vmls/},
year = {2017}
}
@article{Rissman2004,
abstract = {The inherently multivariate nature of functional brain imaging data affords the unique opportunity to explore how anatomically disparate brain areas interact during cognitive tasks. We introduce a new method for characterizing inter-regional interactions using event-related functional magnetic resonance imaging (fMRI) data. This method's principle advantage over existing analytical techniques is its ability to model the functional connectivity between brain regions during distinct stages of a cognitive task. The method is implemented by using separate covariates to model the activity evoked during each stage of each individual trial in the context of the general linear model (GLM). The resulting parameter estimates (beta values) are sorted according to the stage from which they were derived to form a set of stage-specific beta series. Regions whose beta series are correlated during a given stage are inferred to be functionally interacting during that stage. To validate the assumption that correlated fluctuations in trial-to-trial beta values imply functional connectivity, we applied the method to an event-related fMRI data set in which subjects performed two sequence-tapping tasks. In concordance with previous electrophysiological and fMRI coherence studies, we found that the task requiring greater bimanual coordination induced stronger correlations between motor regions of the two hemispheres. The method was then applied to an event-related fMRI data set in which subjects performed a delayed recognition task. Distinct functional connectivity maps were generated during the component stages of this task, illustrating how important and novel observations of neural networks within the isolated stages of a cognitive task can be obtained. {\textcopyright} 2004 Elsevier Inc. All rights reserved.},
author = {Rissman, Jesse and Gazzaley, Adam and D'Esposito, Mark},
doi = {10.1016/j.neuroimage.2004.06.035},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Rissman, Gazzaley, D'Esposito - 2004 - Measuring functional connectivity during distinct stages of a cognitive task.pdf:pdf},
isbn = {1053-8119 (Print)$\backslash$r1053-8119 (Linking)},
issn = {10538119},
journal = {NeuroImage},
keywords = {Correlation,Delay period,Functional connectivity,Network,Working memory},
number = {2},
pages = {752--763},
pmid = {15488425},
title = {{Measuring functional connectivity during distinct stages of a cognitive task}},
volume = {23},
year = {2004}
}
@book{Kolaczyk2009,
abstract = {In the past decade, the study of networks has increased dramatically. Researchers from across the sciences—including biology and bioinformatics, computer science, economics, engineering, mathematics, physics, sociology, and statistics—are more and more involved with the collection and statistical analysis of network-indexed data. As a result, statistical methods and models are being developed in this area at a furious pace, with contributions coming from a wide spectrum of disciplines. This book provides an up-to-date treatment of the foundations common to the statistical analysis of network data across the disciplines. The material is organized according to a statistical taxonomy, although the presentation entails a conscious balance of concepts versus mathematics. In addition, the examples—including extended cases studies—are drawn widely from the literature. This book should be of substantial interest both to statisticians and to anyone else working in the area of ‘network science.'},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kolaczyk, Eric D.},
booktitle = {Design},
doi = {10.1007/978-0-387-88146-1},
eprint = {arXiv:1011.1669v3},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Kolaczyk - 2009 - Statistical Analysis of Network Data(3).pdf:pdf},
isbn = {9780387881454},
issn = {0006341X},
keywords = {complex networks,network,network analysis,network modeling,network statistical},
pages = {214},
pmid = {15772297},
title = {{Statistical Analysis of Network Data}},
url = {http://link.springer.com/10.1007/978-1-4939-0983-4},
year = {2009}
}
@article{Reverdy2015,
abstract = {—We contribute to the development of a systematic means to infer features of humandecision-making frombehavioral data.Motivated by the common use of softmax selection inmodels of human decision-making, we study the maximum-likelihood (ML) parameter estimation problem for softmax decision-making models with linear objective functions. We present conditions under which the likelihood function is convex. These allow us to provide sufficient conditions for convergence of the resulting ML estimator and to construct its asymptotic distribution. In the case of models with nonlinear objective functions, we show how the es- timator can be applied by linearizing about a nominal parameter value.We apply the estimator to fit the stochastic Upper Credible Limit (UCL) model of human decision-making to human subject data. The fits show statistically significant differences in behavior across related, but distinct, tasks. Note},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.04635v1},
author = {Reverdy, Paul and Leonard, Naomi Ehrich},
doi = {10.1109/TASE.2015.2499244},
eprint = {arXiv:1502.04635v1},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Reverdy, Leonard - 2015 - Parameter Estimation in Softmax Decision-Making Models With Linear Objective Functions(2).pdf:pdf},
issn = {15455955},
journal = {IEEE Transactions on Automation Science and Engineering},
number = {1},
pages = {54--67},
title = {{Parameter Estimation in Softmax Decision-Making Models With Linear Objective Functions}},
volume = {13},
year = {2015}
}
@article{Parr2018,
abstract = {This paper is an attempt to explain all the matrix calculus you need in order to understand the training of deep neural networks. We assume no math knowledge beyond what you learned in calculus 1, and provide links to help you refresh the necessary math where needed. Note that you do not need to understand this material before you start learning to train and use deep learning in practice; rather, this material is for those who are already familiar with the basics of neural networks, and wish to deepen their understanding of the underlying math. Don't worry if you get stuck at some point along the way---just go back and reread the previous section, and try writing down and working through some examples. And if you're still stuck, we're happy to answer your questions in the Theory category at forums.fast.ai. Note: There is a reference section at the end of the paper summarizing all the key matrix calculus rules and terminology discussed here. See related articles at http://explained.ai},
archivePrefix = {arXiv},
arxivId = {1802.01528},
author = {Parr, Terence and Howard, Jeremy},
doi = {http://dx.doi.org/10.1016/j.plantsci.2013.05.005},
eprint = {1802.01528},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Parr, Howard - 2018 - The Matrix Calculus You Need For Deep Learning.pdf:pdf},
isbn = {0168-9452},
pages = {1--33},
title = {{The Matrix Calculus You Need For Deep Learning}},
url = {http://arxiv.org/abs/1802.01528},
volume = {2018},
year = {2018}
}
@article{ALLISON1999,
abstract = {In logit and probit regression analysis, a common practice is to estimate separate models for two or more groups and then compare coefficients across groups. An equivalent method is to test for interactions between particular predictors and dummy (indicator) variables representing the groups. Both methods may lead to invalid conclusions if residual variation differs across groups. New tests are proposed that adjust for unequal residual variation.},
archivePrefix = {arXiv},
arxivId = {0803973233},
author = {ALLISON, PAUL D.},
doi = {10.1177/0049124199028002003},
eprint = {0803973233},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/ALLISON - 1999 - Comparing Logit and Probit Coefficients Across Groups(2).pdf:pdf},
isbn = {0049-1241},
issn = {0049-1241},
journal = {Sociological Methods {\&} Research},
number = {2},
pages = {186--208},
pmid = {803973233},
title = {{Comparing Logit and Probit Coefficients Across Groups}},
url = {http://journals.sagepub.com/doi/10.1177/0049124199028002003},
volume = {28},
year = {1999}
}
@article{Benjamin2017,
abstract = {We propose to change the default P-value threshold for statistical significance for claims of new discoveries from 0.05 to 0.005.},
archivePrefix = {arXiv},
arxivId = {psyarxiv/mky9j},
author = {Benjamin, Daniel J. and Berger, James O. and Johannesson, Magnus and Nosek, Brian A. and Wagenmakers, E.-J. and Berk, Richard and Bollen, Kenneth A. and Brembs, Bj{\"{o}}rn and Brown, Lawrence and Camerer, Colin and Cesarini, David and Chambers, Christopher D. and Clyde, Merlise and Cook, Thomas D. and {De Boeck}, Paul and Dienes, Zoltan and Dreber, Anna and Easwaran, Kenny and Efferson, Charles and Fehr, Ernst and Fidler, Fiona and Field, Andy P. and Forster, Malcolm and George, Edward I. and Gonzalez, Richard and Goodman, Steven and Green, Edwin and Green, Donald P. and Greenwald, Anthony G. and Hadfield, Jarrod D. and Hedges, Larry V. and Held, Leonhard and {Hua Ho}, Teck and Hoijtink, Herbert and Hruschka, Daniel J. and Imai, Kosuke and Imbens, Guido and Ioannidis, John P. A. and Jeon, Minjeong and Jones, James Holland and Kirchler, Michael and Laibson, David and List, John and Little, Roderick and Lupia, Arthur and Machery, Edouard and Maxwell, Scott E. and McCarthy, Michael and Moore, Don A. and Morgan, Stephen L. and Munaf{\'{o}}, Marcus and Nakagawa, Shinichi and Nyhan, Brendan and Parker, Timothy H. and Pericchi, Luis and Perugini, Marco and Rouder, Jeff and Rousseau, Judith and Savalei, Victoria and Sch{\"{o}}nbrodt, Felix D. and Sellke, Thomas and Sinclair, Betsy and Tingley, Dustin and {Van Zandt}, Trisha and Vazire, Simine and Watts, Duncan J. and Winship, Christopher and Wolpert, Robert L. and Xie, Yu and Young, Cristobal and Zinman, Jonathan and Johnson, Valen E.},
doi = {10.1038/s41562-017-0189-z},
eprint = {mky9j},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Benjamin et al. - 2017 - Redefine statistical significance(5).pdf:pdf;:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Benjamin et al. - 2017 - Redefine statistical significance(6).pdf:pdf},
issn = {2397-3374},
journal = {Nature Human Behaviour},
primaryClass = {psyarxiv},
title = {{Redefine statistical significance}},
url = {http://www.nature.com/articles/s41562-017-0189-z},
year = {2017}
}
@article{Holford2005,
abstract = {Most biologists use nonlinear regression more than any other statistical technique, but there are very few places to learn about curve-fitting. This book, by the author of the very successful Intuitive Biostatistics, addresses this relatively focused need of an extraordinarily broad range of scientists.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Holford, Nicholas H. G.},
doi = {10.1002/sim.2181},
eprint = {arXiv:1011.1669v3},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Holford - 2005 - 2. Fitting models to biological data using linear and non-linear regression a practical guide to curve fitting. Harv(2).pdf:pdf},
isbn = {9780195171808},
issn = {02776715},
journal = {Statistics in Medicine},
number = {17},
pages = {2745--2746},
pmid = {25246403},
title = {{2. Fitting models to biological data using linear and non-linear regression: a practical guide to curve fitting. Harvey Motulsky and Arthur Christopoulos, Oxford University Press, Oxford, 2004. No. of pages: 352. Price: {\pounds}19.99, {\$}29.29 (paperback); {\pounds}40.00,}},
url = {http://doi.wiley.com/10.1002/sim.2181},
volume = {24},
year = {2005}
}
@article{Gelman2006,
abstract = {It is common to summarize statistical comparisons by declarations of statistical significance or nonsignificance. Here we discuss one problem with such declarations, namely that changes in statistical significance are often not themselves statistically significant. By this, we are not merely making the commonplace observation that any particular threshold is arbitrary?for example, only a small change is required to move an estimate from a 5.1{\%} significance level to 4.9{\%}, thus moving it into statistical significance. Rather, we are pointing out that even large changes in significance levels can correspond to small, nonsignificant changes in the underlying quantities.The error we describe is conceptually different from other oft-cited problems?that statistical significance is not the same as practical importance, that dichotomization into significant and nonsignificant results encourages the dismissal of observed differences in favor of the usually less interesting null hypothesis of no difference, and that any particular threshold for declaring significance is arbitrary. We are troubled by all of these concerns and do not intend to minimize their importance. Rather, our goal is to bring attention to this additional error of interpretation. We illustrate with a theoretical example and two applied examples. The ubiquity of this statistical error leads us to suggest that students and practitioners be made more aware that the difference between ?significant? and ?not significant? is not itself statistically significant. It is common to summarize statistical comparisons by declarations of statistical significance or nonsignificance. Here we discuss one problem with such declarations, namely that changes in statistical significance are often not themselves statistically significant. By this, we are not merely making the commonplace observation that any particular threshold is arbitrary?for example, only a small change is required to move an estimate from a 5.1{\%} significance level to 4.9{\%}, thus moving it into statistical significance. Rather, we are pointing out that even large changes in significance levels can correspond to small, nonsignificant changes in the underlying quantities.The error we describe is conceptually different from other oft-cited problems?that statistical significance is not the same as practical importance, that dichotomization into significant and nonsignificant results encourages the dismissal of observed differences in favor of the usually less interesting null hypothesis of no difference, and that any particular threshold for declaring significance is arbitrary. We are troubled by all of these concerns and do not intend to minimize their importance. Rather, our goal is to bring attention to this additional error of interpretation. We illustrate with a theoretical example and two applied examples. The ubiquity of this statistical error leads us to suggest that students and practitioners be made more aware that the difference between ?significant? and ?not significant? is not itself statistically significant.},
author = {Gelman, Andrew and Stern, Hal},
doi = {10.1198/000313006X152649},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Gelman, Stern - 2006 - The Difference Between “Significant” and “Not Significant” is not Itself Statistically Significant(2).pdf:pdf},
isbn = {0602440371100},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {comparison,hypothesis testing,meta-analysis,pairwise,replication},
number = {4},
pages = {328--331},
title = {{The Difference Between “Significant” and “Not Significant” is not Itself Statistically Significant}},
volume = {60},
year = {2006}
}
@book{Bishop2006,
author = {Bishop, Christopher M.},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Bishop - 2006 - Pattern Recognition and Machine Learning.pdf:pdf},
isbn = {0-387-31073-8},
title = {{Pattern Recognition and Machine Learning}},
year = {2006}
}
@article{Penny2003,
abstract = {This chapter discusses the statistical inferences obtained from functional imaging studies involving many subjects. There are two main reasons for studying multiple subjects. The first is that one may be interested in individual differences, as in many areas of psychology. The second is that one is interested in what is common to the subjects. Throughout this chapter different analysis methods are illustrated using data from a PET study of verbal fluency. This chapter describes how neuroimaging data from multiple subjects can be analyzed using fixed-effects (FFX) or Random-effects (RFX) analysis. FFX analysis is used for reporting case studies, and RFX is used to make inferences about the population from which subjects are drawn. Furthermore it shows how the analyses are implemented and then describes the underlying mathematical models. In neuroimaging, RFX is implemented using the computationally efficient summary statistic approach. This chapter also shows that this is mathematically equivalent to the more computationally demanding maximum likelihood procedure. {\textcopyright} 2004 Elsevier Inc. All rights reserved.},
author = {Penny, William and Holmes, A.},
doi = {10.1016/B978-012264841-0/50044-5},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Penny, Holmes - 2003 - Random-Effects Analysis(2).pdf:pdf},
isbn = {9780122648410},
issn = {1053-8119},
journal = {Human Brain Function: Second Edition},
pages = {843--850},
pmid = {10334905},
title = {{Random-Effects Analysis}},
year = {2003}
}
@article{RTeam2018,
address = {Vienna, Austria},
author = {{R Development Core Team}},
doi = {R Foundation for Statistical Computing, Vienna, Austria. ISBN 3-900051-07-0, URL http://www.R-project.org.},
isbn = {3-900051-07-0},
issn = {16000706},
pages = {1},
title = {{R: A language and environment for statistical computing.}},
url = {https://www.r-project.org/},
year = {2018}
}
@article{Varoquaux2017,
abstract = {Predictive models ground many state-of-the-art developments in statistical brain image analysis: decoding, MVPA, searchlight, or extraction of biomarkers. The principled approach to establish their validity and usefulness is cross-validation, testing prediction on unseen data. Here, I would like to raise awareness on error bars of cross-validation, which are often underestimated. Simple experiments show that sample sizes of many neuroimaging studies inherently lead to large error bars, eg ±10{\%} for 100 samples. The standard error across folds strongly underestimates them. These large error bars compromise the reliability of conclusions drawn with predictive models, such as biomarkers or methods developments where, unlike with cognitive neuroimaging MVPA approaches, more samples cannot be acquired by repeating the experiment across many subjects. Solutions to increase sample size must be investigated, tackling possible increases in heterogeneity of the data.},
archivePrefix = {arXiv},
arxivId = {1706.07581},
author = {Varoquaux, Ga{\"{e}}l},
doi = {10.1016/j.neuroimage.2017.06.061},
eprint = {1706.07581},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Varoquaux - 2017 - Cross-validation failure Small sample sizes lead to large error bars.pdf:pdf},
issn = {10959572},
journal = {NeuroImage},
keywords = {Biomarkers,Cross-validation,Decoding,FMRI,MVPA,Model selection,Statistics},
number = {June},
pages = {1--10},
pmid = {28655633},
title = {{Cross-validation failure: Small sample sizes lead to large error bars}},
year = {2017}
}
@article{MacCallum2002,
abstract = {The authors examine the practice of dichotomization of quantitative measures, wherein relationships among variables are examined after 1 or more variables have been converted to dichotomous variables by splitting the sample at some point on the scale(s) of measurement. A common form of dichotomization is the median split, where the independent variable is split at the median to form high and low groups, which are then compared with respect to their means on the dependent variable. The consequences of dichotomization for measurement and statistical analyses are illustrated and discussed. The use of dichotomization in practice is described, and justifications that are offered for such usage are examined. The authors present the case that dichotomization is rarely defensible and often will yield misleading results.},
author = {MacCallum, Robert C. and Zhang, Shaobo and Preacher, Kristopher J. and Rucker, Derek D.},
doi = {10.1037/1082-989X.7.1.19},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/MacCallum et al. - 2002 - On the practice of dichotomization of quantitative variables(2).pdf:pdf},
isbn = {1082-989X},
issn = {1939-1463},
journal = {Psychological Methods},
number = {1},
pages = {19--40},
pmid = {11928888},
title = {{On the practice of dichotomization of quantitative variables.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1082-989X.7.1.19},
volume = {7},
year = {2002}
}
@article{Ioannidis2005,
abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
archivePrefix = {arXiv},
arxivId = {gr-qc/0208024},
author = {Ioannidis, John P A},
doi = {10.1371/journal.pmed.0020124},
eprint = {0208024},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Ioannidis - 2005 - Why most published research findings are false.pdf:pdf},
isbn = {3540239081},
issn = {15491277},
journal = {PLoS Medicine},
number = {8},
pages = {0696--0701},
pmid = {16060722},
primaryClass = {gr-qc},
title = {{Why most published research findings are false}},
volume = {2},
year = {2005}
}
@article{Wager2007,
abstract = {Meta-analysis is an increasingly popular and valuable tool for summarizing results across many neuroimaging studies. It can be used to establish consensus on the locations of functional regions, test hypotheses developed from patient and animal studies and develop new hypotheses on structure-function correspondence. It is particularly valuable in neuroimaging because most studies do not adequately correct for multiple comparisons; based on statistical thresholds used, we estimate that roughly 10-20{\%} of reported activations in published studies are false positives. In this article, we briefly summarize some of the most popular meta-analytic approaches and their limitations, and we outline a revised multilevel approach with increased validity for establishing consistency across studies. We also discuss multivariate methods by which meta-analysis can be used to develop and test hypotheses about co-activity of brain regions. Finally, we argue that meta-analyses can make a uniquely valuable contribution to predicting psychological states from patterns of brain activity, and we briefly discuss some methods for making such predictions.},
author = {Wager, Tor D. and Lindquist, Martin and Kaplan, Lauren},
doi = {10.1093/scan/nsm015},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Wager, Lindquist, Kaplan - 2007 - Meta-analysis of functional neuroimaging data Current and future directions(2).pdf:pdf},
isbn = {1749-5024 (Electronic)$\backslash$n1749-5016 (Linking)},
issn = {17495016},
journal = {Social Cognitive and Affective Neuroscience},
keywords = {Analysis methods,Meta-analysis,Neuroimaging,PET,fMRI},
number = {2},
pages = {150--158},
pmid = {18985131},
title = {{Meta-analysis of functional neuroimaging data: Current and future directions}},
volume = {2},
year = {2007}
}
@article{Clauset2004,
abstract = {The discovery and analysis of community structure in networks is a topic of considerable recent interest within the physics community, but most methods proposed so far are unsuitable for very large networks because of their computational cost. Here we present a hierarchical agglomeration algorithm for detecting community structure which is faster than many competing algorithms: its running time on a network with n vertices and m edges is O(m d log n) where d is the depth of the dendrogram describing the community structure. Many real-world networks are sparse and hierarchical, with m {\~{}} n and d {\~{}} log n, in which case our algorithm runs in essentially linear time, O(n log{\^{}}2 n). As an example of the application of this algorithm we use it to analyze a network of items for sale on the web-site of a large online retailer, items in the network being linked if they are frequently purchased by the same buyer. The network has more than 400,000 vertices and 2 million edges. We show that our algorithm can extract meaningful communities from this network, revealing large-scale patterns present in the purchasing habits of customers.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0408187},
author = {Clauset, Aaron and Newman, M. E. J. and Moore, Cristopher},
doi = {10.1103/PhysRevE.70.066111},
eprint = {0408187},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Clauset, Newman, Moore - 2004 - Finding community structure in very large networks(2).pdf:pdf},
isbn = {1063-651X},
issn = {1539-3755},
pages = {1--6},
primaryClass = {cond-mat},
title = {{Finding community structure in very large networks}},
url = {http://arxiv.org/abs/cond-mat/0408187{\%}0Ahttp://dx.doi.org/10.1103/PhysRevE.70.066111},
volume = {066111},
year = {2004}
}
@article{Benjamini2001,
abstract = {Benjamini and Hochberg suggest that the false discovery rate may be the appropriate error rate to control in many applied multiple testing prob-lems. A simple procedure was given there as an FDR controlling procedure for independent test statistics and was shown to be much more powerful than comparable procedures which control the traditional familywise error rate. We prove that this same procedure also controls the false discovery rate when the test statistics have positive regression dependency on each of the test statistics corresponding to the true null hypotheses. This condition for positive dependency is general enough to cover many problems of prac-tical interest, including the comparisons of many treatments with a single control, multivariate normal test statistics with positive correlation matrix and multivariate t. Furthermore, the test statistics may be discrete, and the tested hypotheses composite without posing special difficulties. For all other forms of dependency, a simple conservative modification of the proce-dure controls the false discovery rate. Thus the range of problems for which a procedure with proven FDR control can be offered is greatly increased.},
archivePrefix = {arXiv},
arxivId = {0801.1095},
author = {Benjamini, Yoav and Yekutieli, Daniel},
doi = {10.1214/aos/1013699998},
eprint = {0801.1095},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Benjamini, Yekutieli - 2001 - The control of the false discovery rate in multiple testing under dependency.pdf:pdf},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Discrete test statistics,FDR,Hochberg's procedure,MTP2 densities,Multiple comparisons procedures,Multiple endpoints many-to-one comparisons,Positive regression dependency,Simes' equality,Unidimensional latent variables},
number = {4},
pages = {1165--1188},
pmid = {18298808},
title = {{The control of the false discovery rate in multiple testing under dependency}},
volume = {29},
year = {2001}
}
@article{Jialong,
author = {Jialong, Jialong He and M-file, Prevent and M-file, Allow and Editor, Array and Browser, Workspace and Get, Unix},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Jialong et al. - Unknown - Working with Operating Environment Managing Variables and the Workspace Language Constructs and Debugging (2).pdf:pdf},
title = {{Working with Operating Environment Managing Variables and the Workspace Language Constructs and Debugging Object-Oriented Programming}}
}
@article{Ypma2014,
abstract = {This document describes how to use nloptr, which is an R interface to NLopt. NLopt is a free/open-source library for nonlinear optimiza-tion started by Steven G. Johnson, providing a common interface for a number of different free optimization routines available online as well as original implementations of various other algorithms. The NLopt library is available under the GNU Lesser General Public License (LGPL), and the copyrights are owned by a variety of authors.},
author = {Ypma, J.},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Ypma - 2014 - Introduction to nloptr an R interface to NLopt.pdf:pdf},
pages = {1--15},
title = {{Introduction to nloptr: an R interface to NLopt}},
url = {https://cran.rstudio.com/web/packages/nloptr/vignettes/nloptr.pdf{\%}0Ahttps://cran.rstudio.com/web/packages/nloptr/index.html},
year = {2014}
}
@book{Integrated,
abstract = {This book series reflects the recent rapid growth in the development and application of R, the programming language and software environment for statistical computing and graphics. R is now widely used in academic research, education, and industry. It is constantly growing, with new versions of the core software released regularly and more than 5,000 packages available. It is difficult for the documentation to keep pace with the expansion of the software, and this vital book series provides a forum for the publication of books covering many aspects of the development and application of R. The scope of the series is wide, covering three main threads: • Applications of R to specific disciplines such as biology, epidemiology, genetics, engineering, finance, and the social sciences. • Using R for the study of topics of statistical methodology, such as linear and mixed modeling, time series, Bayesian methods, and missing data. • The development of R, including programming, building packages, and graphics. The books will appeal to programmers and developers of R software, as well as applied statisticians and data analysts in many fields. The books will feature detailed worked examples and R code fully integrated into the text, ensuring their usefulness to researchers, practitioners and students.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Integrated, A New and Environment, Development},
doi = {10.1201/b17487},
eprint = {arXiv:1011.1669v3},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Integrated, Environment - Unknown - Advanced R.pdf:pdf},
isbn = {978-1-4665-8696-3},
issn = {0717-6163},
pages = {1--45},
pmid = {15003161},
title = {{Advanced R}}
}
@article{Zou2005,
author = {Zou, H and Hastie, T},
doi = {10.1111/j.1467-9868.2005.00503.x},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Zou, Hastie - 2005 - Regression and variable selection via the elastic net.pdf:pdf},
journal = {J R Stat Soc: Ser B},
keywords = {grouping effect,lars algorithm,lasso,p,penalization},
pages = {301--320},
title = {{Regression and variable selection via the elastic net}},
url = {http://dx.doi.org/10.1111/j.1467-9868.2005.00503.x},
volume = {67},
year = {2005}
}
@article{Tobyne2017,
abstract = {Human frontal cortex is commonly described as being insensitive to sensory modality, however several recent studies cast doubt on this view. Our laboratory previously reported two visual-biased attention regions interleaved with two auditory-biased attention regions, bilaterally, within lateral frontal cortex. These regions selectively formed functional networks with posterior visual-biased and auditory-biased attention regions. Here, we conducted a series of functional connectivity analyses to validate and expand this analysis to 469 subjects from the Human Connectome Project (HCP). Functional connectivity analyses replicated the original findings and revealed a novel hemispheric connectivity bias. We also subdivided lateral frontal cortex into 21 thin-slice ROIs and observed bilateral patterns of spatially alternating visual-biased and auditory-biased attention network connectivity. Finally, we performed a correlation difference analysis that revealed five additional bilateral lateral frontal regions differentially connected to either the visual-biased or auditory-biased attention networks. These findings leverage the HCP dataset to demonstrate that sensory-biased attention networks may have widespread influence in lateral frontal cortical organization.},
author = {Tobyne, Sean M. and Osher, David E. and Michalka, Samantha W. and Somers, David C.},
doi = {10.1016/j.neuroimage.2017.08.020},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Tobyne et al. - 2017 - Sensory-biased attention networks in human lateral frontal cortex revealed by intrinsic functional connectivit(2).pdf:pdf},
issn = {10959572},
journal = {NeuroImage},
keywords = {Attention networks,Auditory attention,Connectomics,Human Connectome Project,Resting state functional connectivity,Visual attention},
number = {August},
pages = {362--372},
pmid = {28830764},
publisher = {Elsevier Ltd},
title = {{Sensory-biased attention networks in human lateral frontal cortex revealed by intrinsic functional connectivity}},
url = {https://doi.org/10.1016/j.neuroimage.2017.08.020},
volume = {162},
year = {2017}
}
@article{P.B.STARKANDD.A.FREEDMAN2016,
author = {{P. B. STARK AND D. A. FREEDMAN}},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/P. B. STARK AND D. A. FREEDMAN - 2016 - WHAT IS THE CHANCE OF AN EARTHQUAKE(2).pdf:pdf},
pages = {1--14},
title = {{WHAT IS THE CHANCE OF AN EARTHQUAKE?}},
year = {2016}
}
@article{Luxburg2007,
author = {Luxburg, Ulrike Von},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Luxburg - 2007 - Luxburg07{\_}tutorial{\_}44880.pdf:pdf},
keywords = {graph laplacian,spectral clustering},
pages = {1--32},
title = {{Luxburg07{\_}tutorial{\_}4488[0]}},
url = {papers2://publication/uuid/1D74E402-979E-4AB3-B684-D5727C8998A1},
year = {2007}
}
@article{Zhang2015,
abstract = {Networks are a popular tool for representing elements in a system and their interconnectedness. Many observed networks can be viewed as only samples of some true underlying network. Such is frequently the case, for example, in the monitoring and study of massive, online social networks. We study the problem of how to estimate the degree distribution - an object of fundamental interest - of a true underlying network from its sampled network. In particular, we show that this problem can be formulated as an inverse problem. Playing a key role in this formulation is a matrix relating the expectation of our sampled degree distribution to the true underlying degree distribution. Under many network sampling designs, this matrix can be defined entirely in terms of the design and is found to be ill-conditioned. As a result, our inverse problem frequently is ill-posed. Accordingly, we offer a constrained, penalized weighted least-squares approach to solving this problem. A Monte Carlo variant of Stein's unbiased risk estimation (SURE) is used to select the penalization parameter. We explore the behavior of our resulting estimator of network degree distribution in simulation, using a variety of combinations of network models and sampling regimes. In addition, we demonstrate the ability of our method to accurately reconstruct the degree distributions of various sub-communities within online social networks corresponding to Friendster, Orkut and LiveJournal. Overall, our results show that the true degree distributions from both homogeneous and inhomogeneous networks can be recovered with substantially greater accuracy than reflected in the empirical degree distribution resulting from the original sampling.},
archivePrefix = {arXiv},
arxivId = {arXiv:1305.4977v4},
author = {Zhang, Yaonan and Kolaczyk, Eric D. and Spencer, Bruce D.},
doi = {10.1214/14-AOAS800},
eprint = {arXiv:1305.4977v4},
file = {:Users/Claudio/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Kolaczyk, Spencer - 2015 - Estimating network degree distributions under sampling An inverse problem, with applications to mon(2).pdf:pdf},
issn = {19417330},
journal = {Annals of Applied Statistics},
keywords = {Constrained penalized weighted least squares,Degree distribution,Inverse problem,Network,Network sampling},
number = {1},
pages = {166--199},
title = {{Estimating network degree distributions under sampling: An inverse problem, with applications to monitoring social media networks}},
volume = {9},
year = {2015}
}
